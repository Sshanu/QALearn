{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file2id import file2id\n",
    "from sim2id import sim2id\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(17, 35), match='\\n\\n1 decision trees'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.1 what does it mean to learn?'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.2 some canonical learning problems'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n1.3 the decision tree model of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.4 formalizing the learning problem'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.5 chapter summary and outlook'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n\\n2 limits of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n2.1 data generating distributions'>\n",
      "<_sre.SRE_Match object; span=(0, 57), match='\\n2.2 inductive bias: what we know before the dat>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n2.3 not everything is learnable'>\n",
      "<_sre.SRE_Match object; span=(0, 31), match='\\n2.4 underﬁtting and overﬁtting'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n2.5 separation of training and test data'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n2.6 models, parameters and hyperparameters'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n2.7 real world applications of machine learning>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n\\n\\n\\n3 geometry and nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n3.1 from data to feature vectors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.2 k-nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.3 decision boundaries'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n3.4 k-means clustering'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n3.5 warning: high dimensions are scary'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n\\n4 the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n4.1 bio-inspired learning'>\n",
      "<_sre.SRE_Match object; span=(0, 52), match='\\n4.2 error-driven updating: the perceptron algor>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n4.3 geometric intrepretation'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n4.4 interpreting perceptron weights'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n4.5 perceptron convergence and linear separabil>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n4.6 improved generalization: voting and averagi>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n4.7 limitations of the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n5 practical issues'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.1 the importance of good features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.2 irrelevant and redundant features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.3 feature pruning and normalization'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.4 combinatorial feature explosion'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n5.5 evaluating model performance'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n5.6 cross validation'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n5.7 hypothesis testing and statistical signiﬁca>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n5.8 debugging learning algorithms'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n5.9 bias/variance trade-off'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n\\n6 beyond binary classification'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n6.1 learning with imbalanced data'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n6.2 multiclass classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 12), match='\\n6.3 ranking'>\n",
      "<_sre.SRE_Match object; span=(0, 17), match='\\n\\n7 linear models'>\n",
      "<_sre.SRE_Match object; span=(0, 49), match='\\n7.1 the optimization framework for linear model>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n7.2 convex surrogate loss functions'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n7.3 weight regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n7.4 optimization with gradient descent'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n7.5 from gradients to subgradients'>\n",
      "<_sre.SRE_Match object; span=(0, 46), match='\\n7.6 closed-form optimization for squared loss'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n7.7 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n8 bias and fairness'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n8.1 train/test mismatch'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n8.2 unsupervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n8.3 supervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n8.4 fairness and data bias'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n8.5 how badly can it go?'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n9 probabilistic modeling'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n9.1 classiﬁcation by density estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n9.2 statistical estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.3 naive bayes models'>\n",
      "<_sre.SRE_Match object; span=(0, 15), match='\\n9.4 prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.5 generative stories'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.6 conditional models'>\n",
      "<_sre.SRE_Match object; span=(0, 30), match='\\n9.7 regularization via priors'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n10 neural networks'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n10.1 bio-inspired multi-layer networks'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n10.2 the back-propagation algorithm'>\n",
      "<_sre.SRE_Match object; span=(0, 55), match='\\n10.3 initialization and convergence of neural n>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n10.4 beyond two layers'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n10.5 breadth versus depth'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n10.6 basis functions'>\n",
      "<_sre.SRE_Match object; span=(0, 19), match='\\n\\n11 kernel methods'>\n",
      "<_sre.SRE_Match object; span=(0, 42), match='\\n11.1 from feature combinations to kernels'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n11.2 kernelized perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n11.3 kernelized k-means'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n11.4 what makes a kernel'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n11.5 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n11.6 understanding support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n12 learning theory'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n12.1 the role of theory'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n12.2 induction is impossible'>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.3 probably approximately correct learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n12.4 pac learning of conjunctions'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n12.5 occam’s razor: simple solutions generalize>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.6 complexity of inﬁnite hypothesis spaces'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n13 ensemble methods'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n13.1 voting multiple classiﬁers'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n13.2 boosting weak learners'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n13.3 random ensembles'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n14 efficient learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n14.1 what does it mean to be fast?'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n14.2 stochastic optimization'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n14.3 sparse regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n14.4 feature hashing'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n15 unsupervised learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n15.1 k-means clustering, revisited'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n15.2 linear dimensionality reduction'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n15.3 autoencoders'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n\\n16 expectation maximization'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n16.1 grading an exam without an answer key'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.2 clustering with a mixture of gaussians'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.3 the expectation maximization framework'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n\\n\\n17 structured prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.1 multiclass perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.2 structured perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n17.3 argmax for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n17.4 structured support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.5 loss-augmented argmax'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n17.6 argmax in general'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n17.7 dynamic programming for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n18 imitation learning'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n18.1 imitation learning by classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n18.2 failure analysis'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n18.3 dataset aggregation'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n18.4 expensive algorithms as experts'>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n18.5 structured prediction via imitation learni>\n",
      "<_sre.SRE_Match object; span=(16549, 16581), match=' 18 (out of 20) correctly. thus,'>\n",
      "['1', '1.1', '1.2', '1.3', '1.4', '1.5', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.6', '2.7', '3', '3.1', '3.2', '3.3', '3.4', '3.5', '4', '4.1', '4.2', '4.3', '4.4', '4.5', '4.6', '4.7', '5', '5.1', '5.2', '5.3', '5.4', '5.5', '5.6', '5.7', '5.8', '5.9', '6', '6.1', '6.2', '6.3', '7', '7.1', '7.2', '7.3', '7.4', '7.5', '7.6', '7.7', '8', '8.1', '8.2', '8.3', '8.4', '8.5', '9', '9.1', '9.2', '9.3', '9.4', '9.5', '9.6', '9.7', '10', '10.1', '10.2', '10.3', '10.4', '10.5', '10.6', '11', '11.1', '11.2', '11.3', '11.4', '11.5', '11.6', '12', '12.1', '12.2', '12.3', '12.4', '12.5', '12.6', '13', '13.1', '13.2', '13.3', '14', '14.1', '14.2', '14.3', '14.4', '15', '15.1', '15.2', '15.3', '16', '16.1', '16.2', '16.3', '17', '17.1', '17.2', '17.3', '17.4', '17.5', '17.6', '17.7', '18', '18.1', '18.2', '18.3', '18.4', '18.5']\n",
      "len 1.1 4318\n",
      "pass 0\n",
      "len 1.2 1814\n",
      "pass 1\n",
      "len 1.3 9939\n",
      "pass 2\n",
      "len 1.4 8251\n",
      "pass 3\n",
      "len 1.5 3356\n",
      "pass 4\n",
      "len 2.1 2527\n",
      "pass 5\n",
      "len 2.2 3016\n",
      "pass 6\n",
      "len 2.3 3626\n",
      "pass 7\n",
      "len 2.4 5157\n",
      "pass 8\n",
      "len 2.5 2843\n",
      "pass 9\n",
      "len 2.6 4563\n",
      "pass 10\n",
      "len 2.7 6090\n",
      "pass 11\n",
      "len 3.1 5096\n",
      "pass 12\n",
      "len 3.2 8575\n",
      "pass 13\n",
      "len 3.3 2865\n",
      "pass 14\n",
      "len 3.4 5237\n",
      "pass 15\n",
      "len 3.5 10876\n",
      "pass 16\n",
      "len 4.1 3288\n",
      "pass 17\n",
      "len 4.2 6451\n",
      "pass 18\n",
      "len 4.3 5796\n",
      "pass 19\n",
      "len 4.4 2095\n",
      "pass 20\n",
      "len 4.5 9291\n",
      "pass 21\n",
      "len 4.6 5933\n",
      "pass 22\n",
      "len 4.7 4264\n",
      "pass 23\n",
      "len 5.1 3263\n",
      "pass 24\n",
      "len 5.2 6089\n",
      "pass 25\n",
      "len 5.3 7768\n",
      "pass 26\n",
      "len 5.4 3049\n",
      "pass 27\n",
      "len 5.5 7444\n",
      "pass 28\n",
      "len 5.6 5129\n",
      "pass 29\n",
      "len 5.7 7753\n",
      "pass 30\n",
      "len 5.8 4368\n",
      "pass 31\n",
      "len 5.9 4930\n",
      "pass 32\n",
      "len 6.1 9268\n",
      "pass 33\n",
      "len 6.2 10255\n",
      "pass 34\n",
      "len 6.3 11598\n",
      "pass 35\n",
      "len 7.1 4465\n",
      "pass 36\n",
      "len 7.2 5076\n",
      "pass 37\n",
      "len 7.3 5913\n",
      "pass 38\n",
      "len 7.4 7033\n",
      "pass 39\n",
      "len 7.5 3335\n",
      "pass 40\n",
      "len 7.6 5780\n",
      "pass 41\n",
      "len 7.7 10256\n",
      "pass 42\n",
      "len 8.1 3511\n",
      "pass 43\n",
      "len 8.2 6970\n",
      "pass 44\n",
      "len 8.3 4855\n",
      "pass 45\n",
      "len 8.4 4822\n",
      "pass 46\n",
      "len 8.5 8695\n",
      "pass 47\n",
      "len 9.1 3382\n",
      "pass 48\n",
      "len 9.2 5191\n",
      "pass 49\n",
      "len 9.3 3917\n",
      "pass 50\n",
      "len 9.4 3335\n",
      "pass 51\n",
      "len 9.5 2450\n",
      "pass 52\n",
      "len 9.6 4002\n",
      "pass 53\n",
      "len 9.7 4153\n",
      "pass 54\n",
      "len 10.1 7924\n",
      "pass 55\n",
      "len 10.2 5319\n",
      "pass 56\n",
      "len 10.3 4381\n",
      "pass 57\n",
      "len 10.4 3176\n",
      "pass 58\n",
      "len 10.5 4570\n",
      "pass 59\n",
      "len 10.6 4037\n",
      "pass 60\n",
      "len 11.1 2915\n",
      "pass 61\n",
      "len 11.2 4040\n",
      "pass 62\n",
      "len 11.3 2433\n",
      "pass 63\n",
      "len 11.4 6275\n",
      "pass 64\n",
      "len 11.5 6045\n",
      "pass 65\n",
      "len 11.6 6464\n",
      "pass 66\n",
      "len 12.1 2142\n",
      "pass 67\n",
      "len 12.2 2924\n",
      "pass 68\n",
      "len 12.3 2710\n",
      "pass 69\n",
      "len 12.4 6472\n",
      "pass 70\n",
      "len 12.5 4421\n",
      "pass 71\n",
      "len 12.6 5832\n",
      "pass 72\n",
      "len 13.1 5196\n",
      "pass 73\n",
      "len 13.2 8815\n",
      "pass 74\n",
      "len 13.3 3584\n",
      "pass 75\n",
      "len 14.1 1259\n",
      "pass 76\n",
      "len 14.2 5576\n",
      "pass 77\n",
      "len 14.3 4093\n",
      "pass 78\n",
      "len 14.4 5218\n",
      "pass 79\n",
      "len 15.1 9277\n",
      "pass 80\n",
      "len 15.2 7380\n",
      "pass 81\n",
      "len 15.3 1299\n",
      "pass 82\n",
      "len 16.1 7126\n",
      "pass 83\n",
      "len 16.2 4304\n",
      "pass 84\n",
      "len 16.3 8119\n",
      "pass 85\n",
      "len 17.1 4614\n",
      "pass 86\n",
      "len 17.2 4340\n",
      "pass 87\n",
      "len 17.3 5294\n",
      "pass 88\n",
      "len 17.4 7789\n",
      "pass 89\n",
      "len 17.5 2853\n",
      "pass 90\n",
      "len 17.6 4276\n",
      "pass 91\n",
      "len 17.7 10611\n",
      "pass 92\n",
      "len 18.1 4694\n",
      "pass 93\n",
      "len 18.2 2727\n",
      "pass 94\n",
      "len 18.3 6019\n",
      "pass 95\n",
      "len 18.4 3135\n",
      "pass 96\n",
      "len 18.5 7069\n"
     ]
    }
   ],
   "source": [
    "index_list, sections, flag = file2id(\"qalearn/media/txt/A_Course_in_Machine_Learning.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"qalearn/media/data/A_Course_in_Machine_Learning\", \"wb\")\n",
    "pkl.dump([index_list, sections], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1structured prediction via imitation learning a ﬁnal case where an expert can often be computed algorithmically\\narises when one solves structured prediction (see chapter 17) via\\nimitation learning. it is clearest how this can work in the case of\\nsequence labeling. recall there that predicted outputs should be\\nsequences of labels. the running example from the earlier chapter\\nwas: figure 18.3: imit:dldfs: depth limited\\ndepth-ﬁrst search x = “ monsters eat tasty bunnies “\\ny = noun verb adj noun (18.6)\\n(18.7) one can easily cast the prediction of y as a sequential decision mak-\\ning problem, by treating the production of y in a left-to-right manner.\\nin this case, we have a time horizon t = 4. we want to learn a policy\\nf that ﬁrst predicts “noun” then “verb” then “adj” then “noun” on\\nthis input. 220 a course in machine learning let’s suppose that the input to f consists of features extracted both\\nfrom the input (x) and the current predicted output preﬁx ˆy, denoted\\nφ(x, ˆy). for instance, φ(x, ˆy) might represent a similar set of features\\nto those use in chapter 17. it is perhaps easiest to think of f as just\\na classiﬁer: given some features of the input sentence x (“monsters\\neat tasty bunnies”), and some features about previous predictions in\\nthe output preﬁx (so far, produced “noun verb”), the goal of f is to\\npredict the tag for the next word (“tasty”) in this context. an important question is: what is the “expert” in this case? in-\\ntuitively, the expert should provide the correct next label, but what\\ndoes this mean? that depends on the loss function being optimized.\\nunder hamming loss (sum zero/one loss over each individual pre-\\ndiction), the expert is straightforward. when the expert is asked to\\nproduce an action for the third word, the expert’s response is always\\n“adj” (or whatever happens to be the correct label for the third word\\nin the sentence it is currently training on). more generally, the expert gets to look at x, y and a preﬁx ˆy of the\\noutput. note, importantly, that the preﬁx might be wrong! in particular,\\nafter the ﬁrst iteration of dagger, the preﬁx will be predicted by\\nthe learned policy, which may make mistakes! the expert also has\\nsome structured loss function (cid:96) that it is trying to minimize. like\\nin the previous section, the expert’s goal is to choose the action that\\nminimizes the long-term loss according to (cid:96) on this example. to be more formal, we need a bit of notation. let best((cid:96), y, ˆy) denote the loss (according to (cid:96) and the ground truth y) of the best\\nreachable output starting at ˆy. for instance, if y is “noun verb adj\\nnoun” and ˆy is “noun noun”, and the loss is hamming loss, then the\\nbest achievable output (predicting left-to-right) is “noun noun adj\\nnoun” which has a loss of 1. thus, best for this situation is 1. given that notion of best, the expert is easy to deﬁne: expert((cid:96), y, ˆy) = argmin a best((cid:96), y, ˆy ◦ a) (18.8) namely, it is the action that leads to the best possible completion\\nafter taking that action. so in the example above, the expert action\\nis “adj”. for some problems and some loss functions, computing\\nthe expert is easy. in particular, for sequence labeling under ham-\\nming loss, it’s trivial. in the case that you can compute the expert\\nexactly, it is often called an oracle.5 for some other problems, exactly\\ncomputing an oracle is computationally expensive or intractable. in\\nthose cases, one can often resort to depth limited depth-ﬁrst-search\\n(algorithm 18.4) to compute an approximate oracle as an expert. to be very concrete, a typical implementation of dagger applied\\nto sequence labeling would go as follows. each structured training\\nexample (a pair of sentence and tag-sequence) gives rise to one trajec- 5 some literature calls it a “dynamic\\noracle”, though the extra word is\\nunnecessary. imitation learning 221 tory. at training time, a predict tag seqence is generated left-to-right,\\nstarting with the empty sequence. at any given time step, you are\\nattempting to predict the label of the tth word in the input. you de-\\nﬁne a feature vector φ(x, ˆy), which will typically consist of: (a) the tth\\nword, (b) left and right neighbors of the tth word, (c) the last few pre-\\ndictions in ˆy, and (d) anything else you can think of. in particular, the\\nfeatures are not limited to markov style features, because we’re not\\nlonger trying to do dynamic programming. the expert label for the\\ntth word is just the corresponding label in the ground truth y. given\\nall this, one can run dagger (algorithm 18.4) exactly as speciﬁed.\\nmoving to structured prediction problems other than sequence labeling problems is beyond the scope of this book. the general\\nframework is to cast your structured prediction problem as a sequen-\\ntial decision making problem. once you’ve done that, you need to\\ndecide on features (this is the easy part) and an expert (this is often\\nthe harder part). however, once you’ve done so, there are generic\\nlibraries for “compiling” your speciﬁcation down to code. 1further reading todo further reading code and datasets rating +2\\n+2\\n+2\\n+2\\n+2\\n+1\\n+1\\n+1\\n0\\n0\\n0\\n0\\n-1\\n-1\\n-1\\n-1\\n-2\\n-2\\n-2\\n-2 easy? ai? sys? thy? morning? y\\ny\\nn\\nn\\nn\\ny\\ny\\nn\\nn\\ny\\nn\\ny\\ny\\nn\\nn\\ny\\nn\\nn\\ny\\ny y\\ny\\ny\\nn\\ny\\ny\\ny\\ny\\nn\\nn\\ny\\ny\\ny\\nn\\nn\\nn\\nn\\ny\\nn\\nn n\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\nn\\nn\\nn\\ny\\ny\\ny\\ny\\ny\\ny\\ny\\ny\\ny y\\ny\\nn\\ny\\nn\\nn\\ny\\ny\\nn\\ny\\ny\\ny\\nn\\ny\\nn\\nn\\ny\\nn\\nn\\nn n\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\ny\\ny\\nn\\ny\\ny\\nn\\ny\\ny\\nn\\ny\\nn\\ny table 1: course rating data set bibliography shai ben-david, john blitzer, koby crammer, and fernando pereira.\\nanalysis of representations for domain adaptation. advances in\\nneural information processing systems, 19:137, 2007. steffen bickel, michael bruckner, and tobias scheffer. discriminative\\nlearning for differing training and test distributions. in proceedings\\nof the international conference on machine learning (icml), 2007. sergey brin. near neighbor search in large metric spaces. in confer-\\nence on very large databases (vldb), 1995. hal daumé iii. frustratingly easy domain adaptation. in conference\\nof the association for computational linguistics (acl), prague, czech\\nrepublic, 2007. sorelle a friedler, carlos scheidegger, and suresh venkatasub-\\nramanian. on the (im)possibility of fairness. arxiv preprint\\narxiv:1609.07236, 2016. moritz hardt, eric price, and nathan srebro. equality of oppor-\\ntunity in supervised learning. in advances in neural information\\nprocessing systems, pages 3315–3323, 2016. matti kääriäinen. lower bounds for reductions. talk at the atomic\\nlearning workshop (tti-c), march 2006.\\ntom m. mitchell. machine learning. mcgraw hill, 1997.\\nj. ross quinlan. induction of decision trees. machine learning, 1(1):\\n81–106, 1986. frank rosenblatt. the perceptron: a probabilistic model for infor-\\nmation storage and organization in the brain. psychological review,\\n65:386–408, 1958. reprinted in neurocomputing (mit press, 1998). stéphane ross, geoff j. gordon, and j. andrew bagnell. a reduction\\nof imitation learning and structured prediction to no-regret online\\nlearning. in proceedings of the workshop on artiﬁcial intelligence and\\nstatistics (aistats), 2011.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
