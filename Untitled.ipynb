{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file2id import file2id\n",
    "from sim2id import sim2id\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(17, 35), match='\\n\\n1 decision trees'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.1 what does it mean to learn?'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.2 some canonical learning problems'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n1.3 the decision tree model of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.4 formalizing the learning problem'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.5 chapter summary and outlook'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n\\n2 limits of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n2.1 data generating distributions'>\n",
      "<_sre.SRE_Match object; span=(0, 57), match='\\n2.2 inductive bias: what we know before the dat>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n2.3 not everything is learnable'>\n",
      "<_sre.SRE_Match object; span=(0, 31), match='\\n2.4 underﬁtting and overﬁtting'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n2.5 separation of training and test data'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n2.6 models, parameters and hyperparameters'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n2.7 real world applications of machine learning>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n\\n\\n\\n3 geometry and nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n3.1 from data to feature vectors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.2 k-nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.3 decision boundaries'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n3.4 k-means clustering'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n3.5 warning: high dimensions are scary'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n\\n4 the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n4.1 bio-inspired learning'>\n",
      "<_sre.SRE_Match object; span=(0, 52), match='\\n4.2 error-driven updating: the perceptron algor>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n4.3 geometric intrepretation'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n4.4 interpreting perceptron weights'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n4.5 perceptron convergence and linear separabil>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n4.6 improved generalization: voting and averagi>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n4.7 limitations of the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n5 practical issues'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.1 the importance of good features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.2 irrelevant and redundant features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.3 feature pruning and normalization'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.4 combinatorial feature explosion'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n5.5 evaluating model performance'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n5.6 cross validation'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n5.7 hypothesis testing and statistical signiﬁca>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n5.8 debugging learning algorithms'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n5.9 bias/variance trade-off'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n\\n6 beyond binary classification'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n6.1 learning with imbalanced data'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n6.2 multiclass classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 12), match='\\n6.3 ranking'>\n",
      "<_sre.SRE_Match object; span=(0, 17), match='\\n\\n7 linear models'>\n",
      "<_sre.SRE_Match object; span=(0, 49), match='\\n7.1 the optimization framework for linear model>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n7.2 convex surrogate loss functions'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n7.3 weight regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n7.4 optimization with gradient descent'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n7.5 from gradients to subgradients'>\n",
      "<_sre.SRE_Match object; span=(0, 46), match='\\n7.6 closed-form optimization for squared loss'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n7.7 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n8 bias and fairness'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n8.1 train/test mismatch'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n8.2 unsupervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n8.3 supervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n8.4 fairness and data bias'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n8.5 how badly can it go?'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n9 probabilistic modeling'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n9.1 classiﬁcation by density estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n9.2 statistical estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.3 naive bayes models'>\n",
      "<_sre.SRE_Match object; span=(0, 15), match='\\n9.4 prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.5 generative stories'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.6 conditional models'>\n",
      "<_sre.SRE_Match object; span=(0, 30), match='\\n9.7 regularization via priors'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n10 neural networks'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n10.1 bio-inspired multi-layer networks'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n10.2 the back-propagation algorithm'>\n",
      "<_sre.SRE_Match object; span=(0, 55), match='\\n10.3 initialization and convergence of neural n>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n10.4 beyond two layers'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n10.5 breadth versus depth'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n10.6 basis functions'>\n",
      "<_sre.SRE_Match object; span=(0, 19), match='\\n\\n11 kernel methods'>\n",
      "<_sre.SRE_Match object; span=(0, 42), match='\\n11.1 from feature combinations to kernels'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n11.2 kernelized perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n11.3 kernelized k-means'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n11.4 what makes a kernel'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n11.5 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n11.6 understanding support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n12 learning theory'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n12.1 the role of theory'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n12.2 induction is impossible'>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.3 probably approximately correct learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n12.4 pac learning of conjunctions'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n12.5 occam’s razor: simple solutions generalize>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.6 complexity of inﬁnite hypothesis spaces'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n13 ensemble methods'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n13.1 voting multiple classiﬁers'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n13.2 boosting weak learners'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n13.3 random ensembles'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n14 efficient learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n14.1 what does it mean to be fast?'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n14.2 stochastic optimization'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n14.3 sparse regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n14.4 feature hashing'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n15 unsupervised learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n15.1 k-means clustering, revisited'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n15.2 linear dimensionality reduction'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n15.3 autoencoders'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n\\n16 expectation maximization'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n16.1 grading an exam without an answer key'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.2 clustering with a mixture of gaussians'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.3 the expectation maximization framework'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n\\n\\n17 structured prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.1 multiclass perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.2 structured perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n17.3 argmax for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n17.4 structured support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.5 loss-augmented argmax'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n17.6 argmax in general'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n17.7 dynamic programming for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n18 imitation learning'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n18.1 imitation learning by classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n18.2 failure analysis'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n18.3 dataset aggregation'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n18.4 expensive algorithms as experts'>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n18.5 structured prediction via imitation learni>\n",
      "<_sre.SRE_Match object; span=(16549, 16581), match=' 18 (out of 20) correctly. thus,'>\n",
      "['1', '1.1', '1.2', '1.3', '1.4', '1.5', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.6', '2.7', '3', '3.1', '3.2', '3.3', '3.4', '3.5', '4', '4.1', '4.2', '4.3', '4.4', '4.5', '4.6', '4.7', '5', '5.1', '5.2', '5.3', '5.4', '5.5', '5.6', '5.7', '5.8', '5.9', '6', '6.1', '6.2', '6.3', '7', '7.1', '7.2', '7.3', '7.4', '7.5', '7.6', '7.7', '8', '8.1', '8.2', '8.3', '8.4', '8.5', '9', '9.1', '9.2', '9.3', '9.4', '9.5', '9.6', '9.7', '10', '10.1', '10.2', '10.3', '10.4', '10.5', '10.6', '11', '11.1', '11.2', '11.3', '11.4', '11.5', '11.6', '12', '12.1', '12.2', '12.3', '12.4', '12.5', '12.6', '13', '13.1', '13.2', '13.3', '14', '14.1', '14.2', '14.3', '14.4', '15', '15.1', '15.2', '15.3', '16', '16.1', '16.2', '16.3', '17', '17.1', '17.2', '17.3', '17.4', '17.5', '17.6', '17.7', '18', '18.1', '18.2', '18.3', '18.4', '18.5']\n",
      "['decision trees', 'what does it mean to learn?', 'some canonical learning problems', 'the decision tree model of learning', 'formalizing the learning problem', 'chapter summary and outlook', 'limits of learning', 'data generating distributions', 'inductive bias: what we know before the data arrives', 'not everything is learnable', 'underﬁtting and overﬁtting', 'separation of training and test data', 'models, parameters and hyperparameters', 'real world applications of machine learning', 'geometry and nearest neighbors', 'from data to feature vectors', 'k-nearest neighbors', 'decision boundaries', 'k-means clustering', 'warning: high dimensions are scary', 'the perceptron', 'bio-inspired learning', 'error-driven updating: the perceptron algorithm', 'geometric intrepretation', 'interpreting perceptron weights', 'perceptron convergence and linear separability', 'improved generalization: voting and averaging', 'limitations of the perceptron', 'practical issues', 'the importance of good features', 'irrelevant and redundant features', 'feature pruning and normalization', 'combinatorial feature explosion', 'evaluating model performance', 'cross validation', 'hypothesis testing and statistical signiﬁcance', 'debugging learning algorithms', 'bias/variance trade-off', 'beyond binary classification', 'learning with imbalanced data', 'multiclass classiﬁcation', 'ranking', 'linear models', 'the optimization framework for linear models', 'convex surrogate loss functions', 'weight regularization', 'optimization with gradient descent', 'from gradients to subgradients', 'closed-form optimization for squared loss', 'support vector machines', 'bias and fairness', 'train/test mismatch', 'unsupervised adaptation', 'supervised adaptation', 'fairness and data bias', 'how badly can it go?', 'probabilistic modeling', 'classiﬁcation by density estimation', 'statistical estimation', 'naive bayes models', 'prediction', 'generative stories', 'conditional models', 'regularization via priors', 'neural networks', 'bio-inspired multi-layer networks', 'the back-propagation algorithm', 'initialization and convergence of neural networks', 'beyond two layers', 'breadth versus depth', 'basis functions', 'kernel methods', 'from feature combinations to kernels', 'kernelized perceptron', 'kernelized k-means', 'what makes a kernel', 'support vector machines', 'understanding support vector machines', 'learning theory', 'the role of theory', 'induction is impossible', 'probably approximately correct learning', 'pac learning of conjunctions', 'occam’s razor: simple solutions generalize', 'complexity of inﬁnite hypothesis spaces', 'ensemble methods', 'voting multiple classiﬁers', 'boosting weak learners', 'random ensembles', 'efficient learning', 'what does it mean to be fast?', 'stochastic optimization', 'sparse regularization', 'feature hashing', 'unsupervised learning', 'k-means clustering, revisited', 'linear dimensionality reduction', 'autoencoders', 'expectation maximization', 'grading an exam without an answer key', 'clustering with a mixture of gaussians', 'the expectation maximization framework', 'structured prediction', 'multiclass perceptron', 'structured perceptron', 'argmax for sequences', 'structured support vector machines', 'loss-augmented argmax', 'argmax in general', 'dynamic programming for sequences', 'imitation learning', 'imitation learning by classiﬁcation', 'failure analysis', 'dataset aggregation', 'expensive algorithms as experts', 'structured prediction via imitation learning']\n",
      "[['decision trees what does it mean to learn?', 'what does it mean to learn?', '1.1'], ['decision trees some canonical learning problems', 'some canonical learning problems', '1.2'], ['decision trees the decision tree model of learning', 'the decision tree model of learning', '1.3'], ['decision trees formalizing the learning problem', 'formalizing the learning problem', '1.4'], ['decision trees chapter summary and outlook', 'chapter summary and outlook', '1.5'], ['limits of learning data generating distributions', 'data generating distributions', '2.1'], ['limits of learning inductive bias: what we know before the data arrives', 'inductive bias: what we know before the data arrives', '2.2'], ['limits of learning not everything is learnable', 'not everything is learnable', '2.3'], ['limits of learning underﬁtting and overﬁtting', 'underﬁtting and overﬁtting', '2.4'], ['limits of learning separation of training and test data', 'separation of training and test data', '2.5'], ['limits of learning models, parameters and hyperparameters', 'models, parameters and hyperparameters', '2.6'], ['limits of learning real world applications of machine learning', 'real world applications of machine learning', '2.7'], ['geometry and nearest neighbors from data to feature vectors', 'from data to feature vectors', '3.1'], ['geometry and nearest neighbors k-nearest neighbors', 'k-nearest neighbors', '3.2'], ['geometry and nearest neighbors decision boundaries', 'decision boundaries', '3.3'], ['geometry and nearest neighbors k-means clustering', 'k-means clustering', '3.4'], ['geometry and nearest neighbors warning: high dimensions are scary', 'warning: high dimensions are scary', '3.5'], ['the perceptron bio-inspired learning', 'bio-inspired learning', '4.1'], ['the perceptron error-driven updating: the perceptron algorithm', 'error-driven updating: the perceptron algorithm', '4.2'], ['the perceptron geometric intrepretation', 'geometric intrepretation', '4.3'], ['the perceptron interpreting perceptron weights', 'interpreting perceptron weights', '4.4'], ['the perceptron perceptron convergence and linear separability', 'perceptron convergence and linear separability', '4.5'], ['the perceptron improved generalization: voting and averaging', 'improved generalization: voting and averaging', '4.6'], ['the perceptron limitations of the perceptron', 'limitations of the perceptron', '4.7'], ['practical issues the importance of good features', 'the importance of good features', '5.1'], ['practical issues irrelevant and redundant features', 'irrelevant and redundant features', '5.2'], ['practical issues feature pruning and normalization', 'feature pruning and normalization', '5.3'], ['practical issues combinatorial feature explosion', 'combinatorial feature explosion', '5.4'], ['practical issues evaluating model performance', 'evaluating model performance', '5.5'], ['practical issues cross validation', 'cross validation', '5.6'], ['practical issues hypothesis testing and statistical signiﬁcance', 'hypothesis testing and statistical signiﬁcance', '5.7'], ['practical issues debugging learning algorithms', 'debugging learning algorithms', '5.8'], ['practical issues bias/variance trade-off', 'bias/variance trade-off', '5.9'], ['beyond binary classification learning with imbalanced data', 'learning with imbalanced data', '6.1'], ['beyond binary classification multiclass classiﬁcation', 'multiclass classiﬁcation', '6.2'], ['beyond binary classification ranking', 'ranking', '6.3'], ['linear models the optimization framework for linear models', 'the optimization framework for linear models', '7.1'], ['linear models convex surrogate loss functions', 'convex surrogate loss functions', '7.2'], ['linear models weight regularization', 'weight regularization', '7.3'], ['linear models optimization with gradient descent', 'optimization with gradient descent', '7.4'], ['linear models from gradients to subgradients', 'from gradients to subgradients', '7.5'], ['linear models closed-form optimization for squared loss', 'closed-form optimization for squared loss', '7.6'], ['linear models support vector machines', 'support vector machines', '7.7'], ['bias and fairness train/test mismatch', 'train/test mismatch', '8.1'], ['bias and fairness unsupervised adaptation', 'unsupervised adaptation', '8.2'], ['bias and fairness supervised adaptation', 'supervised adaptation', '8.3'], ['bias and fairness fairness and data bias', 'fairness and data bias', '8.4'], ['bias and fairness how badly can it go?', 'how badly can it go?', '8.5'], ['probabilistic modeling classiﬁcation by density estimation', 'classiﬁcation by density estimation', '9.1'], ['probabilistic modeling statistical estimation', 'statistical estimation', '9.2'], ['probabilistic modeling naive bayes models', 'naive bayes models', '9.3'], ['probabilistic modeling prediction', 'prediction', '9.4'], ['probabilistic modeling generative stories', 'generative stories', '9.5'], ['probabilistic modeling conditional models', 'conditional models', '9.6'], ['probabilistic modeling regularization via priors', 'regularization via priors', '9.7'], ['neural networks bio-inspired multi-layer networks', 'bio-inspired multi-layer networks', '10.1'], ['neural networks the back-propagation algorithm', 'the back-propagation algorithm', '10.2'], ['neural networks initialization and convergence of neural networks', 'initialization and convergence of neural networks', '10.3'], ['neural networks beyond two layers', 'beyond two layers', '10.4'], ['neural networks breadth versus depth', 'breadth versus depth', '10.5'], ['neural networks basis functions', 'basis functions', '10.6'], ['kernel methods from feature combinations to kernels', 'from feature combinations to kernels', '11.1'], ['kernel methods kernelized perceptron', 'kernelized perceptron', '11.2'], ['kernel methods kernelized k-means', 'kernelized k-means', '11.3'], ['kernel methods what makes a kernel', 'what makes a kernel', '11.4'], ['kernel methods support vector machines', 'support vector machines', '11.5'], ['kernel methods understanding support vector machines', 'understanding support vector machines', '11.6'], ['learning theory the role of theory', 'the role of theory', '12.1'], ['learning theory induction is impossible', 'induction is impossible', '12.2'], ['learning theory probably approximately correct learning', 'probably approximately correct learning', '12.3'], ['learning theory pac learning of conjunctions', 'pac learning of conjunctions', '12.4'], ['learning theory occam’s razor: simple solutions generalize', 'occam’s razor: simple solutions generalize', '12.5'], ['learning theory complexity of inﬁnite hypothesis spaces', 'complexity of inﬁnite hypothesis spaces', '12.6'], ['ensemble methods voting multiple classiﬁers', 'voting multiple classiﬁers', '13.1'], ['ensemble methods boosting weak learners', 'boosting weak learners', '13.2'], ['ensemble methods random ensembles', 'random ensembles', '13.3'], ['efficient learning what does it mean to be fast?', 'what does it mean to be fast?', '14.1'], ['efficient learning stochastic optimization', 'stochastic optimization', '14.2'], ['efficient learning sparse regularization', 'sparse regularization', '14.3'], ['efficient learning feature hashing', 'feature hashing', '14.4'], ['unsupervised learning k-means clustering, revisited', 'k-means clustering, revisited', '15.1'], ['unsupervised learning linear dimensionality reduction', 'linear dimensionality reduction', '15.2'], ['unsupervised learning autoencoders', 'autoencoders', '15.3'], ['expectation maximization grading an exam without an answer key', 'grading an exam without an answer key', '16.1'], ['expectation maximization clustering with a mixture of gaussians', 'clustering with a mixture of gaussians', '16.2'], ['expectation maximization the expectation maximization framework', 'the expectation maximization framework', '16.3'], ['structured prediction multiclass perceptron', 'multiclass perceptron', '17.1'], ['structured prediction structured perceptron', 'structured perceptron', '17.2'], ['structured prediction argmax for sequences', 'argmax for sequences', '17.3'], ['structured prediction structured support vector machines', 'structured support vector machines', '17.4'], ['structured prediction loss-augmented argmax', 'loss-augmented argmax', '17.5'], ['structured prediction argmax in general', 'argmax in general', '17.6'], ['structured prediction dynamic programming for sequences', 'dynamic programming for sequences', '17.7'], ['imitation learning imitation learning by classiﬁcation', 'imitation learning by classiﬁcation', '18.1'], ['imitation learning failure analysis', 'failure analysis', '18.2'], ['imitation learning dataset aggregation', 'dataset aggregation', '18.3'], ['imitation learning expensive algorithms as experts', 'expensive algorithms as experts', '18.4'], ['imitation learning structured prediction via imitation learning', 'structured prediction via imitation learning', '18.5']]\n",
      "pass 0\n",
      "pass 1\n",
      "pass 2\n",
      "pass 3\n",
      "pass 4\n",
      "pass 5\n",
      "pass 6\n",
      "pass 7\n",
      "pass 8\n",
      "pass 9\n",
      "pass 10\n",
      "pass 11\n",
      "pass 12\n",
      "pass 13\n",
      "pass 14\n",
      "pass 15\n",
      "pass 16\n",
      "pass 17\n",
      "pass 18\n",
      "pass 19\n",
      "pass 20\n",
      "pass 21\n",
      "pass 22\n",
      "pass 23\n",
      "pass 24\n",
      "pass 25\n",
      "pass 26\n",
      "pass 27\n",
      "pass 28\n",
      "pass 29\n",
      "pass 30\n",
      "pass 31\n",
      "pass 32\n",
      "pass 33\n",
      "pass 34\n",
      "pass 35\n",
      "pass 36\n",
      "pass 37\n",
      "pass 38\n",
      "pass 39\n",
      "pass 40\n",
      "pass 41\n",
      "pass 42\n",
      "pass 43\n",
      "pass 44\n",
      "pass 45\n",
      "pass 46\n",
      "pass 47\n",
      "pass 48\n",
      "pass 49\n",
      "pass 50\n",
      "pass 51\n",
      "pass 52\n",
      "pass 53\n",
      "pass 54\n",
      "pass 55\n",
      "pass 56\n",
      "pass 57\n",
      "pass 58\n",
      "pass 59\n",
      "pass 60\n",
      "pass 61\n",
      "pass 62\n",
      "pass 63\n",
      "pass 64\n",
      "pass 65\n",
      "pass 66\n",
      "pass 67\n",
      "pass 68\n",
      "pass 69\n",
      "pass 70\n",
      "pass 71\n",
      "pass 72\n",
      "pass 73\n",
      "pass 74\n",
      "pass 75\n",
      "pass 76\n",
      "pass 77\n",
      "pass 78\n",
      "pass 79\n",
      "pass 80\n",
      "pass 81\n",
      "pass 82\n",
      "pass 83\n",
      "pass 84\n",
      "pass 85\n",
      "pass 86\n",
      "pass 87\n",
      "pass 88\n",
      "pass 89\n",
      "pass 90\n",
      "pass 91\n",
      "pass 92\n",
      "pass 93\n",
      "pass 94\n",
      "pass 95\n",
      "pass 96\n"
     ]
    }
   ],
   "source": [
    "index_list, sections, flag = file2id(\"qalearn/media/txt/A_Course_in_Machine_Learning.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"qalearn/media/data/A_Course_in_Machine_Learning\", \"wb\")\n",
    "pkl.dump([index_list, sections], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
