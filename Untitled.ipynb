{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file2id import file2id\n",
    "from sim2id import sim2id\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(17, 35), match='\\n\\n1 decision trees'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.1 what does it mean to learn?'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.2 some canonical learning problems'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n1.3 the decision tree model of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.4 formalizing the learning problem'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.5 chapter summary and outlook'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n\\n2 limits of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n2.1 data generating distributions'>\n",
      "<_sre.SRE_Match object; span=(0, 57), match='\\n2.2 inductive bias: what we know before the dat>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n2.3 not everything is learnable'>\n",
      "<_sre.SRE_Match object; span=(0, 31), match='\\n2.4 underﬁtting and overﬁtting'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n2.5 separation of training and test data'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n2.6 models, parameters and hyperparameters'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n2.7 real world applications of machine learning>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n\\n\\n\\n3 geometry and nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n3.1 from data to feature vectors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.2 k-nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.3 decision boundaries'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n3.4 k-means clustering'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n3.5 warning: high dimensions are scary'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n\\n4 the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n4.1 bio-inspired learning'>\n",
      "<_sre.SRE_Match object; span=(0, 52), match='\\n4.2 error-driven updating: the perceptron algor>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n4.3 geometric intrepretation'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n4.4 interpreting perceptron weights'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n4.5 perceptron convergence and linear separabil>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n4.6 improved generalization: voting and averagi>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n4.7 limitations of the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n5 practical issues'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.1 the importance of good features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.2 irrelevant and redundant features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.3 feature pruning and normalization'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.4 combinatorial feature explosion'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n5.5 evaluating model performance'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n5.6 cross validation'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n5.7 hypothesis testing and statistical signiﬁca>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n5.8 debugging learning algorithms'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n5.9 bias/variance trade-off'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n\\n6 beyond binary classification'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n6.1 learning with imbalanced data'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n6.2 multiclass classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 12), match='\\n6.3 ranking'>\n",
      "<_sre.SRE_Match object; span=(0, 17), match='\\n\\n7 linear models'>\n",
      "<_sre.SRE_Match object; span=(0, 49), match='\\n7.1 the optimization framework for linear model>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n7.2 convex surrogate loss functions'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n7.3 weight regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n7.4 optimization with gradient descent'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n7.5 from gradients to subgradients'>\n",
      "<_sre.SRE_Match object; span=(0, 46), match='\\n7.6 closed-form optimization for squared loss'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n7.7 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n8 bias and fairness'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n8.1 train/test mismatch'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n8.2 unsupervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n8.3 supervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n8.4 fairness and data bias'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n8.5 how badly can it go?'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n9 probabilistic modeling'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n9.1 classiﬁcation by density estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n9.2 statistical estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.3 naive bayes models'>\n",
      "<_sre.SRE_Match object; span=(0, 15), match='\\n9.4 prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.5 generative stories'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.6 conditional models'>\n",
      "<_sre.SRE_Match object; span=(0, 30), match='\\n9.7 regularization via priors'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n10 neural networks'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n10.1 bio-inspired multi-layer networks'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n10.2 the back-propagation algorithm'>\n",
      "<_sre.SRE_Match object; span=(0, 55), match='\\n10.3 initialization and convergence of neural n>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n10.4 beyond two layers'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n10.5 breadth versus depth'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n10.6 basis functions'>\n",
      "<_sre.SRE_Match object; span=(0, 19), match='\\n\\n11 kernel methods'>\n",
      "<_sre.SRE_Match object; span=(0, 42), match='\\n11.1 from feature combinations to kernels'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n11.2 kernelized perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n11.3 kernelized k-means'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n11.4 what makes a kernel'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n11.5 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n11.6 understanding support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n12 learning theory'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n12.1 the role of theory'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n12.2 induction is impossible'>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.3 probably approximately correct learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n12.4 pac learning of conjunctions'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n12.5 occam’s razor: simple solutions generalize>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.6 complexity of inﬁnite hypothesis spaces'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n13 ensemble methods'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n13.1 voting multiple classiﬁers'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n13.2 boosting weak learners'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n13.3 random ensembles'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n14 efficient learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n14.1 what does it mean to be fast?'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n14.2 stochastic optimization'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n14.3 sparse regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n14.4 feature hashing'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n15 unsupervised learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n15.1 k-means clustering, revisited'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n15.2 linear dimensionality reduction'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n15.3 autoencoders'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n\\n16 expectation maximization'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n16.1 grading an exam without an answer key'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.2 clustering with a mixture of gaussians'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.3 the expectation maximization framework'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n\\n\\n17 structured prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.1 multiclass perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.2 structured perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n17.3 argmax for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n17.4 structured support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.5 loss-augmented argmax'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n17.6 argmax in general'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n17.7 dynamic programming for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n18 imitation learning'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n18.1 imitation learning by classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n18.2 failure analysis'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n18.3 dataset aggregation'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n18.4 expensive algorithms as experts'>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n18.5 structured prediction via imitation learni>\n",
      "<_sre.SRE_Match object; span=(16549, 16581), match=' 18 (out of 20) correctly. thus,'>\n",
      "['1', '1.1', '1.2', '1.3', '1.4', '1.5', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.6', '2.7', '3', '3.1', '3.2', '3.3', '3.4', '3.5', '4', '4.1', '4.2', '4.3', '4.4', '4.5', '4.6', '4.7', '5', '5.1', '5.2', '5.3', '5.4', '5.5', '5.6', '5.7', '5.8', '5.9', '6', '6.1', '6.2', '6.3', '7', '7.1', '7.2', '7.3', '7.4', '7.5', '7.6', '7.7', '8', '8.1', '8.2', '8.3', '8.4', '8.5', '9', '9.1', '9.2', '9.3', '9.4', '9.5', '9.6', '9.7', '10', '10.1', '10.2', '10.3', '10.4', '10.5', '10.6', '11', '11.1', '11.2', '11.3', '11.4', '11.5', '11.6', '12', '12.1', '12.2', '12.3', '12.4', '12.5', '12.6', '13', '13.1', '13.2', '13.3', '14', '14.1', '14.2', '14.3', '14.4', '15', '15.1', '15.2', '15.3', '16', '16.1', '16.2', '16.3', '17', '17.1', '17.2', '17.3', '17.4', '17.5', '17.6', '17.7', '18', '18.1', '18.2', '18.3', '18.4', '18.5']\n",
      "len 1.1 3306\n",
      "len 1.1 2227\n",
      "len 1.1 1217\n",
      "len 1.1 189\n",
      "pass 0\n",
      "len 1.2 804\n",
      "pass 1\n",
      "len 1.3 8908\n",
      "len 1.3 7850\n",
      "len 1.3 6720\n",
      "len 1.3 5693\n",
      "len 1.3 4673\n",
      "len 1.3 3612\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/QALearn/file2id.py\u001b[0m in \u001b[0;36mbreakdown\u001b[0;34m(text, max_char)\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_char\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_char\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/QALearn/file2id.py\u001b[0m in \u001b[0;36mbreakdown\u001b[0;34m(text, max_char)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                 \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_char\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_char\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-47b9fae909dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qalearn/media/txt/A_Course_in_Machine_Learning.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/QALearn/file2id.py\u001b[0m in \u001b[0;36mfile2id\u001b[0;34m(text_file_loc)\u001b[0m\n\u001b[1;32m    163\u001b[0m                         \u001b[0mnew_index_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                                 \u001b[0msections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbreakdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                                 \u001b[0msections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                 \u001b[0mnew_index_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/QALearn/file2id.py\u001b[0m in \u001b[0;36mbreakdown\u001b[0;34m(text, max_char)\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                 \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_char\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_char\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmax_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_char\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "index_list, sections, flag = file2id(\"qalearn/media/txt/A_Course_in_Machine_Learning.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"qalearn/media/data/A_Course_in_Machine_Learning\", \"wb\")\n",
    "pkl.dump([index_list, sections], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1structured prediction via imitation learning a ﬁnal case where an expert can often be computed algorithmically\\narises when one solves structured prediction (see chapter 17) via\\nimitation learning. it is clearest how this can work in the case of\\nsequence labeling. recall there that predicted outputs should be\\nsequences of labels. the running example from the earlier chapter\\nwas: figure 18.3: imit:dldfs: depth limited\\ndepth-ﬁrst search x = “ monsters eat tasty bunnies “\\ny = noun verb adj noun (18.6)\\n(18.7) one can easily cast the prediction of y as a sequential decision mak-\\ning problem, by treating the production of y in a left-to-right manner.\\nin this case, we have a time horizon t = 4. we want to learn a policy\\nf that ﬁrst predicts “noun” then “verb” then “adj” then “noun” on\\nthis input. 220 a course in machine learning let’s suppose that the input to f consists of features extracted both\\nfrom the input (x) and the current predicted output preﬁx ˆy, denoted\\nφ(x, ˆy). for instance, φ(x, ˆy) might represent a similar set of features\\nto those use in chapter 17. it is perhaps easiest to think of f as just\\na classiﬁer: given some features of the input sentence x (“monsters\\neat tasty bunnies”), and some features about previous predictions in\\nthe output preﬁx (so far, produced “noun verb”), the goal of f is to\\npredict the tag for the next word (“tasty”) in this context. an important question is: what is the “expert” in this case? in-\\ntuitively, the expert should provide the correct next label, but what\\ndoes this mean? that depends on the loss function being optimized.\\nunder hamming loss (sum zero/one loss over each individual pre-\\ndiction), the expert is straightforward. when the expert is asked to\\nproduce an action for the third word, the expert’s response is always\\n“adj” (or whatever happens to be the correct label for the third word\\nin the sentence it is currently training on). more generally, the expert gets to look at x, y and a preﬁx ˆy of the\\noutput. note, importantly, that the preﬁx might be wrong! in particular,\\nafter the ﬁrst iteration of dagger, the preﬁx will be predicted by\\nthe learned policy, which may make mistakes! the expert also has\\nsome structured loss function (cid:96) that it is trying to minimize. like\\nin the previous section, the expert’s goal is to choose the action that\\nminimizes the long-term loss according to (cid:96) on this example. to be more formal, we need a bit of notation. let best((cid:96), y, ˆy) denote the loss (according to (cid:96) and the ground truth y) of the best\\nreachable output starting at ˆy. for instance, if y is “noun verb adj\\nnoun” and ˆy is “noun noun”, and the loss is hamming loss, then the\\nbest achievable output (predicting left-to-right) is “noun noun adj\\nnoun” which has a loss of 1. thus, best for this situation is 1. given that notion of best, the expert is easy to deﬁne: expert((cid:96), y, ˆy) = argmin a best((cid:96), y, ˆy ◦ a) (18.8) namely, it is the action that leads to the best possible completion\\nafter taking that action. so in the example above, the expert action\\nis “adj”. for some problems and some loss functions, computing\\nthe expert is easy. in particular, for sequence labeling under ham-\\nming loss, it’s trivial. in the case that you can compute the expert\\nexactly, it is often called an oracle.5 for some other problems, exactly\\ncomputing an oracle is computationally expensive or intractable. in\\nthose cases, one can often resort to depth limited depth-ﬁrst-search\\n(algorithm 18.4) to compute an approximate oracle as an expert. to be very concrete, a typical implementation of dagger applied\\nto sequence labeling would go as follows. each structured training\\nexample (a pair of sentence and tag-sequence) gives rise to one trajec- 5 some literature calls it a “dynamic\\noracle”, though the extra word is\\nunnecessary. imitation learning 221 tory. at training time, a predict tag seqence is generated left-to-right,\\nstarting with the empty sequence. at any given time step, you are\\nattempting to predict the label of the tth word in the input. you de-\\nﬁne a feature vector φ(x, ˆy), which will typically consist of: (a) the tth\\nword, (b) left and right neighbors of the tth word, (c) the last few pre-\\ndictions in ˆy, and (d) anything else you can think of. in particular, the\\nfeatures are not limited to markov style features, because we’re not\\nlonger trying to do dynamic programming. the expert label for the\\ntth word is just the corresponding label in the ground truth y. given\\nall this, one can run dagger (algorithm 18.4) exactly as speciﬁed.\\nmoving to structured prediction problems other than sequence labeling problems is beyond the scope of this book. the general\\nframework is to cast your structured prediction problem as a sequen-\\ntial decision making problem. once you’ve done that, you need to\\ndecide on features (this is the easy part) and an expert (this is often\\nthe harder part). however, once you’ve done so, there are generic\\nlibraries for “compiling” your speciﬁcation down to code. 1further reading todo further reading code and datasets rating +2\\n+2\\n+2\\n+2\\n+2\\n+1\\n+1\\n+1\\n0\\n0\\n0\\n0\\n-1\\n-1\\n-1\\n-1\\n-2\\n-2\\n-2\\n-2 easy? ai? sys? thy? morning? y\\ny\\nn\\nn\\nn\\ny\\ny\\nn\\nn\\ny\\nn\\ny\\ny\\nn\\nn\\ny\\nn\\nn\\ny\\ny y\\ny\\ny\\nn\\ny\\ny\\ny\\ny\\nn\\nn\\ny\\ny\\ny\\nn\\nn\\nn\\nn\\ny\\nn\\nn n\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\nn\\nn\\nn\\ny\\ny\\ny\\ny\\ny\\ny\\ny\\ny\\ny y\\ny\\nn\\ny\\nn\\nn\\ny\\ny\\nn\\ny\\ny\\ny\\nn\\ny\\nn\\nn\\ny\\nn\\nn\\nn n\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\ny\\ny\\nn\\ny\\ny\\nn\\ny\\ny\\nn\\ny\\nn\\ny table 1: course rating data set bibliography shai ben-david, john blitzer, koby crammer, and fernando pereira.\\nanalysis of representations for domain adaptation. advances in\\nneural information processing systems, 19:137, 2007. steffen bickel, michael bruckner, and tobias scheffer. discriminative\\nlearning for differing training and test distributions. in proceedings\\nof the international conference on machine learning (icml), 2007. sergey brin. near neighbor search in large metric spaces. in confer-\\nence on very large databases (vldb), 1995. hal daumé iii. frustratingly easy domain adaptation. in conference\\nof the association for computational linguistics (acl), prague, czech\\nrepublic, 2007. sorelle a friedler, carlos scheidegger, and suresh venkatasub-\\nramanian. on the (im)possibility of fairness. arxiv preprint\\narxiv:1609.07236, 2016. moritz hardt, eric price, and nathan srebro. equality of oppor-\\ntunity in supervised learning. in advances in neural information\\nprocessing systems, pages 3315–3323, 2016. matti kääriäinen. lower bounds for reductions. talk at the atomic\\nlearning workshop (tti-c), march 2006.\\ntom m. mitchell. machine learning. mcgraw hill, 1997.\\nj. ross quinlan. induction of decision trees. machine learning, 1(1):\\n81–106, 1986. frank rosenblatt. the perceptron: a probabilistic model for infor-\\nmation storage and organization in the brain. psychological review,\\n65:386–408, 1958. reprinted in neurocomputing (mit press, 1998). stéphane ross, geoff j. gordon, and j. andrew bagnell. a reduction\\nof imitation learning and structured prediction to no-regret online\\nlearning. in proceedings of the workshop on artiﬁcial intelligence and\\nstatistics (aistats), 2011.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.3 initialization and convergence of neural networks neural networks 135 based on what you know about linear models, you might be tempted\\nto initialize all the weights in a neural network to zero. you might\\nalso have noticed that in algorithm 10.2, this is not what’s done:\\nthey’re initialized to small random values. the question is why? the answer is because an initialization of w = 0 and v = 0 will lead to “uninteresting” solutions. in other words, if you initialize the\\nmodel in this way, it will eventually get stuck in a bad local optimum.\\nto see this, ﬁrst realize that on any example x, the activation hi of the\\nhidden units will all be zero since w = 0. this means that on the ﬁrst\\niteration, the gradient on the output weights (v) will be zero, so they\\nwill stay put. furthermore, the gradient w1,d for the dth feature on\\nthe ith unit will be exactly the same as the gradient w2,d for the same\\nfeature on the second unit. this means that the weight matrix, after\\na gradient step, will change in exactly the same way for every hidden\\nunit. thinking through this example for iterations 2 . . . , the values of\\nthe hidden units will always be exactly the same, which means that\\nthe weights feeding in to any of the hidden units will be exactly the\\nsame. eventually the model will converge, but it will converge to a\\nsolution that does not take advantage of having access to the hidden\\nunits. this shows that neural networks are sensitive to their initialization.\\nin particular, the function that they optimize is non-convex, meaning\\nthat it might have plentiful local optima. (one of which is the trivial\\nlocal optimum described in the preceding paragraph.) in a sense,\\nneural networks must have local optima. suppose you have a two\\nlayer network with two hidden units that’s been optimized. you have\\nweights w1 from inputs to the ﬁrst hidden unit, weights w2 from in-\\nputs to the second hidden unit and weights (v1, v2) from the hidden\\nunits to the output. if i give you back another network with w1 and\\nw2 swapped, and v1 and v2 swapped, the network computes exactly\\nthe same thing, but with a markedly different weight structure. this\\nphenomena is known as symmetric modes (“mode” referring to an\\noptima) meaning that there are symmetries in the weight space. it\\nwould be one thing if there were lots of modes and they were all\\nsymmetric: then ﬁnding one of them would be as good as ﬁnding\\nany other. unfortunately there are additional local optima that are\\nnot global optima. random initialization of the weights of a network is a way to address both of these problems. by initializing a network with small\\nrandom weights (say, uniform between −0.1 and 0.1), the network is\\nunlikely to fall into the trivial, symmetric local optimum. moreover,\\nby training a collection of networks, each with a different random figure 10.3: convergence of randomly\\ninitialized networks 136 a course in machine learning initialization, you can often obtain better solutions that with just\\none initialization. in other words, you can train ten networks with\\ndifferent random seeds, and then pick the one that does best on held-\\nout data. figure 10.3 shows prototypical test-set performance for ten\\nnetworks with different random initialization, plus an eleventh plot\\nfor the trivial symmetric network initialized with zeros. one of the typical complaints about neural networks is that they are ﬁnicky. in particular, they have a rather large number of knobs to\\ntune:\\n1. the number of layers\\n2. the number of hidden units per layer\\n3. the gradient descent learning rate η\\n4. the initialization\\n5. the stopping iteration or weight regularization the last of these is minor (early stopping is an easy regularization\\nmethod that does not require much effort to tune), but the others\\nare somewhat signiﬁcant. even for two layer networks, having to\\nchoose the number of hidden units, and then get the learning rate\\nand initialization “right” can take a bit of work. clearly it can be\\nautomated, but nonetheless it takes time. another difﬁculty of neural networks is that their weights can\\nbe difﬁcult to interpret. you’ve seen that, for linear networks, you\\ncan often interpret high weights as indicative of positive examples\\nand low weights as indicative of negative examples. in multilayer\\nnetworks, it becomes very difﬁcult to try to understand what the\\ndifferent hidden units are doing. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
