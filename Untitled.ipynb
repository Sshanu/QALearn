{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file2id import file2id\n",
    "from sim2id import sim2id\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(17, 35), match='\\n\\n1 decision trees'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.1 what does it mean to learn?'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.2 some canonical learning problems'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n1.3 the decision tree model of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n1.4 formalizing the learning problem'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n1.5 chapter summary and outlook'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n\\n2 limits of learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n2.1 data generating distributions'>\n",
      "<_sre.SRE_Match object; span=(0, 57), match='\\n2.2 inductive bias: what we know before the dat>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n2.3 not everything is learnable'>\n",
      "<_sre.SRE_Match object; span=(0, 31), match='\\n2.4 underﬁtting and overﬁtting'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n2.5 separation of training and test data'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n2.6 models, parameters and hyperparameters'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n2.7 real world applications of machine learning>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n\\n\\n\\n3 geometry and nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n3.1 from data to feature vectors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.2 k-nearest neighbors'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n3.3 decision boundaries'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n3.4 k-means clustering'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n3.5 warning: high dimensions are scary'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n\\n4 the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n4.1 bio-inspired learning'>\n",
      "<_sre.SRE_Match object; span=(0, 52), match='\\n4.2 error-driven updating: the perceptron algor>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n4.3 geometric intrepretation'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n4.4 interpreting perceptron weights'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n4.5 perceptron convergence and linear separabil>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n4.6 improved generalization: voting and averagi>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n4.7 limitations of the perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n5 practical issues'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.1 the importance of good features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.2 irrelevant and redundant features'>\n",
      "<_sre.SRE_Match object; span=(0, 38), match='\\n5.3 feature pruning and normalization'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n5.4 combinatorial feature explosion'>\n",
      "<_sre.SRE_Match object; span=(0, 33), match='\\n5.5 evaluating model performance'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n5.6 cross validation'>\n",
      "<_sre.SRE_Match object; span=(0, 51), match='\\n5.7 hypothesis testing and statistical signiﬁca>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n5.8 debugging learning algorithms'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n5.9 bias/variance trade-off'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n\\n6 beyond binary classification'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n6.1 learning with imbalanced data'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n6.2 multiclass classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 12), match='\\n6.3 ranking'>\n",
      "<_sre.SRE_Match object; span=(0, 17), match='\\n\\n7 linear models'>\n",
      "<_sre.SRE_Match object; span=(0, 49), match='\\n7.1 the optimization framework for linear model>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n7.2 convex surrogate loss functions'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n7.3 weight regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n7.4 optimization with gradient descent'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n7.5 from gradients to subgradients'>\n",
      "<_sre.SRE_Match object; span=(0, 46), match='\\n7.6 closed-form optimization for squared loss'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n7.7 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n8 bias and fairness'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n8.1 train/test mismatch'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n8.2 unsupervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n8.3 supervised adaptation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n8.4 fairness and data bias'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n8.5 how badly can it go?'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n9 probabilistic modeling'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n9.1 classiﬁcation by density estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n9.2 statistical estimation'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.3 naive bayes models'>\n",
      "<_sre.SRE_Match object; span=(0, 15), match='\\n9.4 prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.5 generative stories'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n9.6 conditional models'>\n",
      "<_sre.SRE_Match object; span=(0, 30), match='\\n9.7 regularization via priors'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n10 neural networks'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n10.1 bio-inspired multi-layer networks'>\n",
      "<_sre.SRE_Match object; span=(0, 36), match='\\n10.2 the back-propagation algorithm'>\n",
      "<_sre.SRE_Match object; span=(0, 55), match='\\n10.3 initialization and convergence of neural n>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n10.4 beyond two layers'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n10.5 breadth versus depth'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n10.6 basis functions'>\n",
      "<_sre.SRE_Match object; span=(0, 19), match='\\n\\n11 kernel methods'>\n",
      "<_sre.SRE_Match object; span=(0, 42), match='\\n11.1 from feature combinations to kernels'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n11.2 kernelized perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n11.3 kernelized k-means'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n11.4 what makes a kernel'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n11.5 support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n11.6 understanding support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 20), match='\\n\\n12 learning theory'>\n",
      "<_sre.SRE_Match object; span=(0, 24), match='\\n12.1 the role of theory'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n12.2 induction is impossible'>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.3 probably approximately correct learning'>\n",
      "<_sre.SRE_Match object; span=(0, 34), match='\\n12.4 pac learning of conjunctions'>\n",
      "<_sre.SRE_Match object; span=(0, 48), match='\\n12.5 occam’s razor: simple solutions generalize>\n",
      "<_sre.SRE_Match object; span=(0, 45), match='\\n12.6 complexity of inﬁnite hypothesis spaces'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n\\n13 ensemble methods'>\n",
      "<_sre.SRE_Match object; span=(0, 32), match='\\n13.1 voting multiple classiﬁers'>\n",
      "<_sre.SRE_Match object; span=(0, 28), match='\\n13.2 boosting weak learners'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n13.3 random ensembles'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n14 efficient learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n14.1 what does it mean to be fast?'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n14.2 stochastic optimization'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n14.3 sparse regularization'>\n",
      "<_sre.SRE_Match object; span=(0, 21), match='\\n14.4 feature hashing'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n\\n15 unsupervised learning'>\n",
      "<_sre.SRE_Match object; span=(0, 35), match='\\n15.1 k-means clustering, revisited'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n15.2 linear dimensionality reduction'>\n",
      "<_sre.SRE_Match object; span=(0, 18), match='\\n15.3 autoencoders'>\n",
      "<_sre.SRE_Match object; span=(0, 29), match='\\n\\n16 expectation maximization'>\n",
      "<_sre.SRE_Match object; span=(0, 43), match='\\n16.1 grading an exam without an answer key'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.2 clustering with a mixture of gaussians'>\n",
      "<_sre.SRE_Match object; span=(0, 44), match='\\n16.3 the expectation maximization framework'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n\\n\\n17 structured prediction'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.1 multiclass perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.2 structured perceptron'>\n",
      "<_sre.SRE_Match object; span=(0, 26), match='\\n17.3 argmax for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 40), match='\\n17.4 structured support vector machines'>\n",
      "<_sre.SRE_Match object; span=(0, 27), match='\\n17.5 loss-augmented argmax'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n17.6 argmax in general'>\n",
      "<_sre.SRE_Match object; span=(0, 39), match='\\n17.7 dynamic programming for sequences'>\n",
      "<_sre.SRE_Match object; span=(0, 23), match='\\n\\n18 imitation learning'>\n",
      "<_sre.SRE_Match object; span=(0, 41), match='\\n18.1 imitation learning by classiﬁcation'>\n",
      "<_sre.SRE_Match object; span=(0, 22), match='\\n18.2 failure analysis'>\n",
      "<_sre.SRE_Match object; span=(0, 25), match='\\n18.3 dataset aggregation'>\n",
      "<_sre.SRE_Match object; span=(0, 37), match='\\n18.4 expensive algorithms as experts'>\n",
      "<_sre.SRE_Match object; span=(0, 50), match='\\n18.5 structured prediction via imitation learni>\n",
      "<_sre.SRE_Match object; span=(16549, 16581), match=' 18 (out of 20) correctly. thus,'>\n",
      "['1', '1.1', '1.2', '1.3', '1.4', '1.5', '2', '2.1', '2.2', '2.3', '2.4', '2.5', '2.6', '2.7', '3', '3.1', '3.2', '3.3', '3.4', '3.5', '4', '4.1', '4.2', '4.3', '4.4', '4.5', '4.6', '4.7', '5', '5.1', '5.2', '5.3', '5.4', '5.5', '5.6', '5.7', '5.8', '5.9', '6', '6.1', '6.2', '6.3', '7', '7.1', '7.2', '7.3', '7.4', '7.5', '7.6', '7.7', '8', '8.1', '8.2', '8.3', '8.4', '8.5', '9', '9.1', '9.2', '9.3', '9.4', '9.5', '9.6', '9.7', '10', '10.1', '10.2', '10.3', '10.4', '10.5', '10.6', '11', '11.1', '11.2', '11.3', '11.4', '11.5', '11.6', '12', '12.1', '12.2', '12.3', '12.4', '12.5', '12.6', '13', '13.1', '13.2', '13.3', '14', '14.1', '14.2', '14.3', '14.4', '15', '15.1', '15.2', '15.3', '16', '16.1', '16.2', '16.3', '17', '17.1', '17.2', '17.3', '17.4', '17.5', '17.6', '17.7', '18', '18.1', '18.2', '18.3', '18.4', '18.5']\n",
      "len 1.1 4288\n",
      "pass 0\n",
      "len 1.2 1778\n",
      "pass 1\n",
      "len 1.3 9900\n",
      "pass 2\n",
      "len 1.4 8215\n",
      "pass 3\n",
      "len 1.5 3304\n",
      "pass 4\n",
      "len 2.1 2494\n",
      "pass 5\n",
      "len 2.2 2960\n",
      "pass 6\n",
      "len 2.3 3595\n",
      "pass 7\n",
      "len 2.4 5127\n",
      "pass 8\n",
      "len 2.5 2803\n",
      "pass 9\n",
      "len 2.6 4521\n",
      "pass 10\n",
      "len 2.7 6010\n",
      "pass 11\n",
      "len 3.1 5064\n",
      "pass 12\n",
      "len 3.2 8552\n",
      "pass 13\n",
      "len 3.3 2842\n",
      "pass 14\n",
      "len 3.4 5215\n",
      "pass 15\n",
      "len 3.5 10821\n",
      "pass 16\n",
      "len 4.1 3263\n",
      "pass 17\n",
      "len 4.2 6400\n",
      "pass 18\n",
      "len 4.3 5768\n",
      "pass 19\n",
      "len 4.4 2060\n",
      "pass 20\n",
      "len 4.5 9241\n",
      "pass 21\n",
      "len 4.6 5884\n",
      "pass 22\n",
      "len 4.7 4212\n",
      "pass 23\n",
      "len 5.1 3228\n",
      "pass 24\n",
      "len 5.2 6052\n",
      "pass 25\n",
      "len 5.3 7731\n",
      "pass 26\n",
      "len 5.4 3014\n",
      "pass 27\n",
      "len 5.5 7412\n",
      "pass 28\n",
      "len 5.6 5109\n",
      "pass 29\n",
      "len 5.7 7703\n",
      "pass 30\n",
      "len 5.8 4335\n",
      "pass 31\n",
      "len 5.9 4872\n",
      "pass 32\n",
      "len 6.1 9235\n",
      "pass 33\n",
      "len 6.2 10227\n",
      "pass 34\n",
      "len 6.3 11571\n",
      "pass 35\n",
      "len 7.1 4417\n",
      "pass 36\n",
      "len 7.2 5041\n",
      "pass 37\n",
      "len 7.3 5888\n",
      "pass 38\n",
      "len 7.4 6995\n",
      "pass 39\n",
      "len 7.5 3301\n",
      "pass 40\n",
      "len 7.6 5735\n",
      "pass 41\n",
      "len 7.7 10209\n",
      "pass 42\n",
      "len 8.1 3488\n",
      "pass 43\n",
      "len 8.2 6943\n",
      "pass 44\n",
      "len 8.3 4830\n",
      "pass 45\n",
      "len 8.4 4796\n",
      "pass 46\n",
      "len 8.5 8647\n",
      "pass 47\n",
      "len 9.1 3343\n",
      "pass 48\n",
      "len 9.2 5165\n",
      "pass 49\n",
      "len 9.3 3895\n",
      "pass 50\n",
      "len 9.4 3321\n",
      "pass 51\n",
      "len 9.5 2428\n",
      "pass 52\n",
      "len 9.6 3980\n",
      "pass 53\n",
      "len 9.7 4105\n",
      "pass 54\n",
      "len 10.1 7886\n",
      "pass 55\n",
      "len 10.2 5284\n",
      "pass 56\n",
      "len 10.3 4327\n",
      "pass 57\n",
      "len 10.4 3154\n",
      "pass 58\n",
      "len 10.5 4545\n",
      "pass 59\n",
      "len 10.6 3999\n",
      "pass 60\n",
      "len 11.1 2874\n",
      "pass 61\n",
      "len 11.2 4014\n",
      "pass 62\n",
      "len 11.3 2410\n",
      "pass 63\n",
      "len 11.4 6251\n",
      "pass 64\n",
      "len 11.5 6017\n",
      "pass 65\n",
      "len 11.6 6403\n",
      "pass 66\n",
      "len 12.1 2119\n",
      "pass 67\n",
      "len 12.2 2896\n",
      "pass 68\n",
      "len 12.3 2666\n",
      "pass 69\n",
      "len 12.4 6439\n",
      "pass 70\n",
      "len 12.5 4374\n",
      "pass 71\n",
      "len 12.6 5768\n",
      "pass 72\n",
      "len 13.1 5165\n",
      "pass 73\n",
      "len 13.2 8788\n",
      "pass 74\n",
      "len 13.3 3541\n",
      "pass 75\n",
      "len 14.1 1226\n",
      "pass 76\n",
      "len 14.2 5548\n",
      "pass 77\n",
      "len 14.3 4067\n",
      "pass 78\n",
      "len 14.4 5173\n",
      "pass 79\n",
      "len 15.1 9243\n",
      "pass 80\n",
      "len 15.2 7344\n",
      "pass 81\n",
      "len 15.3 1254\n",
      "pass 82\n",
      "len 16.1 7084\n",
      "pass 83\n",
      "len 16.2 4261\n",
      "pass 84\n",
      "len 16.3 8051\n",
      "pass 85\n",
      "len 17.1 4588\n",
      "pass 86\n",
      "len 17.2 4314\n",
      "pass 87\n",
      "len 17.3 5269\n",
      "pass 88\n",
      "len 17.4 7750\n",
      "pass 89\n",
      "len 17.5 2827\n",
      "pass 90\n",
      "len 17.6 4254\n",
      "pass 91\n",
      "len 17.7 10551\n",
      "pass 92\n",
      "len 18.1 4654\n",
      "pass 93\n",
      "len 18.2 2706\n",
      "pass 94\n",
      "len 18.3 5995\n",
      "pass 95\n",
      "len 18.4 3099\n",
      "pass 96\n",
      "len 18.5 14337\n"
     ]
    }
   ],
   "source": [
    "index_list, sections, flag = file2id(\"qalearn/media/txt/A_Course_in_Machine_Learning.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"qalearn/media/data/A_Course_in_Machine_Learning\", \"wb\")\n",
    "pkl.dump([index_list, sections], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['? alice has just begun taking a course on machine learning. she knows\\nthat at the end of the course, she will be expected to have “learned”\\nall about this topic. a common way of gauging whether or not she\\nhas learned is for her teacher, bob, to give her a exam. she has done\\nwell at learning if she does well on the exam. but what makes a reasonable exam? if bob spends the entire semester talking about machine learning, and then gives alice an\\nexam on history of pottery, then alice’s performance on this exam\\nwill not be representative of her learning. on the other hand, if the\\nexam only asks questions that bob has answered exactly during lec-\\ntures, then this is also a bad test of alice’s learning, especially if it’s\\nan “open notes” exam. what is desired is that alice observes speciﬁc\\nexamples from the course, and then has to answer new, but related\\nquestions on the exam. this tests whether alice has the ability to generalize. generalization is perhaps the most central concept in\\nmachine learning. as a concrete example, consider a course recommendation system for undergraduate computer science students. we have a collection\\nof students and a collection of courses. each student has taken, and\\nevaluated, a subset of the courses. the evaluation is simply a score\\nfrom −2 (terrible) to +2 (awesome). the job of the recommender\\nsystem is to predict how much a particular student (say, alice) will\\nlike a particular course (say, algorithms). given historical data from course ratings (i.e., the past) we are\\ntrying to predict unseen ratings (i.e., the future). now, we could\\nbe unfair to this system as well. we could ask it whether alice is\\nlikely to enjoy the history of pottery course. this is unfair because\\nthe system has no idea what history of pottery even is, and has no\\nprior experience with this course. on the other hand, we could ask\\nit how much alice will like artiﬁcial intelligence, which she took\\nlast year and rated as +2 (awesome). we would expect the system to\\npredict that she would really like it, but this isn’t demonstrating that\\nthe system has learned: it’s simply recalling its past experience. in\\nthe former case, we’re expecting the system to generalize beyond its\\nexperience, which is unfair. in the latter case, we’re not expecting it\\nto generalize at all. this general set up of predicting the future based on the past is at the core of most machine learning. the objects that our algorithm\\nwill make predictions about are examples. in the recommender sys-\\ntem setting, an example would be some particular student/course\\npair (such as alice/algorithms). the desired prediction would be the\\nrating that alice would give to algorithms. to make this concrete, figure 1.1 shows the general framework of\\ninduction. we are given training data on which our algorithm is ex-\\npected to learn. this training data is the examples that alice observes\\nin her machine learning course, or the historical ratings data for\\nthe recommender system. based on this training data, our learning\\nalgorithm induces a function f that will map a new example to a cor-\\nresponding prediction. for example, our function might guess that\\nf (alice/machine learning) might be high because our training data\\nsaid that alice liked artiﬁcial intelligence. we want our algorithm\\nto be able to make lots of predictions, so we refer to the collection\\nof examples on which we will evaluate our algorithm as the test set.\\nthe test set is a closely guarded secret: it is the ﬁnal exam on which\\nour learning algorithm is being tested. if our algorithm gets to peek\\nat it ahead of time, it’s going to cheat and do better than it should.\\nthe goal of inductive machine learning is to take some training data and use it to induce a function f . this function f will be evalu- decision trees 9 figure 1.1: the general supervised ap-\\nproach to machine learning: a learning\\nalgorithm reads in training data and\\ncomputes a learned function f . this\\nfunction can then automatically label\\nfuture text examples. ? why is it bad if the learning algo-\\nrithm gets to peek at the test data? known labelstrainingdatalearningalgorithmf?testexamplepredictedlabel10 a course in machine learning ated on the test data. the machine learning algorithm has succeeded\\nif its performance on the test data is high. ',\n",
       " ' there are a large number of typical inductive learning problems.\\nthe primary difference between them is in what type of thing they’re\\ntrying to predict. here are some examples: regression: trying to predict a real value. for instance, predict the value of a stock tomorrow given its past performance. or predict\\nalice’s score on the machine learning ﬁnal exam based on her\\nhomework scores. binary classiﬁcation: trying to predict a simple yes/no response.\\nfor instance, predict whether alice will enjoy a course or not.\\nor predict whether a user review of the newest apple product is\\npositive or negative about the product. multiclass classiﬁcation: trying to put an example into one of a num-\\nber of classes. for instance, predict whether a news story is about\\nentertainment, sports, politics, religion, etc. or predict whether a\\ncs course is systems, theory, ai or other. ranking: trying to put a set of objects in order of relevance. for in- stance, predicting what order to put web pages in, in response to a\\nuser query. or predict alice’s ranked preferences over courses she\\nhasn’t taken. the reason that it is convenient to break machine learning prob-\\nlems down by the type of object that they’re trying to predict has to\\ndo with measuring error. recall that our goal is to build a system\\nthat can make “good predictions.” this begs the question: what does\\nit mean for a prediction to be “good?” the different types of learning\\nproblems differ in how they deﬁne goodness. for instance, in regres-\\nsion, predicting a stock price that is off by $0.05 is perhaps much\\nbetter than being off by $200.00. the same does not hold of multi-\\nclass classiﬁcation. there, accidentally predicting “entertainment”\\ninstead of “sports” is no better or worse than predicting “politics.” ',\n",
       " ' the decision tree is a classic and natural model of learning. it is\\nclosely related to the fundamental computer science notion of “di-\\nvide and conquer.” although decision trees can be applied to many ? for each of these types of canon-\\nical machine learning problems,\\ncome up with one or two concrete\\nexamples. learning problems, we will begin with the simplest case: binary clas-\\nsiﬁcation. suppose that your goal is to predict whether some unknown user\\nwill enjoy some unknown course. you must simply answer “yes” or\\n“no.” in order to make a guess, you’re allowed to ask binary ques-\\ntions about the user/course under consideration. for example: you: is the course under consideration in systems?\\nme: yes\\nyou: has this student taken any other systems courses?\\nme: yes\\nyou: has this student liked most previous systems courses?\\nme: no\\nyou: i predict this student will not like this course.\\nthe goal in learning is to ﬁgure out what questions to ask, in what order to ask them, and what answer to predict once you have asked\\nenough questions. the decision tree is so-called because we can write our set of ques- tions and guesses in a tree format, such as that in figure 1.2. in this\\nﬁgure, the questions are written in the internal tree nodes (rectangles)\\nand the guesses are written in the leaves (ovals). each non-terminal\\nnode has two children: the left child speciﬁes what to do if the an-\\nswer to the question is “no” and the right child speciﬁes what to do if\\nit is “yes.” in order to learn, i will give you training data. this data consists\\nof a set of user/course examples, paired with the correct answer for\\nthese examples (did the given user enjoy the given course?). from\\nthis, you must construct your questions. for concreteness, there is a\\nsmall data set in table 1 in the appendix of this book. this training\\ndata consists of 20 course rating examples, with course ratings and\\nanswers to questions that you might ask about this pair. we will\\ninterpret ratings of 0, +1 and +2 as “liked” and ratings of −2 and −1\\nas “hated.” in what follows, we will refer to the questions that you can ask as\\nfeatures and the responses to these questions as feature values. the\\nrating is called the label. an example is just a set of feature values.\\nand our training data is a set of examples, paired with labels. there are a lot of logically possible trees that you could build,\\neven over just this small number of features (the number is in the\\nmillions). it is computationally infeasible to consider all of these to\\ntry to choose the “best” one. instead, we will build our decision tree\\ngreedily. we will begin by asking: if i could only ask one question, what question would i ask?\\nyou want to ﬁnd a feature that is most useful in helping you guess whether this student will enjoy this course. a useful way to think decision trees 11 figure 1.2: a decision tree for a course\\nrecommender system, from which the\\nin-text “dialog” is drawn. figure 1.3: a histogram of labels for (a)\\nthe entire data set; (b-e) the examples\\nin the data set for each value of the ﬁrst\\nfour features. issystems?takenothersys?morning?likedothersys?likenahnahlikelikenonononoyesyesyesyesoverall:easy:ai:systems:theory:60%40%likenah60%40%60%40%yesno82%18%33%67%yesno20%80%100%0%yesno80%20%40%60%yesno12 a course in machine learning about this is to look at the histogram of labels for each feature. 1\\nthis is shown for the ﬁrst four features in figure 1.3. each histogram\\nshows the frequency of “like”/“hate” labels for each possible value\\nof an associated feature. from this ﬁgure, you can see that asking\\nthe ﬁrst feature is not useful: if the value is “no” then it’s hard to\\nguess the label; similarly if the answer is “yes.” on the other hand,\\nasking the second feature is useful: if the value is “no,” you can be\\npretty conﬁdent that this student will hate this course; if the answer\\nis “yes,” you can be pretty conﬁdent that this student will like this\\ncourse. more formally, you will consider each feature in turn. you might consider the feature “is this a system’s course?” this feature has two\\npossible value: no and yes. some of the training examples have an\\nanswer of “no” – let’s call that the “no” set. some of the training\\nexamples have an answer of “yes” – let’s call that the “yes” set. for\\neach set (no and yes) we will build a histogram over the labels.\\nthis is the second histogram in figure 1.3. now, suppose you were\\nto ask this question on a random example and observe a value of\\n“no.” further suppose that you must immediately guess the label for\\nthis example. you will guess “like,” because that’s the more preva-\\nlent label in the no set (actually, it’s the only label in the no set).\\nalternatively, if you receive an answer of “yes,” you will guess “hate”\\nbecause that is more prevalent in the yes set. so, for this single feature, you know what you would guess if you\\nhad to. now you can ask yourself: if i made that guess on the train-\\ning data, how well would i have done? in particular, how many ex-\\namples would i classify correctly? in the no set (where you guessed\\n“like”) you would classify all 10 of them correctly. in the yes set\\n(where you guessed “hate”) you would classify 8 (out of 10) of them\\ncorrectly. so overall you would classify 18 (out of 20) correctly. thus,\\nwe’ll say that the score of the “is this a system’s course?” question is\\n18/20. you will then repeat this computation for each of the available features to us, compute the scores for each of them. when you must\\nchoose which feature consider ﬁrst, you will want to choose the one\\nwith the highest score. but this only lets you choose the ﬁrst feature to ask about. this\\nis the feature that goes at the root of the decision tree. how do we\\nchoose subsequent features? this is where the notion of divide and\\nconquer comes in. you’ve already decided on your ﬁrst feature: “is\\nthis a systems course?” you can now partition the data into two parts:\\nthe no part and the yes part. the no part is the subset of the data\\non which value for this feature is “no”; the yes half is the rest. this\\nis the divide step. 1 a colleague related the story of\\ngetting his 8-year old nephew to\\nguess a number between 1 and 100.\\nhis nephew’s ﬁrst four questions\\nwere: is it bigger than 20? (yes) is\\nit even? (yes) does it have a 7 in it?\\n(no) is it 80? (no). it took 20 more\\nquestions to get it, even though 10\\nshould have been sufﬁcient. at 8,\\nthe nephew hadn’t quite ﬁgured out\\nhow to divide and conquer. http:\\n//blog.computationalcomplexity.\\norg/2007/04/\\ngetting-8-year-old-interested-in.\\nhtml. ? how many training examples\\nwould you classify correctly for\\neach of the other three features\\nfrom figure 1.3? decision trees 13 return leaf(guess) algorithm 1 decisiontreetrain(data, remaining features)\\n1: guess ← most frequent answer in data\\n2: if the labels in data are unambiguous then\\n3:\\n4: else if remaining features is empty then\\n5:\\n6: else\\n7: return leaf(guess)\\nfor all f ∈ remaining features do no ← the subset of data on which f =no\\nyes ← the subset of data on which f =yes\\nscore[f ] ← # of majority vote answers in no\\n+ # of majority vote answers in yes // default answer for this data // base case: no need to split further // base case: cannot split further\\n// we need to query more features // the accuracy we would get if we only queried on f end for\\nf ← the feature with maximal score(f )\\nno ← the subset of data on which f =no\\nyes ← the subset of data on which f =yes\\nleft ← decisiontreetrain(no, remaining features \\\\ {f})\\nright ← decisiontreetrain(yes, remaining features \\\\ {f})\\nreturn node(f , left, right) 18:\\n19: end if 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 5: 6: 7: algorithm 2 decisiontreetest(tree, test point)\\n1: if tree is of the form leaf(guess) then\\n2:\\n3: else if tree is of the form node(f , left, right) then\\n4: if f = no in test point then return guess return decisiontreetest(left, test point) else return decisiontreetest(right, test point) end if 8:\\n9: end if the conquer step is to recurse, and run the same routine (choosing\\nthe feature with the highest score) on the no set (to get the left half\\nof the tree) and then separately on the yes set (to get the right half of\\nthe tree). at some point it will become useless to query on additional fea- tures. for instance, once you know that this is a systems course,\\nyou know that everyone will hate it. so you can immediately predict\\n“hate” without asking any additional questions. similarly, at some\\npoint you might have already queried every available feature and still\\nnot whittled down to a single answer. in both cases, you will need to\\ncreate a leaf node and guess the most prevalent answer in the current\\npiece of the training data that you are looking at. putting this all together, we arrive at the algorithm shown in al-\\ngorithm 1.3.2 this function, decisiontreetrain takes two argu- 2 there are more nuanced algorithms\\nfor building decision trees, some of\\nwhich are discussed in later chapters of\\nthis book. they primarily differ in how\\nthey compute the score function. 14 a course in machine learning ments: our data, and the set of as-yet unused features. it has two\\nbase cases: either the data is unambiguous, or there are no remaining\\nfeatures. in either case, it returns a leaf node containing the most\\nlikely guess at this point. otherwise, it loops over all remaining fea-\\ntures to ﬁnd the one with the highest score. it then partitions the data\\ninto a no/yes split based on the best feature. it constructs its left\\nand right subtrees by recursing on itself. in each recursive call, it uses\\none of the partitions of the data, and removes the just-selected feature\\nfrom consideration. the corresponding prediction algorithm is shown in algorithm 1.3. this function recurses down the decision tree, following the edges\\nspeciﬁed by the feature values in some test point. when it reaches a\\nleaf, it returns the guess associated with that leaf. ',\n",
       " ' as you’ve seen, there are several issues that we must take into ac-\\ncount when formalizing the notion of learning. • the performance of the learning algorithm should be measured on unseen “test” data. • the way in which we measure performance should depend on the problem we are trying to solve. • there should be a strong relationship between the data that our\\nalgorithm sees at training time and the data it sees at test time. in order to accomplish this, let’s assume that someone gives us a\\nloss function, (cid:96)(·,·), of two arguments. the job of (cid:96) is to tell us how\\n“bad” a system’s prediction is in comparison to the truth. in particu-\\nlar, if y is the truth and ˆy is the system’s prediction, then (cid:96)(y, ˆy) is a\\nmeasure of error. for three of the canonical tasks discussed above, we might use the following loss functions:\\nregression: squared loss (cid:96)(y, ˆy) = (y − ˆy)2 or absolute loss (cid:96)(y, ˆy) = y − ˆy. binary classiﬁcation: zero/one loss (cid:96)(y, ˆy) = multiclass classiﬁcation: also zero/one loss. (cid:40) if y = ˆy 0\\n1 otherwise note that the loss function is something that you must decide on based on the goals of learning. now that we have deﬁned our loss function, we need to consider where the data (training and test) comes from. the model that we ? is algorithm 1.3 guaranteed to\\nterminate? this notation means that the loss is zero\\nif the prediction is correct and is one\\notherwise. ? why might it be a bad idea to use\\nzero/one loss to measure perfor-\\nmance for a regression problem? decision trees 15 math review expectated values\\n(x,y)∼d[(cid:96)(y, f (x))] for the expected loss. expectation means “average.” this is saying “if you\\nwe write e\\ndrew a bunch of (x, y) pairs independently at random from d, what would your average loss be?more\\nformally, if d is a discrete probability distribution, then this expectation can be expanded as: e (x,y)∼d[(cid:96)(y, f (x))] = ∑\\n(x,y)∈d [d(x, y)(cid:96)(y, f (x))] (1.1) this is exactly the weighted average loss over the all (x, y) pairs in d, weighted by their probability,\\nd(x, y). if d is a ﬁnite discrete distribution, for instance deﬁned by a ﬁnite data set {(x1, y1), . . . , (xn, yn)\\nthat puts equal weight on each example (probability 1/n), then we get: e (x,y)∼d[(cid:96)(y, f (x))] = ∑\\n(x,y)∈d\\nn∑ = [d(xn, yn)(cid:96)(yn, f (xn))] [d(x, y)(cid:96)(y, f (x))] deﬁnition of expectation n=1 n∑ n=1\\n1\\nn [ 1\\nn\\nn∑ n=1 = = (cid:96)(yn, f (xn))] [(cid:96)(yn, f (xn))] d is discrete and ﬁnite deﬁnition of d rearranging terms (1.2) (1.3) (1.4) (1.5) which is exactly the average loss on that dataset. the most important thing to remember is that there are two equivalent ways to think about expections:\\n(1) the expectation of some function g is the weighted average value of g, where the weights are given by\\nthe underlying probability distribution. (2) the expectation of some function g is your best guess of the\\nvalue of g if you were to draw a single item from the underlying probability distribution. figure 1.4: will use is the probabilistic model of learning. namely, there is a prob-\\nability distribution d over input/output pairs. this is often called\\nthe data generating distribution. if we write x for the input (the\\nuser/course pair) and y for the output (the rating), then d is a distri-\\nbution over (x, y) pairs. a useful way to think about d is that it gives high probability to\\nreasonable (x, y) pairs, and low probability to unreasonable (x, y)\\npairs. a (x, y) pair can be unreasonable in two ways. first, x might\\nbe an unusual input. for example, a x related to an “intro to java”\\ncourse might be highly probable; a x related to a “geometric and\\nsolid modeling” course might be less probable. second, y might\\nbe an unusual rating for the paired x. for instance, if alice were to\\ntake ai 100 times (without remembering that she took it before!),\\nshe would give the course a +2 almost every time. perhaps some 16 a course in machine learning semesters she might give a slightly lower score, but it would be un-\\nlikely to see x =alice/ai paired with y = −2.\\nit is important to remember that we are not making any assump-\\ntions about what the distribution d looks like. (for instance, we’re\\nnot assuming it looks like a gaussian or some other, common distri-\\nbution.) we are also not assuming that we know what d is. in fact,\\nif you know a priori what your data generating distribution is, your\\nlearning problem becomes signiﬁcantly easier. perhaps the hardest\\nthing about machine learning is that we don’t know what d is: all we\\nget is a random sample from it. this random sample is our training\\ndata. our learning problem, then, is deﬁned by two quantities: 1. the loss function (cid:96), which captures our notion of what is important to learn. 2. the data generating distribution d, which deﬁnes what sort of data we expect to see. ? consider the following prediction\\ntask. given a paragraph written\\nabout a course, we have to predict\\nwhether the paragraph is a positive\\nor negative review of the course.\\n(this is the sentiment analysis prob-\\nlem.) what is a reasonable loss\\nfunction? how would you deﬁne\\nthe data generating distribution? we are given access to training data, which is a random sample of input/output pairs drawn from d. based on this training data, we\\nneed to induce a function f that maps new inputs ˆx to corresponding\\nprediction ˆy. the key property that f should obey is that it should do\\nwell (as measured by (cid:96)) on future examples that are also drawn from\\nd. formally, it’s expected loss \\x01 over d with repsect to (cid:96) should be\\nas small as possible: (x,y)∼d(cid:2)(cid:96)(y, f (x))(cid:3) = ∑ \\x01 (cid:44) e d(x, y)(cid:96)(y, f (x)) (1.6) (x,y) the difﬁculty in minimizing our expected loss from eq (1.6) is\\nthat we don’t know what d is! all we have access to is some training\\ndata sampled from it! suppose that we denote our training data\\nset by d. the training data consists of n-many input/output pairs,\\n(x1, y1), (x2, y2), . . . , (xn, yn). given a learned function f , we can\\ncompute our training error, ˆ\\x01: ˆ\\x01 (cid:44) 1\\nn n∑ n=1 (cid:96)(yn, f (xn)) (1.7) that is, our training error is simply our average error over the train- ing data. of course, we can drive ˆ\\x01 to zero by simply memorizing our train- ing data. but as alice might ﬁnd in memorizing past exams, this\\nmight not generalize well to a new exam! this is the fundamental difﬁculty in machine learning: the thing we have access to is our training error, ˆ\\x01. but the thing we care about ? (x,y)∼d (cid:2)(cid:96)(y, f (x))(cid:3), by thinking verify by calculation that we\\ncan write our training error as\\ne\\nof d as a distribution that places\\nprobability 1/n to each example in\\nd and probability 0 on everything\\nelse. decision trees 17 minimizing is our expected error \\x01. in order to get the expected error\\ndown, our learned function needs to generalize beyond the training\\ndata to some future data that it might not have seen yet! so, putting it all together, we get a formal deﬁnition of induction\\nmachine learning: given (i) a loss function (cid:96) and (ii) a sample d\\nfrom some unknown distribution d, you must compute a function\\nf that has low expected error \\x01 over d with respect to (cid:96). a very important comment is that we should never expect a ma-\\nchine learning algorithm to generalize beyond the data distribution\\nit has seen at training time. in a famous—if posssibly apocryphal—\\nexample from the 1970s, the us government wanted to train a clas-\\nsiﬁer to distinguish between us tanks and russian tanks. they col-\\nlected a training and test set, and managed to build a classiﬁer with\\nnearly 100% accuracy on that data. but when this classiﬁer was run\\nin the “real world”, it failed miserably. it had not, in fact, learned\\nto distinguish between us tanks and russian tanks, but rather just\\nbetween clear photos and blurry photos. in this case, there was a bias\\nin the training data (due to how the training data was collected) that\\ncaused the learning algorithm to learn something other than what we\\nwere hoping for. we will return to this issue in chapter 8; for now,\\nsimply remember that the distribution d for training data must match\\nthe distribution d for the test data. ',\n",
       " ' at this point, you should be able to use decision trees to do machine\\nlearning. someone will give you data. you’ll split it into training,\\ndevelopment and test portions. using the training and development\\ndata, you’ll ﬁnd a good value for maximum depth that trades off\\nbetween underﬁtting and overﬁtting. you’ll then run the resulting\\ndecision tree model on the test data to get an estimate of how well\\nyou are likely to do in the future. you might think: why should i read the rest of this book? aside\\nfrom the fact that machine learning is just an awesome fun ﬁeld to\\nlearn about, there’s a lot left to cover. in the next two chapters, you’ll\\nlearn about two models that have very different inductive biases than\\ndecision trees. you’ll also get to see a very useful way of thinking\\nabout learning: the geometric view of data. this will guide much of\\nwhat follows. after that, you’ll learn how to solve problems more\\ncomplicated that simple binary classiﬁcation. (machine learning\\npeople like binary classiﬁcation a lot because it’s one of the simplest\\nnon-trivial problems that we can work on.) after that, things will\\ndiverge: you’ll learn about ways to think about learning as a formal\\noptimization problem, ways to speed up learning, ways to learn 18 a course in machine learning without labeled data (or with very little labeled data) and all sorts of\\nother fun topics. but throughout, we will focus on the view of machine learning that you’ve seen here. you select a model (and its associated induc-\\ntive biases). you use data to ﬁnd parameters of that model that work\\nwell on the training data. you use development data to avoid under-\\nﬁtting and overﬁtting. and you use test data (which you’ll never look\\nat or touch, right?) to estimate future model performance. then you\\nconquer the world. 1.6 further reading in our discussion of decision trees, we used misclassiﬁcation rate for\\nselecting features. while simple and intuitive, misclassiﬁcation rate\\nhas problems. there has been a signiﬁcant amount of work that\\nconsiders more advanced splitting criteria; the most popular is id3,\\nbased on the mutual information quantity from information the-\\nory. we have also only considered a very simple mechanism for\\ncontrolling inductive bias: limiting the depth of the decision tree.\\nagain, there are more advanced “tree pruning” techniques that typ-\\nically operate by growing deep trees and then pruning back some\\nof the branches. these approaches have the advantage that differ-\\nent branches can have different depths, accounting for the fact that\\nthe amount of data that gets passed down each branch might differ\\ndramatically3. 3 quinlan 1986 learning objectives:\\n• deﬁne “inductive bias” and recog- nize the role of inductive bias in\\nlearning. • illustrate how regularization trades\\noff between underﬁtting and overﬁt-\\nting. • evaluate whether a use of test data is “cheating” or not. dependencies: none. our lives sometimes depend on computers performing as pre-\\ndicted. – philip emeagwali machine learning is a very general and useful framework,\\nbut it is not “magic” and will not always work. in order to better\\nunderstand when it will and when it will not work, it is useful to\\nformalize the learning problem more. this will also help us develop\\ndebugging strategies for learning algorithms. ',\n",
       " ' our underlying assumption for the majority of this book is that\\nlearning problems are characterized by some unknown probability\\ndistribution d over input/output pairs (x, y) ∈ x×y. suppose that\\nsomeone told you what d was. in particular, they gave you a python\\nfunction computed that took two inputs, x and y, and returned the\\nprobability of that x, y pair under d. if you had access to such a func-\\ntion, classiﬁcation becomes simple. we can deﬁne the bayes optimal\\nclassiﬁer as the classiﬁer that, for any test input ˆx, simply returns the\\nˆy that maximizes computed( ˆx, ˆy), or, more formally: f (bo)( ˆx) = arg max ˆy∈y d( ˆx, ˆy) (2.1) this classiﬁer is optimal in one speciﬁc sense: of all possible classiﬁers,\\nit achieves the smallest zero/one error. theorem 1 (bayes optimal classiﬁer). the bayes optimal classiﬁer\\nf (bo) achieves minimal zero/one error of any deterministic classiﬁer. this theorem assumes that you are comparing against deterministic\\nclassiﬁers. you can actually prove a stronger result that f (bo) is opti-\\nmal for randomized classiﬁers as well, but the proof is a bit messier.\\nhowever, the intuition is the same: for a given x, f (bo) chooses the\\nlabel with highest probability, thus minimizing the probability that it\\nmakes an error. proof of theorem 1. consider some other classiﬁer g that claims to\\nbe better than f (bo). then, there must be some x on which g(x) (cid:54)= 20 a course in machine learning f (bo)(x). fix such an x. now, the probability that f (bo) makes an error\\non this particular x is 1 − d(x, f (bo)(x)) and the probability that g\\nmakes an error on this x is 1 − d(x, g(x)). but f (bo) was chosen in\\nsuch a way to maximize d(x, f (bo)(x)), so this must be greater than\\nd(x, g(x)). thus, the probability that f (bo) errs on this particular x is\\nsmaller than the probability that g errs on it. this applies to any x for\\nwhich f (bo)(x) (cid:54)= g(x) and therefore f (bo) achieves smaller zero/one\\nerror than any g. the bayes error rate (or bayes optimal error rate) is the error rate of the bayes optimal classiﬁer. it is the best error rate you can\\never hope to achieve on this classiﬁcation problem (under zero/one\\nloss). the take-home message is that if someone gave you access to\\nthe data distribution, forming an optimal classiﬁer would be trivial.\\nunfortunately, no one gave you this distribution, so we need to ﬁgure\\nout ways of learning the mapping from x to y given only access to a\\ntraining set sampled from d, rather than d itself. ',\n",
       " ' in figure 2.1 you’ll ﬁnd training data for a binary classiﬁcation prob-\\nlem. the two labels are “a” and “b” and you can see four examples\\nfor each label. below, in figure 2.2, you will see some test data. these\\nimages are left unlabeled. go through quickly and, based on the\\ntraining data, label these images. (really do it before you read fur-\\nther! i’ll wait!) most likely you produced one of two labelings: either abba or aabb. which of these solutions is right? the answer is that you can-\\nnot tell based on the training data. if you give this same example\\nto 100 people, 60 − 70 of them come up with the abba prediction\\nand 30 − 40 come up with the aabb prediction. why? presumably\\nbecause the ﬁrst group believes that the relevant distinction is be-\\ntween “bird” and “non-bird” while the second group believes that\\nthe relevant distinction is between “ﬂy” and “no-ﬂy.” this preference for one distinction (bird/non-bird) over another\\n(ﬂy/no-ﬂy) is a bias that different human learners have. in the con-\\ntext of machine learning, it is called inductive bias: in the absense of\\ndata that narrow down the relevant concept, what type of solutions\\nare we more likely to prefer? two thirds of people seem to have an\\ninductive bias in favor of bird/non-bird, and one third seem to have\\nan inductive bias in favor of ﬂy/no-ﬂy. throughout this book you will learn about several approaches to machine learning. the decision tree model is the ﬁrst such approach.\\nthese approaches differ primarily in the sort of inductive bias that figure 2.1: training data for a binary\\nclassiﬁcation problem. figure 2.2: test data for the same\\nclassiﬁcation problem. ? it is also possible that the correct\\nclassiﬁcation on the test data is\\nabab. this corresponds to the bias\\n“is the background in focus.” some-\\nhow no one seems to come up with\\nthis classiﬁcation rule. class aclassblimits of learning 21 they exhibit. consider a variant of the decision tree learning algorithm. in this\\nvariant, we will not allow the trees to grow beyond some pre-deﬁned\\nmaximum depth, d. that is, once we have queried on d-many fea-\\ntures, we cannot query on any more and must just make the best\\nguess we can at that point. this variant is called a shallow decision\\ntree. the key question is: what is the inductive bias of shallow decision\\ntrees? roughly, their bias is that decisions can be made by only look-\\ning at a small number of features. for instance, a shallow decision\\ntree would be very good at learning a function like “students only\\nlike ai courses.” it would be very bad at learning a function like “if\\nthis student has liked an odd number of their past courses, they will\\nlike the next one; otherwise they will not.” this latter is the parity\\nfunction, which requires you to inspect every feature to make a pre-\\ndiction. the inductive bias of a decision tree is that the sorts of things\\nwe want to learn to predict are more like the ﬁrst example and less\\nlike the second example. ',\n",
       " ' although machine learning works well—perhaps astonishingly\\nwell—in many cases, it is important to keep in mind that it is not\\nmagical. there are many reasons why a machine learning algorithm\\nmight fail on some learning task. there could be noise in the training data. noise can occur both at the feature level and at the label level. some features might corre-\\nspond to measurements taken by sensors. for instance, a robot might\\nuse a laser range ﬁnder to compute its distance to a wall. however,\\nthis sensor might fail and return an incorrect value. in a sentiment\\nclassiﬁcation problem, someone might have a typo in their review of\\na course. these would lead to noise at the feature level. there might\\nalso be noise at the label level. a student might write a scathingly\\nnegative review of a course, but then accidentally click the wrong\\nbutton for the course rating. the features available for learning might simply be insufﬁcient. for example, in a medical context, you might wish to diagnose\\nwhether a patient has cancer or not. you may be able to collect a\\nlarge amount of data about this patient, such as gene expressions,\\nx-rays, family histories, etc. but, even knowing all of this information\\nexactly, it might still be impossible to judge for sure whether this pa-\\ntient has cancer or not. as a more contrived example, you might try\\nto classify course reviews as positive or negative. but you may have\\nerred when downloading the data and only gotten the ﬁrst ﬁve char- 22 a course in machine learning acters of each review. if you had the rest of the features you might\\nbe able to do well. but with this limited feature set, there’s not much\\nyou can do. some examples may not have a single correct answer. you might be building a system for “safe web search,” which removes offen-\\nsive web pages from search results. to build this system, you would\\ncollect a set of web pages and ask people to classify them as “offen-\\nsive” or not. however, what one person considers offensive might be\\ncompletely reasonable for another person. it is common to consider\\nthis as a form of label noise. nevertheless, since you, as the designer\\nof the learning system, have some control over this problem, it is\\nsometimes helpful to isolate it as a source of difﬁculty. finally, learning might fail because the inductive bias of the learn-\\ning algorithm is too far away from the concept that is being learned.\\nin the bird/non-bird data, you might think that if you had gotten\\na few more training examples, you might have been able to tell\\nwhether this was intended to be a bird/non-bird classiﬁcation or a\\nﬂy/no-ﬂy classiﬁcation. however, no one i’ve talked to has ever come\\nup with the “background is in focus” classiﬁcation. even with many\\nmore training points, this is such an unusual distinction that it may\\nbe hard for anyone to ﬁgure out it. in this case, the inductive bias of\\nthe learner is simply too misaligned with the target classiﬁcation to\\nlearn. note that the inductive bias source of error is fundamentally dif- ferent than the other three sources of error. in the inductive bias case,\\nit is the particular learning algorithm that you are using that cannot\\ncope with the data. maybe if you switched to a different learning\\nalgorithm, you would be able to learn well. for instance, neptunians\\nmight have evolved to care greatly about whether backgrounds are\\nin focus, and for them this would be an easy classiﬁcation to learn.\\nfor the other three sources of error, it is not an issue to do with the\\nparticular learning algorithm. the error is a fundamental part of the\\nlearning problem. ',\n",
       " ' as with many problems, it is useful to think about the extreme cases\\nof learning algorithms. in particular, the extreme cases of decision\\ntrees. in one extreme, the tree is “empty” and we do not ask any\\nquestions at all. we simply immediately make a prediction. in the\\nother extreme, the tree is “full.” that is, every possible question\\nis asked along every branch. in the full tree, there may be leaves\\nwith no associated training data. for these we must simply choose\\narbitrarily whether to say “yes” or “no.” consider the course recommendation data from table 1. sup- pose we were to build an “empty” decision tree on this data. such a\\ndecision tree will make the same prediction regardless of its input,\\nbecause it is not allowed to ask any questions about its input. since\\nthere are more “likes” than “hates” in the training data (12 versus\\n8), our empty decision tree will simply always predict “likes.” the\\ntraining error, ˆ\\x01, is 8/20 = 40%. on the other hand, we could build a “full” decision tree. since each row in this data is unique, we can guarantee that any leaf in a\\nfull decision tree will have either 0 or 1 examples assigned to it (20\\nof the leaves will have one example; the rest will have none). for the\\nleaves corresponding to training points, the full decision tree will\\nalways make the correct prediction. given this, the training error, ˆ\\x01, is\\n0/20 = 0%. of course our goal is not to build a model that gets 0% error on the training data. this would be easy! our goal is a model that will\\ndo well on future, unseen data. how well might we expect these two\\nmodels to do on future data? the “empty” tree is likely to do not\\nmuch better and not much worse on future data. we might expect\\nthat it would continue to get around 40% error. life is more complicated for the “full” decision tree. certainly\\nif it is given a test example that is identical to one of the training\\nexamples, it will do the right thing (assuming no noise). but for\\neverything else, it will only get about 50% error. this means that\\neven if every other test point happens to be identical to one of the\\ntraining points, it would only get about 25% error. in practice, this is\\nprobably optimistic, and maybe only one in every 10 examples would\\nmatch a training example, yielding a 35% error. so, in one case (empty tree) we’ve achieved about 40% error and in the other case (full tree) we’ve achieved 35% error. this is not\\nvery promising! one would hope to do better! in fact, you might\\nnotice that if you simply queried on a single feature for this data, you\\nwould be able to get very low training error, but wouldn’t be forced\\nto “guess” randomly. this example illustrates the key concepts of underﬁtting and overﬁtting. underﬁtting is when you had the opportunity to learn\\nsomething but didn’t. a student who hasn’t studied much for an up-\\ncoming exam will be underﬁt to the exam, and consequently will not\\ndo well. this is also what the empty tree does. overﬁtting is when\\nyou pay too much attention to idiosyncracies of the training data,\\nand aren’t able to generalize well. often this means that your model\\nis ﬁtting noise, rather than whatever it is supposed to ﬁt. a student\\nwho memorizes answers to past exam questions without understand-\\ning them has overﬁt the training data. like the full tree, this student limits of learning 23 ? convince yourself (either by proof\\nor by simulation) that even in the\\ncase of imbalanced data – for in-\\nstance data that is on average 80%\\npositive and 20% negative – a pre-\\ndictor that guesses randomly (50/50\\npositive/negative) will get about\\n50% error. ? which feature is it, and what is it’s\\ntraining error? 24 a course in machine learning math review law of large numbers\\nconsider some random event, like spins of a roulette wheel, cars driving through an intersection, the\\noutcome of an election, or pasta being appropriately al dente. we often want to make a conclusion\\nabout the entire population (the pot of pasta) based on a much smaller sample (biting a couple pieces\\nof pasta). the law of large numbers tells us that under mild conditions this is an okay thing to do. formally, suppose that v1, v2, . . . , vn are random variables (e.g., vn measures if the nth spaghetti is\\nal dente). assume that these random variables are independent (i.e., v2 and v3 are uncorrelated—\\nthey weren’t both taken from the same place in the pot) and identically distributed (they were all\\ndrawn from the same population—pot—that we wish to measure). we can compute the sample av-\\nn=1 vn and under the strong law of large numbers, you can prove that ¯v → e[v] as\\nerage ¯v = 1\\nn → ∞. namely, the empirical sample average approaches the population average as the number of\\nsamples goes do inﬁnity. n ∑n (technical note: the notion of convergence here is almost sure convergence. in particular, the formal result is\\nlimn→∞ 1\\nthat pr\\npopulation average.) = 1. or, in words, with probability one the sample average reaches the n ∑n vn = e[v] (cid:16) (cid:17) figure 2.3: also will not do well on the exam. a model that is neither overﬁt nor\\nunderﬁt is the one that is expected to do best in the future. ',\n",
       " ' suppose that, after graduating, you get a job working for a company\\nthat provides personalized recommendations for pottery. you go in\\nand implement new algorithms based on what you learned in your\\nmachine learning class (you have learned the power of generaliza-\\ntion!). all you need to do now is convince your boss that you have\\ndone a good job and deserve a raise! how can you convince your boss that your fancy learning algo- rithms are really working? based on what we’ve talked about already with underﬁtting and\\noverﬁtting, it is not enough to just tell your boss what your training\\nerror is. noise notwithstanding, it is easy to get a training error of\\nzero using a simple database query (or grep, if you prefer). your boss\\nwill not fall for that. the easiest approach is to set aside some of your available data as\\n“test data” and use this to evaluate the performance of your learning\\nalgorithm. for instance, the pottery recommendation service that you\\nwork for might have collected 1000 examples of pottery ratings. you\\nwill select 800 of these as training data and set aside the ﬁnal 200 limits of learning 25 ? if you have more data at your dis-\\nposal, why might a 90/10 split be\\npreferable to an 80/20 split? as test data. you will run your learning algorithms only on the 800\\ntraining points. only once you’re done will you apply your learned\\nmodel to the 200 test points, and report your test error on those 200\\npoints to your boss. the hope in this process is that however well you do on the 200\\ntest points will be indicative of how well you are likely to do in the\\nfuture. this is analogous to estimating support for a presidential\\ncandidate by asking a small (random!) sample of people for their\\nopinions. statistics (speciﬁcally, concentration bounds of which the\\n“central limit theorem” is a famous example) tells us that if the sam-\\nple is large enough, it will be a good representative. the 80/20 split\\nis not magic: it’s simply fairly well established. occasionally people\\nuse a 90/10 split instead, especially if they have a lot of data. the cardinal rule of machine learning is: never touch your test data. ever. if that’s not clear enough:\\nnever ever touch your test data!\\nif there is only one thing you learn from this book, let it be that. do not look at your test data. even once. even a tiny peek. once\\nyou do that, it is not test data any more. yes, perhaps your algorithm\\nhasn’t seen it. but you have. and you are likely a better learner than\\nyour learning algorithm. consciously or otherwise, you might make\\ndecisions based on whatever you might have seen. once you look at\\nthe test data, your model’s performance on it is no longer indicative\\nof it’s performance on future unseen data. this is simply because\\nfuture data is unseen, but your “test” data no longer is. ',\n",
       " ' the general approach to machine learning, which captures many ex-\\nisting learning algorithms, is the modeling approach. the idea is that\\nwe come up with some formal model of our data. for instance, we\\nmight model the classiﬁcation decision of a student/course pair as a\\ndecision tree. the choice of using a tree to represent this model is our\\nchoice. we also could have used an arithmetic circuit or a polynomial\\nor some other function. the model tells us what sort of things we can\\nlearn, and also tells us what our inductive bias is. for most models, there will be associated parameters. these are the things that we use the data to decide on. parameters in a decision\\ntree include: the speciﬁc questions we asked, the order in which we\\nasked them, and the classiﬁcation decisions at the leaves. the job of\\nour decision tree learning algorithm decisiontreetrain is to take\\ndata and ﬁgure out a good set of parameters. 26 a course in machine learning many learning algorithms will have additional knobs that you can adjust. in most cases, these knobs amount to tuning the inductive\\nbias of the algorithm. in the case of the decision tree, an obvious\\nknob that one can tune is the maximum depth of the decision tree.\\nthat is, we could modify the decisiontreetrain function so that\\nit stops recursing once it reaches some pre-deﬁned maximum depth.\\nby playing with this depth knob, we can adjust between underﬁtting\\n(the empty tree, depth= 0) and overﬁtting (the full tree, depth= ∞).\\nsuch a knob is called a hyperparameter. it is so called because it is a parameter that controls other parameters of the model. the exact\\ndeﬁnition of hyperparameter is hard to pin down: it’s one of those\\nthings that are easier to identify than deﬁne. however, one of the\\nkey identiﬁers for hyperparameters (and the main reason that they\\ncause consternation) is that they cannot be naively adjusted using the\\ntraining data. in decisiontreetrain, as in most machine learning, the learn- ing algorithm is essentially trying to adjust the parameters of the\\nmodel so as to minimize training error. this suggests an idea for\\nchoosing hyperparameters: choose them so that they minimize train-\\ning error. what is wrong with this suggestion? suppose that you were to treat “maximum depth” as a hyperparameter and tried to tune it on\\nyour training data. to do this, maybe you simply build a collection\\nof decision trees, tree0, tree1, tree2, . . . , tree100, where treed is a tree\\nof maximum depth d. we then computed the training error of each\\nof these trees and chose the “ideal” maximum depth as that which\\nminimizes training error? which one would it pick? the answer is that it would pick d = 100. or, in general, it would pick d as large as possible. why? because choosing a bigger d will\\nnever hurt on the training data. by making d larger, you are simply\\nencouraging overﬁtting. but by evaluating on the training data, over-\\nﬁtting actually looks like a good idea! an alternative idea would be to tune the maximum depth on test data. this is promising because test data peformance is what we\\nreally want to optimize, so tuning this knob on the test data seems\\nlike a good idea. that is, it won’t accidentally reward overﬁtting. of\\ncourse, it breaks our cardinal rule about test data: that you should\\nnever touch your test data. so that idea is immediately off the table.\\nhowever, our “test data” wasn’t magic. we simply took our 1000 examples, called 800 of them “training” data and called the other 200\\n“test” data. so instead, let’s do the following. let’s take our original\\n1000 data points, and select 700 of them as training data. from the\\nremainder, take 100 as development data1 and the remaining 200\\nas test data. the job of the development data is to allow us to tune ? go back to the decisiontree-\\ntrain algorithm and modify it so\\nthat it takes a maximum depth pa-\\nrameter. this should require adding\\ntwo lines of code and modifying\\nthree others. 1 some people call this “validation\\ndata” or “held-out data.” hyperparameters. the general approach is as follows: 1. split your data into 70% training data, 10% development data and 20% test data. 2. for each possible setting of your hyperparameters: (a) train a model using that setting of hyperparameters on the training data. (b) compute this model’s error rate on the development data. 3. from the above collection of models, choose the one that achieved the lowest error rate on development data. 4. evaluate that model on the test data to estimate future test perfor- mance. ',\n",
       " ' figure 2.4 shows a typical sequence of decisions that must be made\\nto deploy a machine learning approach in the real world. in the left\\ncolumn, you can see the generic decision being made. in the right\\ncolumn, an example of this decision for the particular case of adver-\\ntising placement on a search engine we’ve built. in this sequence, (1) we have some real world goal like increasing revenue for our search engine, and decide to try to increase rev-\\nenue by (2) displaying better ads. we convert this task into a ma-\\nchine learning problem by (3) deciding to train a classiﬁer to predict\\nwhether a user will click on an ad or not. in order to apply machine\\nlearning, we must collect some training data; in this case, (4) we col-\\nlect data by logging user interactions with the current system. we\\nmust choose what to log; (5) we choose to log the ad being displayed,\\nthe query the user entered into our search engine, and binary value\\nshowing if they clicked or not. in order to make these logs consumable by a machine learning\\nalgorithm, (6) we convert the data into input/output pairs: in this\\ncase, pairs of words from a bag-of-words representing the query and\\na bag-of-words representing the ad as input, and the click as a ±\\nlabel. we then (7) select a model family (e.g., depth 20 decision trees),\\nand thereby an inductive bias, for instance depth ≤ 20 decision trees. we’re now ready to (8) select a speciﬁc subset of data to use as\\ntraining data: in this case, data from april 2016. we split this into\\ntraining and development and (9) learn a ﬁnal decision tree, tuning\\nthe maximum depth on the development data. we can then use this\\ndecision tree to (10) make predictions on some held-out test data, in limits of learning 27 ? in step 3, you could either choose\\nthe model (trained on the 70% train-\\ning data) that did the best on the\\ndevelopment data. or you could\\nchoose the hyperparameter settings\\nthat did best and retrain the model\\non the 80% union of training and\\ndevelopment data. is either of these\\noptions obviously better or worse? 1 2 3 4 5 6 7 8 9 10 11 12 real world\\ngoal\\nreal world\\nmechanism\\nlearning\\nproblem data collection collected data\\ndata\\nrepresentation\\nselect model\\nfamily\\nselect training\\ndata\\ntrain model &\\nhyperparams\\npredict on test\\ndata evaluate error deploy! increase\\nrevenue\\nbetter ad\\ndisplay\\nclassify\\nclick-through\\ninteraction w/\\ncurrent system query, ad, click\\nbow2, ± click\\ndecision trees,\\ndepth 20\\nsubset from\\napril’16\\nﬁnal decision\\ntree\\nsubset from\\nmay’16\\nzero/one loss\\nfor ± click\\n(hope we\\nachieve our\\ngoal) figure 2.4: a typical design process for\\na machine learning application. 28 a course in machine learning this case from the following month. we can (11) measure the overall\\nquality of our predictor as zero/one loss (clasiﬁcation error) on this\\ntest data and ﬁnally (12) deploy our system. the important thing about this sequence of steps is that in any one, things can go wrong. that is, between any two rows of this table,\\nwe are necessarily accumulating some additional error against our\\noriginal real world goal of increasing revenue. for example, in step 5,\\nwe decided on a representation that left out many possible variables\\nwe could have logged, like time of day or season of year. by leaving\\nout those variables, we set an explicit upper bound on how well our\\nlearned system can do. it is often an effective strategy to run an oracle experiment. in an oracle experiment, we assume that everything below some line can be\\nsolved perfectly, and measure how much impact that will have on a\\nhigher line. as an extreme example, before embarking on a machine\\nlearning approach to the ad display problem, we should measure\\nsomething like: if our classiﬁer were perfect, how much more money\\nwould we make? if the number is not very high, perhaps there is\\nsome better for our time. finally, although this sequence is denoted linearly, the entire pro- cess is highly interactive in practice. a large part of “debugging”\\nmachine learning (covered more extensively in chapter 5 involves\\ntrying to ﬁgure out where in this sequence the biggest losses are and\\nﬁxing that step. in general, it is often useful to build the stupidest thing\\nthat could possibly work, then look at how well it’s doing, and decide if\\nand where to ﬁx it. 2.8 further reading todo further reading learning objectives:\\n• describe a data set as points in a high dimensional space. • explain the curse of dimensionality.\\n• compute distances between points in high dimensional space. • implement a k-nearest neighbor model of learning. • draw decision boundaries.\\n• implement the k-means algorithm for clustering. dependencies: chapter 1 our brains have evolved to get us out of the rain, ﬁnd where the\\nberries are, and keep us from getting killed. our brains did not\\nevolve to help us grasp really large numbers or to look at things in\\na hundred thousand dimensions.\\n– ronald graham you can think of prediction tasks as mapping inputs (course\\nreviews) to outputs (course ratings). as you learned in the previ-\\nous chapter, decomposing an input into a collection of features (e.g.,\\nwords that occur in the review) forms a useful abstraction for learn-\\ning. therefore, inputs are nothing more than lists of feature values.\\nthis suggests a geometric view of data, where we have one dimen-\\nsion for every feature. in this view, examples are points in a high-\\ndimensional space. once we think of a data set as a collection of points in high dimen- sional space, we can start performing geometric operations on this\\ndata. for instance, suppose you need to predict whether alice will\\nlike algorithms. perhaps we can try to ﬁnd another student who is\\nmost “similar” to alice, in terms of favorite courses. say this student\\nis jeremy. if jeremy liked algorithms, then we might guess that alice\\nwill as well. this is an example of a nearest neighbor model of learn-\\ning. by inspecting this model, we’ll see a completely different set of\\nanswers to the key learning questions we discovered in chapter 1. ',\n",
       " ' an example is just a collection of feature values about that example,\\nfor instance the data in table 1 from the appendix. to a person, these\\nfeatures have meaning. one feature might count how many times the\\nreviewer wrote “excellent” in a course review. another might count\\nthe number of exclamation points. a third might tell us if any text is\\nunderlined in the review. to a machine, the features themselves have no meaning. only the feature values, and how they vary across examples, mean some-\\nthing to the machine. from this perspective, you can think about an\\nexample as being represented by a feature vector consisting of one\\n“dimension” for each feature, where each dimenion is simply some\\nreal value. consider a review that said “excellent” three times, had one excla- 30 a course in machine learning mation point and no underlined text. this could be represented by\\nthe feature vector (cid:104)3, 1, 0(cid:105). an almost identical review that happened\\nto have underlined text would have the feature vector (cid:104)3, 1, 1(cid:105). note, here, that we have imposed the convention that for binary features (yes/no features), the corresponding feature values are 0\\nand 1, respectively. this was an arbitrary choice. we could have\\nmade them 0.92 and −16.1 if we wanted. but 0/1 is convenient and\\nhelps us interpret the feature values. when we discuss practical\\nissues in chapter 5, you will see other reasons why 0/1 is a good\\nchoice. figure 3.1 shows the data from table 1 in three views. these three\\nviews are constructed by considering two features at a time in differ-\\nent pairs. in all cases, the plusses denote positive examples and the\\nminuses denote negative examples. in some cases, the points fall on\\ntop of each other, which is why you cannot see 20 unique points in\\nall ﬁgures. the mapping from feature values to vectors is straighforward in the case of real valued features (trivial) and binary features (mapped\\nto zero or one). it is less clear what to do with categorical features.\\nfor example, if our goal is to identify whether an object in an image\\nis a tomato, blueberry, cucumber or cockroach, we might want to\\nknow its color: is it red, blue, green or black? one option would be to map red to a value of 0, blue to a value\\nof 1, green to a value of 2 and black to a value of 3. the problem\\nwith this mapping is that it turns an unordered set (the set of colors)\\ninto an ordered set (the set {0, 1, 2, 3}). in itself, this is not necessarily\\na bad thing. but when we go to use these features, we will measure\\nexamples based on their distances to each other. by doing this map-\\nping, we are essentially saying that red and blue are more similar\\n(distance of 1) than red and black (distance of 3). this is probably\\nnot what we want to say! a solution is to turn a categorical feature that can take four dif-\\nferent values (say: red, blue, green and black) into four binary\\nfeatures (say: isitred?, isitblue?, isitgreen? and isitblack?). in gen-\\neral, if we start from a categorical feature that takes v values, we can\\nmap it to v-many binary indicator features. with that, you should be able to take a data set and map each example to a feature vector through the following mapping: • real-valued features get copied directly. • binary features become 0 (for false) or 1 (for true). • categorical features with v possible values get mapped to v-many binary indicator features. figure 3.1: a ﬁgure showing projections\\nof data in two dimension in three\\nways – see text. top: horizontal axis\\ncorresponds to the ﬁrst feature (easy)\\nand the vertical axis corresponds to\\nthe second feature (ai?); middle:\\nhorizontal is second feature and vertical\\nis third (systems?); bottom: horizontal\\nis ﬁrst and vertical is third. truly,\\nthe data points would like exactly on\\n(0, 0) or (1, 0), etc., but they have been\\npurturbed slightly to show duplicates.\\n? match the example ids from table 1\\nwith the points in figure 3.1. ? the computer scientist in you might\\nbe saying: actually we could map it\\nto log2 v-many binary features! is\\nthis a good idea or not? easy?ai?ai?sys?easy?sys?geometry and nearest neighbors 31 after this mapping, you can think of a single example as a vec-\\ntor in a high-dimensional feature space. if you have d-many fea-\\ntures (after expanding categorical features), then this feature vector\\nwill have d-many components. we will denote feature vectors as\\nx = (cid:104)x1, x2, . . . , xd(cid:105), so that xd denotes the value of the dth fea-\\nture of x. since these are vectors with real-valued components in\\nd-dimensions, we say that they belong to the space rd. for d = 2, our feature vectors are just points in the plane, like in\\nfigure 3.1. for d = 3 this is three dimensional space. for d > 3 it\\nbecomes quite hard to visualize. (you should resist the temptation\\nto think of d = 4 as “time” – this will just make things confusing.)\\nunfortunately, for the sorts of problems you will encounter in ma-\\nchine learning, d ≈ 20 is considered “low dimensional,” d ≈ 1000 is\\n“medium dimensional” and d ≈ 100000 is “high dimensional.” ',\n",
       " ' ? can you think of problems (per-\\nhaps ones already mentioned in this\\nbook!) that are low dimensional?\\nthat are medium dimensional?\\nthat are high dimensional? the biggest advantage to thinking of examples as vectors in a high\\ndimensional space is that it allows us to apply geometric concepts\\nto machine learning. for instance, one of the most basic things\\nthat one can do in a vector space is compute distances. in two-\\ndimensional space, the distance between (cid:104)2, 3(cid:105) and (cid:104)6, 1(cid:105) is given by(cid:112)(2 − 6)2 + (3 − 1)2 = 18 ≈ 4.24. in general, in d-dimensional √ space, the euclidean distance between vectors a and b is given by\\neq (3.1) (see figure 3.2 for geometric intuition in three dimensions): (cid:35) 1 2 (cid:34) d∑ d=1 d(a, b) = (ad − bd)2 (3.1) now that you have access to distances between examples, you can start thinking about what it means to learn again. consider fig-\\nure 3.3. we have a collection of training data consisting of positive\\nexamples and negative examples. there is a test point marked by a\\nquestion mark. your job is to guess the correct label for that point. most likely, you decided that the label of this test point is positive. one reason why you might have thought that is that you believe\\nthat the label for an example should be similar to the label of nearby\\npoints. this is an example of a new form of inductive bias. the nearest neighbor classiﬁer is build upon this insight. in com- parison to decision trees, the algorithm is ridiculously simple. at\\ntraining time, we simply store the entire training set. at test time,\\nwe get a test example ˆx. to predict its label, we ﬁnd the training ex-\\nample x that is most similar to ˆx. in particular, we ﬁnd the training figure 3.2: a ﬁgure showing euclidean\\ndistance in three dimensions. the\\nlength of the green segments are 0.6, 0.6\\nand 0.3 respectively, in the x-, y-, and\\nz-axes. the total distance between the\\n√\\nred dot and the orange dot is therefore 0.62 + 0.62 + 0.32 = 0.9. ? verify that d from eq (3.1) gives the\\nsame result (4.24) for the previous\\ncomputation. (0, .4, .5)(.6, 1, .8)?32 a course in machine learning example x that minimizes d(x, ˆx). since x is a training example, it has\\na corresponding label, y. we predict that the label of ˆx is also y.\\ndespite its simplicity, this nearest neighbor classiﬁer is incred-\\nibly effective. (some might say frustratingly effective.) however, it\\nis particularly prone to overﬁtting label noise. consider the data in\\nfigure 3.4. you would probably want to label the test point positive.\\nunfortunately, it’s nearest neighbor happens to be negative. since the\\nnearest neighbor algorithm only looks at the single nearest neighbor,\\nit cannot consider the “preponderance of evidence” that this point\\nshould probably actually be a positive example. it will make an un-\\nnecessary error. a solution to this problem is to consider more than just the single\\nnearest neighbor when making a classiﬁcation decision. we can con-\\nsider the k-nearest neighbors and let them vote on the correct class\\nfor this test point. if you consider the 3-nearest neighbors of the test\\npoint in figure 3.4, you will see that two of them are positive and one\\nis negative. through voting, positive would win. the full algorithm for k-nearest neighbor classiﬁcation is given in algorithm 3.2. note that there actually is no “training” phase for\\nk-nearest neighbors. in this algorithm we have introduced ﬁve new\\nconventions: 1. the training data is denoted by d. 2. we assume that there are n-many training examples. 3. these examples are pairs (x1, y1), (x2, y2), . . . , (xn, yn). (warning: do not confuse xn, the nth training example, with xd,\\nthe dth feature for example x.) 4. we use [ ]to denote an empty list and ⊕ · to append · to that list.\\n5. our prediction on ˆx is called ˆy. the ﬁrst step in this algorithm is to compute distances from the\\ntest point to all training points (lines 2-4). the data points are then\\nsorted according to distance. we then apply a clever trick of summing\\nthe class labels for each of the k nearest neighbors (lines 6-10) and\\nusing the sign of this sum as our prediction. the big question, of course, is how to choose k. as we’ve seen,\\nwith k = 1, we run the risk of overﬁtting. on the other hand, if\\nk is large (for instance, k = n), then knn-predict will always\\npredict the majority class. clearly that is underﬁtting. so, k is a\\nhyperparameter of the knn algorithm that allows us to trade-off\\nbetween overﬁtting (small value of k) and underﬁtting (large value of\\nk). figure 3.4: a ﬁgure showing an easy\\nnn classiﬁcation problem where the\\ntest point is a ? and should be positive,\\nbut its nn is actually a negative point\\nthat’s noisy.\\n? why is it a good idea to use an odd\\nnumber for k? ? why is the sign of the sum com-\\nputed in lines 2-4 the same as the\\nmajority vote of the associated\\ntraining examples? ? why can’t you simply pick the\\nvalue of k that does best on the\\ntraining data? in other words, why\\ndo we have to treat it like a hy-\\nperparameter rather than just a\\nparameter. ?geometry and nearest neighbors 33 // store distance to training example n // put lowest-distance objects ﬁrst s ← s ⊕ (cid:104)d(xn, ˆx), n(cid:105) algorithm 3 knn-predict(d, k, ˆx)\\n1: s ← [ ]\\n2: for n = 1 to n do\\n3:\\n4: end for\\n5: s ← sort(s)\\nˆy ← 0\\n6:\\n7: for k = 1 to k do\\n(cid:104)dist,n(cid:105) ← sk\\n8:\\nˆy ← ˆy + yn 9:\\n10: end for\\n11: return sign( ˆy) // n this is the kth closest data point\\n// vote according to the label for the nth training point\\n// return +1 if ˆy > 0 and −1 if ˆy < 0 one aspect of inductive bias that we’ve seen for knn is that it\\nassumes that nearby points should have the same label. another\\naspect, which is quite different from decision trees, is that all features\\nare equally important! recall that for decision trees, the key question\\nwas which features are most useful for classiﬁcation? the whole learning\\nalgorithm for a decision tree hinged on ﬁnding a small set of good\\nfeatures. this is all thrown away in knn classiﬁers: every feature\\nis used, and they are all used the same amount. this means that if\\nyou have data with only a few relevant features and lots of irrelevant\\nfeatures, knn is likely to do poorly. a related issue with knn is feature scale. suppose that we are\\ntrying to classify whether some object is a ski or a snowboard (see\\nfigure 3.5). we are given two features about this data: the width\\nand height. as is standard in skiing, width is measured in millime-\\nters and height is measured in centimeters. since there are only two\\nfeatures, we can actually plot the entire training set; see figure 3.6\\nwhere ski is the positive class. based on this data, you might guess\\nthat a knn classiﬁer would do well. suppose, however, that our measurement of the width was com- puted in millimeters (instead of centimeters). this yields the data\\nshown in figure 3.7. since the width values are now tiny, in compar-\\nison to the height values, a knn classiﬁer will effectively ignore the\\nwidth values and classify almost purely based on height. the pre-\\ndicted class for the displayed test point had changed because of this\\nfeature scaling. we will discuss feature scaling more in chapter 5. for now, it is just important to keep in mind that knn does not have the power to\\ndecide which features are important. figure 3.5: a ﬁgure of a ski and a\\nsnowboard. figure 3.6: classiﬁcation data for ski vs\\nsnowboard in 2d figure 3.7: classiﬁcation data for ski vs\\nsnowboard in 2d, with width rescaled\\nto mm. 34 a course in machine learning math review vector arithmetic and vector norms\\na (real-valued) vector is just an array of real values, for instance x = (cid:104)1, 2.5,−6(cid:105) is a three-dimensional\\nvector. in general, if x = (cid:104)x1, x2, . . . , xd(cid:105), then xd is it’s dth component. so x3 = −6 in the previous ex-\\nample.\\nvector sums are computed pointwise, and are only deﬁned when dimensions match, so (cid:104)1, 2.5,−6(cid:105) +\\n(cid:104)2,−2.5, 3(cid:105) = (cid:104)3, 0,−3(cid:105). in general, if c = a + b then cd = ad + bd for all d. vector addition can\\nbe viewed geometrically as taking a vector a, then tacking on b to the end of it; the new end point is\\nexactly c.\\nvectors can be scaled by real values; for instance 2(cid:104)1, 2.5,−6(cid:105) = (cid:104)2, 5,−12(cid:105); this is called scalar multi-\\nplication. in general, ax = (cid:104)ax1, ax2, . . . , axd(cid:105).\\nthe norm of a vector x, written x is its length. unless otherwise speciﬁed, this is its euclidean length,\\nnamely: x = (cid:113) ∑d x2\\nd. ',\n",
       " ' figure 3.8: the standard way that we’ve been thinking about learning algo-\\nrithms up to now is in the query model. based on training data, you\\nlearn something. i then give you a query example and you have to\\nguess it’s label. an alternative, less passive, way to think about a learned model\\nis to ask: what sort of test examples will it classify as positive, and\\nwhat sort will it classify as negative. in figure 3.9, we have a set of\\ntraining data. the background of the image is colored blue in regions\\nthat would be classiﬁed as positive (if a query were issued there)\\nand colored red in regions that would be classiﬁed as negative. this\\ncoloring is based on a 1-nearest neighbor classiﬁer. in figure 3.9, there is a solid line separating the positive regions\\nfrom the negative regions. this line is called the decision boundary\\nfor this classiﬁer. it is the line with positive land on one side and\\nnegative land on the other side. decision boundaries are useful ways to visualize the complex- ity of a learned model. intuitively, a learned model with a decision\\nboundary that is really jagged (like the coastline of norway) is really\\ncomplex and prone to overﬁtting. a learned model with a decision\\nboundary that is really simple (like the bounary between arizona\\nand utah) is potentially underﬁt. now that you know about decision boundaries, it is natural to ask: what do decision boundaries for decision trees look like? in order figure 3.9: decision boundary for 1nn. figure 3.10: decision boundary for knn\\nwith k=3. geometry and nearest neighbors 35 to answer this question, we have to be a bit more formal about how\\nto build a decision tree on real-valued features. (remember that the\\nalgorithm you learned in the previous chapter implicitly assumed\\nbinary feature values.) the idea is to allow the decision tree to ask\\nquestions of the form: “is the value of feature 5 greater than 0.2?”\\nthat is, for real-valued features, the decision tree nodes are param-\\neterized by a feature and a threshold for that feature. an example\\ndecision tree for classifying skis versus snowboards is shown in fig-\\nure 3.11. now that a decision tree can handle feature vectors, we can talk\\nabout decision boundaries. by example, the decision boundary for\\nthe decision tree in figure 3.11 is shown in figure 3.12. in the ﬁgure,\\nspace is ﬁrst split in half according to the ﬁrst query along one axis.\\nthen, depending on which half of the space you look at, it is either\\nsplit again along the other axis, or simply classiﬁed. figure 3.12 is a good visualization of decision boundaries for decision trees in general. their decision boundaries are axis-aligned\\ncuts. the cuts must be axis-aligned because nodes can only query on\\na single feature at a time. in this case, since the decision tree was so\\nshallow, the decision boundary was relatively simple. ',\n",
       " ' up through this point, you have learned all about supervised learn-\\ning (in particular, binary classiﬁcation). as another example of the\\nuse of geometric intuitions and data, we are going to temporarily\\nconsider an unsupervised learning problem. in unsupervised learn-\\ning, our data consists only of examples xn and does not contain corre-\\nsponding labels. your job is to make sense of this data, even though\\nno one has provided you with correct labels. the particular notion of\\n“making sense of” that we will talk about now is the clustering task.\\nconsider the data shown in figure 3.13. since this is unsupervised learning and we do not have access to labels, the data points are\\nsimply drawn as black dots. your job is to split this data set into\\nthree clusters. that is, you should label each data point as a, b or c\\nin whatever way you want. for this data set, it’s pretty clear what you should do. you prob- ably labeled the upper-left set of points a, the upper-right set of\\npoints b and the bottom set of points c. or perhaps you permuted\\nthese labels. but chances are your clusters were the same as mine.\\nthe k-means clustering algorithm is a particularly simple and effective approach to producing clusters on data like you see in fig-\\nure 3.13. the idea is to represent each cluster by it’s cluster center.\\ngiven cluster centers, we can simply assign each point to its nearest figure 3.11: decision tree for ski vs.\\nsnowboard figure 3.12: decision boundary for dt in\\nprevious ﬁgure ? what sort of data might yield a\\nvery simple decision boundary with\\na decision tree and very complex\\ndecision boundary with 1-nearest\\nneighbor? what about the other\\nway around? figure 3.13: simple clustering data...\\nclusters in ul, ur and bc. 36 a course in machine learning center. similarly, if we know the assignment of points to clusters, we\\ncan compute the centers. this introduces a chicken-and-egg problem.\\nif we knew the clusters, we could compute the centers. if we knew\\nthe centers, we could compute the clusters. but we don’t know either.\\nthe general computer science answer to chicken-and-egg problems is iteration. we will start with a guess of the cluster centers. based\\non that guess, we will assign each data point to its closest center.\\ngiven these new assignments, we can recompute the cluster centers.\\nwe repeat this process until clusters stop moving. the ﬁrst few it-\\nerations of the k-means algorithm are shown in figure 3.14. in this\\nexample, the clusters converge very quickly. algorithm 3.4 spells out the k-means clustering algorithm in de-\\ntail. the cluster centers are initialized randomly. in line 6, data point\\nxn is compared against each cluster center µk. it is assigned to cluster\\nk if k is the center with the smallest distance. (that is the “argmin”\\nstep.) the variable zn stores the assignment (a value from 1 to k) of\\nexample n. in lines 8-12, the cluster centers are re-computed. first, xk\\nstores all examples that have been assigned to cluster k. the center of\\ncluster k, µk is then computed as the mean of the points assigned to\\nit. this process repeats until the centers converge. an obvious question about this algorithm is: does it converge?\\na second question is: how long does it take to converge. the ﬁrst\\nquestion is actually easy to answer. yes, it does. and in practice, it\\nusually converges quite quickly (usually fewer than 20 iterations). in\\nchapter 15, we will actually prove that it converges. the question of\\nhow long it takes to converge is actually a really interesting question.\\neven though the k-means algorithm dates back to the mid 1950s, the\\nbest known convergence rates were terrible for a long time. here, ter-\\nrible means exponential in the number of data points. this was a sad\\nsituation because empirically we knew that it converged very quickly.\\nnew algorithm analysis techniques called “smoothed analysis” were\\ninvented in 2001 and have been used to show very fast convergence\\nfor k-means (among other algorithms). these techniques are well\\nbeyond the scope of this book (and this author!) but sufﬁce it to say\\nthat k-means is fast in practice and is provably fast in theory. it is important to note that although k-means is guaranteed to converge and guaranteed to converge quickly, it is not guaranteed to\\nconverge to the “right answer.” the key problem with unsupervised\\nlearning is that we have no way of knowing what the “right answer”\\nis. convergence to a bad solution is usually due to poor initialization. figure 3.14: ﬁrst few iterations of\\nk-means running on previous data set ? what is the difference between un-\\nsupervised and supervised learning\\nthat means that we know what the\\n“right answer” is for supervised\\nlearning but not for unsupervised\\nlearning? µk ← some random location algorithm 4 k-means(d, k)\\n1: for k = 1 to k do\\n2:\\n3: end for\\n4: repeat\\n5: for n = 1 to n do zn ← argmink µk − xn end for\\nfor k = 1 to k do\\nxk ← { xn : zn = k }\\nµk ← mean(xk) 6: 7: 8: 9: 10: end for 11:\\n12: until µs stop changing\\n13: return z geometry and nearest neighbors 37 // randomly initialize center for kth cluster // assign example n to closest center // points assigned to cluster k\\n// re-estimate center of cluster k // return cluster assignments ',\n",
       " ' visualizing one hundred dimensional space is incredibly difﬁcult for\\nhumans. after huge amounts of training, some people have reported\\nthat they can visualize four dimensional space in their heads. but\\nbeyond that seems impossible.1 in addition to being hard to visualize, there are at least two addi-\\ntional problems in high dimensions, both refered to as the curse of\\ndimensionality. one is computational, the other is mathematical.\\nfrom a computational perspective, consider the following prob-\\nlem. for k-nearest neighbors, the speed of prediction is slow for a\\nvery large data set. at the very least you have to look at every train-\\ning example every time you want to make a prediction. to speed\\nthings up you might want to create an indexing data structure. you\\ncan break the plane up into a grid like that shown in figure 3.15.\\nnow, when the test point comes in, you can quickly identify the grid\\ncell in which it lies. now, instead of considering all training points,\\nyou can limit yourself to training points in that grid cell (and perhaps\\nthe neighboring cells). this can potentially lead to huge computa-\\ntional savings.\\nspace up into a grid whose cells are 0.2×0.2, we can clearly do this\\nwith 25 grid cells in two dimensions (assuming the range of the\\nfeatures is 0 to 1 for simplicity). in three dimensions, we’ll need\\n125 = 5×5×5 grid cells. in four dimensions, we’ll need 625. by the\\ntime we get to “low dimensional” data in 20 dimensions, we’ll need\\n95, 367, 431, 640, 625 grid cells (that’s 95 trillion, which is about 6 to\\n7 times the us national debt as of january 2011). so if you’re in 20\\ndimensions, this gridding technique will only be useful if you have at\\nleast 95 trillion training examples. in two dimensions, this procedure is effective. if we want to break 1 if you want to try to get an intu-\\nitive sense of what four dimensions\\nlooks like, i highly recommend the\\nshort 1884 book flatland: a romance\\nof many dimensions by edwin abbott\\nabbott. you can even read it online at\\ngutenberg.org/ebooks/201. figure 3.15: 2d knn with an overlaid\\ngrid, cell with test point highlighted 38 a course in machine learning for “medium dimensional” data (approximately 1000) dimesions, the number of grid cells is a 9 followed by 698 numbers before the\\ndecimal point. for comparison, the number of atoms in the universe\\nis approximately 1 followed by 80 zeros. so even if each atom yielded\\na googul training examples, we’d still have far fewer examples than\\ngrid cells. for “high dimensional” data (approximately 100000) di-\\nmensions, we have a 1 followed by just under 70, 000 zeros. far too\\nbig a number to even really comprehend. sufﬁce it to say that for even moderately high dimensions, the\\namount of computation involved in these problems is enormous.\\nin addition to the computational difﬁculties of working in high\\ndimensions, there are a large number of strange mathematical oc-\\ncurances there. in particular, many of your intuitions that you’ve\\nbuilt up from working in two and three dimensions just do not carry\\nover to high dimensions. we will consider two effects, but there are\\ncountless others. the ﬁrst is that high dimensional spheres look more\\nlike porcupines than like balls.2 the second is that distances between\\npoints in high dimensions are all approximately the same. let’s start in two dimensions as in figure 3.16. we’ll start with four green spheres, each of radius one and each touching exactly two\\nother green spheres. (remember that in two dimensions a “sphere”\\nis just a “circle.”) we’ll place a red sphere in the middle so that it\\ntouches all four green spheres. we can easily compute the radius of\\nthis small sphere. the pythagorean theorem says that 12 + 12 = (1 +\\n2 − 1 ≈ 0.41. thus, by calculation,\\nr)2, so solving for r we get r =\\nthe blue sphere lies entirely within the cube (cube = square) that\\ncontains the grey spheres. (yes, this is also obvious from the picture,\\nbut perhaps you can see where this is going.) √ now we can do the same experiment in three dimensions, as shown in figure 3.17. again, we can use the pythagorean theorem\\nto compute the radius of the blue sphere. now, we get 12 + 12 + 12 =\\n3 − 1 ≈ 0.73. this is still entirely enclosed in the\\n(1 + r)2, so r =\\ncube of width four that holds all eight grey spheres. √ at this point it becomes difﬁcult to produce ﬁgures, so you’ll have to apply your imagination. in four dimensions, we would have\\n16 green spheres (called hyperspheres), each of radius one. they\\nwould still be inside a cube (called a hypercube) of width four. the\\n4 − 1 = 1. continuing\\nblue hypersphere would have radius r =\\n√\\nto ﬁve dimensions, the blue hypersphere embedded in 256 green\\nhyperspheres would have radius r = 5 − 1 ≈ 1.23 and so on. √ in general, in d-dimensional space, there will be 2d green hyper- spheres of radius one. each green hypersphere will touch exactly\\nn-many other hyperspheres. the blue hyperspheres in the middle\\nwill touch them all and will have radius r = d − 1. √ ? how does the above analysis relate\\nto the number of data points you\\nwould need to ﬁll out a full decision\\ntree with d-many features? what\\ndoes this say about the importance\\nof shallow trees? 2 this result was related to me by mark\\nreid, who heard about it from marcus\\nhutter. figure 3.16: 2d spheres in spheres figure 3.17: 3d spheres in spheres geometry and nearest neighbors 39 think about this for a moment. as the number of dimensions grows, the radius of the blue hypersphere grows without bound!. for\\n√\\nexample, in 9-dimensions the radius of the blue hypersphere is now\\n9 − 1 = 2. but with a radius of two, the blue hypersphere is now\\n“squeezing” between the green hypersphere and touching the edges\\nof the hypercube. in 10 dimensional space, the radius is approxi-\\nmately 2.16 and it pokes outside the cube. the second strange fact we will consider has to do with the dis-\\ntances between points in high dimensions. we start by considering\\nrandom points in one dimension. that is, we generate a fake data set\\nconsisting of 100 random points between zero and one. we can do\\nthe same in two dimensions and in three dimensions. see figure ??\\nfor data distributed uniformly on the unit hypercube in different\\ndimensions. now, pick two of these points at random and compute the dis-\\ntance between them. repeat this process for all pairs of points and\\naverage the results. for the data shown in figure ??, the average\\ndistance between points in one dimension is about 0.346; in two di-\\nmensions is about 0.518; and in three dimensions is 0.615. the fact\\nthat these increase as the dimension increases is not surprising. the\\nfurthest two points can be in a 1-dimensional hypercube (line) is 1;\\nthe furthest in a 2-dimensional hypercube (square) is\\n2 (opposite\\ncorners); the furthest in a 3-d hypercube is\\nthe furthest two points in a d-dimensional hypercube will be √\\nd.\\nyou can actually compute these values analytically. write unid\\nfor the uniform distribution in d dimensions. the quantity we are\\ninterested in computing is: √ √ 3 and so on. in general, (cid:104) (cid:104) a − b(cid:105)(cid:105) avgdist(d) = ea∼unid eb∼unid (3.2) √\\nwe can actually compute this in closed form and arrive at avgdist(d) = d/3. because we know that the maximum distance between two\\nd, this says that the ratio between average dis- points grows like\\ntance and maximum distance converges to 1/3. √ √ what is more interesting, however, is the variance of the distribu-\\ntion of distances. you can show that in d dimensions, the variance\\n18, independent of d. this means that when you look\\nis constant 1/\\nat (variance) divided-by (max distance), the variance behaves like\\n1/\\nas d grows3. 18d, which means that the effective variance continues to shrink √ when i ﬁrst saw and re-proved this result, i was skeptical, as i\\nimagine you are. so i implemented it. in figure 3.18 you can see\\nthe results. this presents a histogram of distances between random\\npoints in d dimensions for d ∈ {1, 2, 3, 10, 20, 100}. as you can see,\\nd, even for\\nall of these distances begin to concentrate around 0.4 √ 3 brin 1995 figure 3.18: histogram of distances in\\nd=2,8,32,128,512 0.00.20.40.60.81.0distance / sqrt(dimensionality)02000400060008000100001200014000# of pairs of points at that distancedimensionality versus uniform point distances2 dims8 dims32 dims128 dims512 dims40 a course in machine learning “medium dimension” problems. you should now be terriﬁed: the only bit of information that knn gets is distances. and you’ve just seen that in moderately high di-\\nmensions, all distances becomes equal. so then isn’t it the case that\\nknn simply cannot work? the answer has to be no. the reason is that the data that we get is not uniformly distributed over the unit hypercube. we can see this\\nby looking at two real-world data sets. the ﬁrst is an image data set\\nof hand-written digits (zero through nine); see section ??. although\\nthis data is originally in 256 dimensions (16 pixels by 16 pixels), we\\ncan artiﬁcally reduce the dimensionality of this data. in figure 3.19\\nyou can see the histogram of average distances between points in this\\ndata at a number of dimensions. as you can see from these histograms, distances have not con-\\ncentrated around a single value. this is very good news: it means\\nthat there is hope for learning algorithms to work! nevertheless, the\\nmoral is that high dimensions are weird. 3.6 further reading todo further reading figure 3.19: knn:mnist: histogram of\\ndistances in multiple d for mnist algebra is nothing more than geometry, in words; geometry is\\nnothing more than algebra, in pictures. – sophie germain so far, you’ve seen two types of learning models: in decision\\ntrees, only a small number of features are used to make decisions; in\\nnearest neighbor algorithms, all features are used equally. neither of\\nthese extremes is always desirable. in some problems, we might want\\nto use most of the features, but use some more than others. in this chapter, we’ll discuss the perceptron algorithm for learn-\\ning weights for features. as we’ll see, learning weights for features\\namounts to learning a hyperplane classiﬁer: that is, basically a di-\\nvision of space into two halves by a straight line, where one half is\\n“positive” and one half is “negative.” in this sense, the perceptron\\ncan be seen as explicitly ﬁnding a good linear decision boundary. learning objectives:\\n• describe the biological motivation behind the perceptron. • classify learning algorithms based\\non whether they are error-driven or\\nnot. • implement the perceptron algorithm for binary classiﬁcation. • draw perceptron weight vectors\\nand the corresponding decision\\nboundaries in two dimensions. • contrast the decision boundaries\\nof decision trees, nearest neighbor\\nalgorithms and perceptrons. • compute the margin of a given weight vector on a given data set. ',\n",
       " ' dependencies: chapter 1, chapter 3 folk biology tells us that our brains are made up of a bunch of little\\nunits, called neurons, that send electrical signals to one another. the\\nrate of ﬁring tells us how “activated” a neuron is. a single neuron,\\nlike that shown in figure 4.1 might have three incoming neurons.\\nthese incoming neurons are ﬁring at different rates (i.e., have dif-\\nferent activations). based on how much these incoming neurons are\\nﬁring, and how “strong” the neural connections are, our main neu-\\nron will “decide” how strongly it wants to ﬁre. and so on through\\nthe whole brain. learning in the brain happens by neurons becom-\\nming connected to other neurons, and the strengths of connections\\nadapting over time. the real biological world is much more complicated than this.\\nhowever, our goal isn’t to build a brain, but to simply be inspired\\nby how they work. we are going to think of our learning algorithm\\nas a single neuron. it receives input from d-many other neurons,\\none for each input feature. the strength of these inputs are the fea-\\nture values. this is shown schematically in figure 4.1. each incom-\\ning connection has a weight and the neuron simply sums up all the\\nweighted inputs. based on this sum, it decides whether to “ﬁre” or figure 4.1: a picture of a neuron figure 4.2: ﬁgure showing feature\\nvector and weight vector and products\\nand sum 42 a course in machine learning not. firing is interpreted as being a positive example and not ﬁring is\\ninterpreted as being a negative example. in particular, if the weighted\\nsum is positive, it “ﬁres” and otherwise it doesn’t ﬁre. this is shown\\ndiagramatically in figure 4.2.\\nmathematically, an input vector x = (cid:104)x1, x2, . . . , xd(cid:105) arrives. the neuron stores d-many weights, w1, w2, . . . , wd. the neuron computes\\nthe sum:\\nd∑ a = (4.1) wdxd d=1 to determine it’s amount of “activation.” if this activiation is posi-\\ntive (i.e., a > 0) it predicts that this example is a positive example.\\notherwise it predicts a negative example. the weights of this neuron are fairly easy to interpret. suppose that a feature, for instance “is this a system’s class?” gets a zero\\nweight. then the activation is the same regardless of the value of\\nthis feature. so features with zero weight are ignored. features with\\npositive weights are indicative of positive examples because they\\ncause the activation to increase. features with negative weights are\\nindicative of negative examples because they cause the activiation to\\ndecrease. it is often convenient to have a non-zero threshold. in other words, we might want to predict positive if a > θ for some value\\nθ. the way that is most convenient to achieve this is to introduce a\\nbias term into the neuron, so that the activation is always increased\\nby some ﬁxed value b. thus, we compute: (4.2) (cid:35) (cid:34) d∑ d=1 a = wdxd + b ? what would happen if we encoded\\nbinary features like “is this a sys-\\ntem’s class” as no=0 and yes=−1\\n(rather than the standard no=0 and\\nyes=+1)? this is the complete neural model of learning. the model is pa-\\nrameterized by d-many weights, w1, w2, . . . , wd, and a single scalar\\nbias value b. ? if you wanted the activation thresh-\\nold to be a > θ instead of a > 0,\\nwhat value would b have to be? ',\n",
       " ' the perceptron is a classic learning algorithm for the neural model\\nof learning. like k-nearest neighbors, it is one of those frustrating\\nalgorithms that is incredibly simple and yet works amazingly well,\\nfor some types of problems. the algorithm is actually quite different than either the decision\\ntree algorithm or the knn algorithm. first, it is online. this means\\nthat instead of considering the entire data set at the same time, it only\\never looks at one example. it processes that example and then goes the perceptron 43 // initialize weights\\n// initialize bias algorithm 5 perceptrontrain(d, maxiter)\\n1: wd ← 0, for all d = 1 . . . d\\n2: b ← 0\\n3: for iter = 1 . . . maxiter do\\n4: for all (x,y) ∈ d do d=1 wd xd + b a ← ∑d\\nif ya ≤ 0 then\\nwd ← wd + yxd, for all d = 1 . . . d\\nb ← b + y 5: 6: 7: 8: 9: // compute activation for this example // update weights\\n// update bias end if\\nend for 10:\\n11: end for\\n12: return w0, w1, . . . , wd, b algorithm 6 perceptrontest(w0, w1, . . . , wd, b, ˆx)\\n1: a ← ∑d\\n2: return sign(a) d=1 wd ˆxd + b // compute activation for the test example on to the next one. second, it is error driven. this means that, so\\nlong as it is doing well, it doesn’t bother updating its parameters.\\nthe algorithm maintains a “guess” at good parameters (weights\\nand bias) as it runs. it processes one example at a time. for a given\\nexample, it makes a prediction. it checks to see if this prediction\\nis correct (recall that this is training data, so we have access to true\\nlabels). if the prediction is correct, it does nothing. only when the\\nprediction is incorrect does it change its parameters, and it changes\\nthem in such a way that it would do better on this example next\\ntime around. it then goes on to the next example. once it hits the\\nlast example in the training set, it loops back around for a speciﬁed\\nnumber of iterations. the training algorithm for the perceptron is shown in algo- rithm 4.2 and the corresponding prediction algorithm is shown in\\nalgorithm 4.2. there is one “trick” in the training algorithm, which\\nprobably seems silly, but will be useful later. it is in line 6, when we\\ncheck to see if we want to make an update or not. we want to make\\nan update if the current prediction (just sign(a)) is incorrect. the\\ntrick is to multiply the true label y by the activation a and compare\\nthis against zero. since the label y is either +1 or −1, you just need\\nto realize that ya is positive whenever a and y have the same sign.\\nin other words, the product ya is positive if the current prediction is\\ncorrect. the particular form of update for the perceptron is quite simple. the weight wd is increased by yxd and the bias is increased by y. the\\ngoal of the update is to adjust the parameters so that they are “bet-\\nter” for the current example. in other words, if we saw this example ? it is very very important to check\\nya ≤ 0 rather than ya < 0. why? 44 a course in machine learning twice in a row, we should do a better job the second time around. to see why this particular update achieves this, consider the fol- lowing scenario. we have some current set of parameters w1, . . . , wd, b.\\nwe observe an example (x, y). for simplicity, suppose this is a posi-\\ntive example, so y = +1. we compute an activation a, and make an\\nerror. namely, a < 0. we now update our weights and bias. let’s call\\nthe new weights w(cid:48)\\nd, b(cid:48). suppose we observe the same exam-\\nple again and need to compute a new activation a(cid:48). we proceed by a\\nlittle algebra: 1, . . . , w(cid:48) a(cid:48) = = = d∑ d=1 d∑ d=1 d∑ d=1 w(cid:48)\\ndxd + b(cid:48) (wd + xd)xd + (b + 1) wdxd + b + d∑ d=1 xdxd + 1 = a + d∑ d=1 x2\\nd + 1 > a (4.3) (4.4) (4.5) (4.6) d + 1. but x2 so the difference between the old activation a and the new activa-\\nd ≥ 0, since it’s squared. so this value is\\ntion a(cid:48) is ∑d x2\\nalways at least one. thus, the new activation is always at least the old\\nactivation plus one. since this was a positive example, we have suc-\\ncessfully moved the activation in the proper direction. (though note\\nthat there’s no guarantee that we will correctly classify this point the\\nsecond, third or even fourth time around!) the only hyperparameter of the perceptron algorithm is maxiter, the number of passes to make over the training data. if we make\\nmany many passes over the training data, then the algorithm is likely\\nto overﬁt. (this would be like studying too long for an exam and just\\nconfusing yourself.) on the other hand, going over the data only\\none time might lead to underﬁtting. this is shown experimentally in\\nfigure 4.3. the x-axis shows the number of passes over the data and\\nthe y-axis shows the training error and the test error. as you can see,\\nthere is a “sweet spot” at which test performance begins to degrade\\ndue to overﬁtting. one aspect of the perceptron algorithm that is left underspeciﬁed\\nis line 4, which says: loop over all the training examples. the natural\\nimplementation of this would be to loop over them in a constant\\norder. the is actually a bad idea. consider what the perceptron algorithm would do on a data set that consisted of 500 positive examples followed by 500 negative\\nexamples. after seeing the ﬁrst few positive examples (maybe ﬁve),\\nit would likely decide that every example is positive, and would stop ? this analysis hold for the case pos-\\nitive examples (y = +1). it should\\nalso hold for negative examples.\\nwork it out. figure 4.3: training and test error via\\nearly stopping learning anything. it would do well for a while (next 495 examples),\\nuntil it hit the batch of negative examples. then it would take a while\\n(maybe ten examples) before it would start predicting everything as\\nnegative. by the end of one pass through the data, it would really\\nonly have learned from a handful of examples (ﬁfteen in this case). so one thing you need to avoid is presenting the examples in some\\nﬁxed order. this can easily be accomplished by permuting the order\\nof examples once in the beginning and then cycling over the data set\\nin the same (permuted) order each iteration. however, it turns out\\nthat you can actually do better if you re-permute the examples in each\\niteration. figure 4.4 shows the effect of re-permuting on convergence\\nspeed. in practice, permuting each iteration tends to yield about 20%\\nsavings in number of iterations. in theory, you can actually prove that\\nit’s expected to be about twice as fast. ',\n",
       " ' the perceptron 45 figure 4.4: training and test error for\\npermuting versus not-permuting ? if permuting the data each iteration\\nsaves somewhere between 20% and\\n50% of your time, are there any\\ncases in which you might not want\\nto permute the data every iteration? a question you should be asking yourself by now is: what does the\\ndecision boundary of a perceptron look like? you can actually answer\\nthat question mathematically. for a perceptron, the decision bound-\\nary is precisely where the sign of the activation, a, changes from −1\\nto +1. in other words, it is the set of points x that achieve zero ac-\\ntivation. the points that are not clearly positive nor negative. for\\nsimplicity, we’ll ﬁrst consider the case where there is no “bias” term\\n(or, equivalently, the bias is zero). formally, the decision boundary b\\nis: (cid:40) (cid:41) b = x : ∑\\nd wdxd = 0 (4.7) we can now apply some linear algebra. recall that ∑d wdxd is just\\nthe dot product between the vector w = (cid:104)w1, w2, . . . , wd(cid:105) and the\\nvector x. we will write this as w · x. two vectors have a zero dot\\nproduct if and only if they are perpendicular. thus, if we think of\\nthe weights as a vector w, then the decision boundary is simply the\\nplane perpendicular to w. 46 a course in machine learning math review dot products\\ngiven two vectors u and v their dot product u · v is ∑d udvd. the dot product\\ngrows large and positive when u and v point in same direction, grows large\\nand negative when u and v point in opposite directions, and is zero when\\ntheir are perpendicular. a useful geometric interpretation of dot products is\\nprojection. suppose u = 1, so that u is a unit vector. we can think of any\\nother vector v as consisting of two components: (a) a component in the di-\\nrection of u and (b) a component that’s perpendicular to u. this is depicted\\ngeometrically to the right: here, u = (cid:104)0.8, 0.6(cid:105) and v = (cid:104)0.37, 0.73(cid:105). we\\ncan think of v as the sum of two vectors, a and b, where a is parallel to u and b is perpendicular. the\\nlength of b is exactly u · v = 0.734, which is why you can think of dot products as projections: the dot\\nproduct between u and v is the “projection of v onto u.” figure 4.5: this is shown pictorially in figure 4.6. here, the weight vector is\\nshown, together with it’s perpendicular plane. this plane forms the\\ndecision boundary between positive points and negative points. the\\nvector points in the direction of the positive examples and away from\\nthe negative examples. one thing to notice is that the scale of the weight vector is irrele- vant from the perspective of classiﬁcation. suppose you take a weight\\nvector w and replace it with 2w. all activations are now doubled.\\nbut their sign does not change. this makes complete sense geometri-\\ncally, since all that matters is which side of the plane a test point falls\\non, now how far it is from that plane. for this reason, it is common\\nto work with normalized weight vectors, w, that have length one; i.e.,\\nw = 1.\\nthe geometric intuition can help us even more when we realize\\nthat dot products compute projections. that is, the value w · x is\\njust the distance of x from the origin when projected onto the vector\\nw. this is shown in figure 4.7. in that ﬁgure, all the data points are\\nprojected onto w. below, we can think of this as a one-dimensional\\nversion of the data, where each data point is placed according to its\\nprojection along w. this distance along w is exactly the activiation of\\nthat example, with no bias. from here, you can start thinking about the role of the bias term. previously, the threshold would be at zero. any example with a\\nnegative projection onto w would be classiﬁed negative; any exam-\\nple with a positive projection, positive. the bias simply moves this\\nthreshold. now, after the projection is computed, b is added to get\\nthe overall activation. the projection plus b is then compared against figure 4.6: picture of data points with\\nhyperplane and weight vector ? if i give you a non-zero weight vec-\\ntor w, how do i compute a weight\\nvector w(cid:48) that points in the same\\ndirection but has a norm of one? figure 4.7: the same picture as before,\\nbut with projections onto weight vector;\\nthen, below, those points along a one-\\ndimensional axis with zero marked. uv}a}bthe perceptron 47 figure 4.8: perc:bias: perceptron\\npicture with bias zero. thus, from a geometric perspective, the role of the bias is to shift\\nthe decision boundary away from the origin, in the direction of w. it\\nis shifted exactly −b units. so if b is positive, the boundary is shifted\\naway from w and if b is negative, the boundary is shifted toward w.\\nthis is shown in figure 4.8. this makes intuitive sense: a positive\\nbias means that more examples should be classiﬁed positive. by\\nmoving the decision boundary in the negative direction, more space\\nyields a positive classiﬁcation.\\nd dimensional space, it is always a d − 1-dimensional hyperplane.\\n(in two dimensions, a 1-d hyperplane is simply a line. in three di-\\nmensions, a 2-d hyperplane is like a sheet of paper.) this hyperplane\\ndivides space in half. in the rest of this book, we’ll refer to the weight\\nvector, and to hyperplane it deﬁnes, interchangeably. the decision boundary for a perceptron is a very magical thing. in the perceptron update can also be considered geometrically. (for simplicity, we will consider the unbiased case.) consider the sit-\\nuation in figure 4.9. here, we have a current guess as to the hy-\\nperplane, and positive training example comes in that is currently\\nmis-classiﬁed. the weights are updated: w ← w + yx. this yields the\\nnew weight vector, also shown in the figure. in this case, the weight\\nvector changed enough that this training example is now correctly\\nclassiﬁed. ',\n",
       " ' figure 4.9: perceptron picture with\\nupdate, no bias you may ﬁnd yourself having run the perceptron, learned a really\\nawesome classiﬁer, and then wondering “what the heck is this clas-\\nsiﬁer doing?” you might ask this question because you’re curious to\\nlearn something about the underlying data. you might ask this ques-\\ntion because you want to make sure that the perceptron is learning\\n“the right thing.” you might ask this question because you want to\\nremove a bunch of features that aren’t very useful because they’re\\nexpensive to compute or take a lot of storage. the perceptron learns a classiﬁer of the form x (cid:55)→ sign (∑d wdxd + b). a reasonable question to ask is: how sensitive is the ﬁnal classiﬁca-\\ntion to small changes in some particular feature. we can answer this\\nquestion by taking a derivative. if we arbitrarily take the 7th fea-\\n(∑d wdxd + b) = w7. this says: the rate at\\nture we can compute ∂\\n∂x7\\nwhich the activation changes as a function of the 7th feature is ex-\\nactly w7. this gives rise to a useful heuristic for interpreting percep-\\ntron weights: sort all the weights from largest (positive) to largest\\n(negative), and take the top ten and bottom ten. the top ten are the\\nfeatures that the perceptron is most sensitive to for making positive 48 a course in machine learning predictions. the bottom ten are the features that the perceptron is\\nmost sensitive to for making negative predictions. this heuristic is useful, especially when the inputs x consist en- tirely of binary values (like a bag of words representation). the\\nheuristic is less useful when the range of the individual features\\nvaries signiﬁcantly. the issue is that if you have one feat x5 that’s ei-\\nther 0 or 1, and another feature x7 that’s either 0 or 100, but w5 = w7,\\nit’s reasonable to say that w7 is more important because it is likely to\\nhave a much larger inﬂuence on the ﬁnal prediction. the easiest way\\nto compensate for this is simply to scale your features ahead of time:\\nthis is another reason why feature scaling is a useful preprocessing\\nstep. ',\n",
       " ' you already have an intuitive feeling for why the perceptron works:\\nit moves the decision boundary in the direction of the training exam-\\nples. a question you should be asking yourself is: does the percep-\\ntron converge? if so, what does it converge to? and how long does it\\ntake? it is easy to construct data sets on which the perceptron algorithm\\nwill never converge. in fact, consider the (very uninteresting) learn-\\ning problem with no features. you have a data set consisting of one\\npositive example and one negative example. since there are no fea-\\ntures, the only thing the perceptron algorithm will ever do is adjust\\nthe bias. given this data, you can run the perceptron for a bajillion\\niterations and it will never settle down. as long as the bias is non-\\nnegative, the negative example will cause it to decrease. as long as\\nit is non-positive, the positive example will cause it to increase. ad\\ninﬁnitum. (yes, this is a very contrived example.) what does it mean for the perceptron to converge? it means that\\nit can make an entire pass through the training data without making\\nany more updates. in other words, it has correctly classiﬁed every\\ntraining example. geometrically, this means that it was found some\\nhyperplane that correctly segregates the data into positive and nega-\\ntive examples, like that shown in figure 4.10. in this case, this data is linearly separable. this means that there exists some hyperplane that puts all the positive examples on one side\\nand all the negative examples on the other side. if the training is not\\nlinearly separable, like that shown in figure 4.11, then the perceptron\\nhas no hope of converging. it could never possibly classify each point\\ncorrectly. the somewhat surprising thing about the perceptron algorithm is\\nthat if the data is linearly separable, then it will converge to a weight figure 4.10: separable data figure 4.11: inseparable data the perceptron 49 vector that separates the data. (and if the data is inseparable, then it\\nwill never converge.) this is great news. it means that the perceptron\\nconverges whenever it is even remotely possible to converge. the second question is: how long does it take to converge? by “how long,” what we really mean is “how many updates?” as is the\\ncase for much learning theory, you will not be able to get an answer\\nof the form “it will converge after 5293 updates.” this is asking too\\nmuch. the sort of answer we can hope to get is of the form “it will\\nconverge after at most 5293 updates.” what you might expect to see is that the perceptron will con- verge more quickly for easy learning problems than for hard learning\\nproblems. this certainly ﬁts intuition. the question is how to deﬁne\\n“easy” and “hard” in a meaningful way. one way to make this def-\\ninition is through the notion of margin. if i give you a data set and\\nhyperplane that separates itthen the margin is the distance between\\nthe hyperplane and the nearest point. intuitively, problems with large\\nmargins should be easy (there’s lots of “wiggle room” to ﬁnd a sepa-\\nrating hyperplane); and problems with small margins should be hard\\n(you really have to get a very speciﬁc well tuned weight vector).\\nformally, given a data set d, a weight vector w and bias b, the margin of w, b on d is deﬁned as: (cid:40) min(x,y)∈d y(cid:0)w · x + b(cid:1) margin(d, w, b) = −∞ if w separates d\\notherwise (4.8) in words, the margin is only deﬁned if w, b actually separate the data\\n(otherwise it is just −∞). in the case that it separates the data, we\\nﬁnd the point with the minimum activation, after the activation is\\nmultiplied by the label. for some historical reason (that is unknown to the author), mar-\\ngins are always denoted by the greek letter γ (gamma). one often\\ntalks about the margin of a data set. the margin of a data set is the\\nlargest attainable margin on this data. formally: margin(d) = sup\\nw,b margin(d, w, b) (4.9) in words, to compute the margin of a data set, you “try” every possi-\\nble w, b pair. for each pair, you compute its margin. we then take the\\nlargest of these as the overall margin of the data.1 if the data is not\\nlinearly separable, then the value of the sup, and therefore the value\\nof the margin, is −∞. there is a famous theorem due to rosenblatt2 that shows that the\\nnumber of errors that the perceptron algorithm makes is bounded by\\nγ−2. more formally: so long as the margin is not −∞,\\nit is always positive. geometrically\\nthis makes sense, but why does\\neq (4.8) yield this? ? 1 you can read “sup” as “max” if you\\nlike: the only difference is a technical\\ndifference in how the −∞ case is\\nhandled.\\n2 rosenblatt 1958 50 a course in machine learning theorem 2 (perceptron convergence theorem). suppose the perceptron\\nalgorithm is run on a linearly separable data set d with margin γ > 0.\\nassume that x ≤ 1 for all x ∈ d. then the algorithm will converge after\\nat most 1 γ2 updates. the proof of this theorem is elementary, in the sense that it does not use any fancy tricks: it’s all just algebra. the idea behind the\\nproof is as follows. if the data is linearly separable with margin γ,\\nthen there exists some weight vector w∗ that achieves this margin.\\nobviously we don’t know what w∗ is, but we know it exists. the\\nperceptron algorithm is trying to ﬁnd a weight vector w that points\\nroughly in the same direction as w∗. (for large γ, “roughly” can be\\nvery rough. for small γ, “roughly” is quite precise.) every time the\\nperceptron makes an update, the angle between w and w∗ changes.\\nwhat we prove is that the angle actually decreases. we show this in\\ntwo steps. first, the dot product w · w∗ increases a lot. second, the\\nnorm w does not increase very much. since the dot product is\\nincreasing, but w isn’t getting too long, the angle between them has\\nto be shrinking. the rest is algebra. proof of theorem 2. the margin γ > 0 must be realized by some set\\nof parameters, say x∗. suppose we train a perceptron on this data.\\ndenote by w(0) the initial weight vector, w(1) the weight vector after\\nthe ﬁrst update, and w(k) the weight vector after the kth update. (we\\nare essentially ignoring data points on which the perceptron doesn’t\\nupdate itself.) first, we will show that w∗ · w(k) grows quicky as a function of k. second, we will show that(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12) does not grow quickly. first, suppose that the kth update happens on example (x, y). we\\nare trying to show that w(k) is becoming aligned with w∗. because we\\nupdated, know that this example was misclassiﬁed: yw(k-1) · x < 0.\\nafter the update, we get w(k) = w(k-1) + yx. we do a little computa-\\ntion: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 w∗ · w(k) = w∗ ·(cid:16) (cid:17)\\n= w∗ · w(k-1) + yw∗ · x\\n≥ w∗ · w(k-1) + γ w(k-1) + yx deﬁnition of w(k) (4.10)\\n(4.11)\\n(4.12)\\nthus, every time w(k) is updated, its projection onto w∗ increases by\\nat least γ. therefore: w∗ · w(k) ≥ kγ.\\nbecause w(k) is getting closer to w∗, not just because it’s getting ex-\\nceptionally long. to do this, we compute the norm of w(k): next, we need to show that the increase of γ along w∗ occurs vector algebra\\nw∗ has margin γ def. of w(k) quadratic rule (4.14) assumption and a < 0 (4.15) = = + 1 + 0 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 + y2 x2 + 2yw(k-1) · x (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k-1) + yx\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k-1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2\\n≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k-1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2\\ndate. therefore: (cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)2 ≤ k.\\nthat(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12) ≥ w∗ · w(k). putting this together, we have:\\nk ≥ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ w∗ · w(k) ≥ kγ √ now we put together the two things we have learned before. by\\nour ﬁrst conclusion, we know w∗ · w(k) ≥ kγ. but our second con-\\nclusion, k ≥(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)2. finally, because w∗ is a unit vector, we know √ thus, the squared norm of w(k) increases by at most one every up- the perceptron 51 (4.13) taking the left-most and right-most terms, we get that\\ndividing both sides by k, we get 1√\\nk\\nthis means that once we’ve made 1\\nγ2 updates, we cannot make any\\nmore! k ≥ kγ.\\n≥ γ and therefore k ≤ 1\\nγ2 . (4.16) √ it is important to keep in mind what this proof shows and what it does not show. it shows that if i give the perceptron data that\\nis linearly separable with margin γ > 0, then the perceptron will\\nconverge to a solution that separates the data. and it will converge\\nquickly when γ is large. it does not say anything about the solution,\\nother than the fact that it separates the data. in particular, the proof\\nmakes use of the maximum margin separator. but the perceptron\\nis not guaranteed to ﬁnd this maximum margin separator. the data\\nmay be separable with margin 0.9 and the perceptron might still\\nﬁnd a separating hyperplane with a margin of only 0.000001. later\\n(in chapter 7), we will see algorithms that explicitly try to ﬁnd the\\nmaximum margin solution. ',\n",
       " ' in the beginning of this chapter, there was a comment that the per-\\nceptron works amazingly well. this was a half-truth. the “vanilla”\\nperceptron algorithm does well, but not amazingly well. in order to\\nmake it more competitive with other learning algorithms, you need\\nto modify it a bit to get better generalization. the key issue with the\\nvanilla perceptron is that it counts later points more than it counts earlier\\npoints. to see why, consider a data set with 10, 000 examples. suppose that after the ﬁrst 100 examples, the perceptron has learned a really ? perhaps we don’t want to assume\\nthat all x have norm at most 1. if\\nthey have all have norm at most\\nr, you can achieve a very simi-\\nlar bound. modify the perceptron\\nconvergence proof to handle this\\ncase. ? why does the perceptron conver-\\ngence bound not contradict the\\nearlier claim that poorly ordered\\ndata points (e.g., all positives fol-\\nlowed by all negatives) will cause\\nthe perceptron to take an astronom-\\nically long time to learn? 52 a course in machine learning good classiﬁer. it’s so good that it goes over the next 9899 exam-\\nples without making any updates. it reaches the 10, 000th example\\nand makes an error. it updates. for all we know, the update on this\\n10, 000th example completely ruins the weight vector that has done so\\nwell on 99.99% of the data! what we would like is for weight vectors that “survive” a long time to get more say than weight vectors that are overthrown quickly.\\none way to achieve this is by voting. as the perceptron learns, it\\nremembers how long each hyperplane survives. at test time, each\\nhyperplane encountered during training “votes” on the class of a test\\nexample. if a particular hyperplane survived for 20 examples, then\\nit gets a vote of 20. if it only survived for one example, it only gets a\\nvote of 1. in particular, let (w, b)(1), . . . , (w, b)(k) be the k + 1 weight\\nvectors encountered during training, and c(1), . . . , c(k) be the survival\\ntimes for each of these weight vectors. (a weight vector that gets\\nimmediately updated gets c = 1; one that survives another round\\ngets c = 2 and so on.) then the prediction on a test point is: (cid:32) k∑ (cid:16) w(k) · ˆx + b(k)(cid:17)(cid:33) ˆy = sign c(k)sign (4.17) k=1 this algorithm, known as the voted perceptron works quite well in\\npractice, and there is some nice theory showing that it is guaranteed\\nto generalize better than the vanilla perceptron. unfortunately, it is\\nalso completely impractical. if there are 1000 updates made during\\nperceptron learning, the voted perceptron requires that you store\\n1000 weight vectors, together with their counts. this requires an\\nabsurd amount of storage, and makes prediction 1000 times slower\\nthan the vanilla perceptron. a much more practical alternative is the averaged perceptron. the idea is similar: you maintain a collection of weight vectors and\\nsurvival times. however, at test time, you predict according to the\\naverage weight vector, rather than the voting. in particular, the predic-\\ntion is: c(k)(cid:16) w(k) · ˆx + b(k)(cid:17)(cid:33) (cid:32) k∑ k=1 ˆy = sign the only difference between the voted prediction, eq (4.17), and the\\naveraged prediction, eq (4.18), is the presense of the interior sign\\noperator. with a little bit of algebra, we can rewrite the test-time\\nprediction as: (cid:33) (4.18) (4.19) (cid:33) (cid:32)(cid:32) k∑ k=1 ˆy = sign c(k)w(k) · ˆx + k∑ k=1 c(k)b(k) the advantage of the averaged perceptron is that we can simply\\nmaintain a running sum of the averaged weight vector (the blue term) ? the training algorithm for the voted\\nperceptron is the same as the\\nvanilla perceptron. in particular,\\nin line 5 of algorithm 4.2, the ac-\\ntivation on a training example is\\ncomputed based on the current\\nweight vector, not based on the voted\\nprediction. why? the perceptron 53 algorithm 7 averagedperceptrontrain(d, maxiter)\\n1: w ← (cid:104)0, 0, . . . 0(cid:105)\\n2: u ← (cid:104)0, 0, . . . 0(cid:105)\\n3: c ← 1\\n4: for iter = 1 . . . maxiter do\\n5: b ← 0\\n,\\n, β ← 0 // initialize weights and bias\\n// initialize cached weights and bias\\n// initialize example counter to one if y(w · x + b) ≤ 0 then for all (x,y) ∈ d do\\nw ← w + y x\\nb ← b + y\\nu ← u + y c x\\nβ ← β + y c 6: 7: 8: 9: 10: 11: 12: end if\\nc ← c + 1 end for 13:\\n14: end for\\n15: return w - 1 c u, b - 1 c β // update weights\\n// update bias\\n// update cached weights\\n// update cached bias // increment counter regardless of update // return averaged weights and bias and averaged bias (the red term). test-time prediction is then just as\\nefﬁcient as it is with the vanilla perceptron. the full training algorithm for the averaged perceptron is shown\\nin algorithm 4.6. some of the notation is changed from the original\\nperceptron: namely, vector operations are written as vector opera-\\ntions, and the activation computation is folded into the error check-\\ning. it is probably not immediately apparent from algorithm 4.6 that the computation unfolding is precisely the calculation of the averaged\\nweights and bias. the most natural implementation would be to keep\\ntrack of an averaged weight vector u. at the end of every example,\\nyou would increase u ← u + w (and similarly for the bias). however,\\nsuch an implementation would require that you updated the aver-\\naged vector on every example, rather than just on the examples that\\nwere incorrectly classiﬁed! since we hope that eventually the per-\\nceptron learns to do a good job, we would hope that it will not make\\nupdates on every example. so, ideally, you would like to only update\\nthe averaged weight vector when the actual weight vector changes.\\nthe slightly clever computation in algorithm 4.6 achieves this. the averaged perceptron is almost always better than the percep-\\ntron, in the sense that it generalizes better to test data. however, that\\ndoes not free you from having to do early stopping. it will, eventu-\\nally, overﬁt. ',\n",
       " ' although the perceptron is very useful, it is fundamentally limited in\\na way that neither decision trees nor knn are. its limitation is that ? by writing out the computation of\\nthe averaged weights from eq (4.18)\\nas a telescoping sum, derive the\\ncomputation from algorithm 4.6. 54 a course in machine learning its decision boundaries can only be linear. the classic way of showing\\nthis limitation is through the xor problem (xor = exclusive or). the\\nxor problem is shown graphically in figure 4.12. it consists of four\\ndata points, each at a corner of the unit square. the labels for these\\npoints are the same, along the diagonals. you can try, but you will\\nnot be able to ﬁnd a linear decision boundary that perfectly separates\\nthese data points. one question you might ask is: do xor-like problems exist in the real world? unfortunately for the perceptron, the answer is yes.\\nconsider a sentiment classiﬁcation problem that has three features\\nthat simply say whether a given word is contained in a review of\\na course. these features are: excellent, terrible and not. the\\nexcellent feature is indicative of positive reviews and the terrible\\nfeature is indicative of negative reviews. but in the presence of the\\nnot feature, this categorization ﬂips. one way to address this problem is by adding feature combina-\\ntions. we could add two additional features: excellent-and-not\\nand terrible-and-not that indicate a conjunction of these base\\nfeatures. by assigning weights as follows, you can achieve the desired\\neffect: wexecellent = +1\\nwexecllent-and-not = −2 wterrible = −1\\nwterrible-and-not = +2 wnot = 0 in this particular case, we have addressed the problem. however, if\\nwe start with d-many features, if we want to add all pairs, we’ll blow\\n2 ) = o(d2) features through this feature mapping. and\\nup to (d\\nthere’s no guarantee that pairs of features is enough. we might need\\n3 ) = o(d2) features. these\\ntriples of features, and now we’re up to (d\\nadditional features will drastically increase computation and will\\noften result in a stronger propensity to overﬁtting. in fact, the “xor problem” is so signiﬁcant that it basically killed research in classiﬁers with linear decision boundaries for a decade\\nor two. later in this book, we will see two alternative approaches to\\ntaking key ideas from the perceptron and generating classiﬁers with\\nnon-linear decision boundaries. one approach is to combine multi-\\nple perceptrons in a single framework: this is the neural networks\\napproach (see chapter 10). the second approach is to ﬁnd computa-\\ntionally efﬁcient ways of doing feature mapping in a computationally\\nand statistically efﬁcient way: this is the kernels approach (see chap-\\nter 11). 4.8 further reading todo further reading figure 4.12: picture of xor problem ? suppose that you took the xor\\nproblem and added one new fea-\\nture: x3 = x1 ∧ x2 (the logical and\\nof the two existing features). write\\nout feature weights and a bias that\\nwould achieve perfect classiﬁcation\\non this data. a ship in port is safe, but that is not what ships are for.\\ngrace hopper – at this point, you have seen three qualitatively different models\\nfor learning: decision trees, nearest neighbors, and perceptrons. you\\nhave also learned about clustering with the k-means algorithm. you\\nwill shortly learn about more complex models, most of which are\\nvariants on things you already know. however, before attempting\\nto understand more complex models of learning, it is important to\\nhave a ﬁrm grasp on how to use machine learning in practice. this\\nchapter is all about how to go from an abstract learning problem\\nto a concrete implementation. you will see some examples of “best\\npractices” along with justiﬁcations of these practices. in many ways, going from an abstract problem to a concrete learn- ing task is more of an art than a science. however, this art can have\\na huge impact on the practical performance of learning systems. in\\nmany cases, moving to a more complicated learning algorithm will\\ngain you a few percent improvement. going to a better representa-\\ntion will gain you an order of magnitude improvement. to this end,\\nwe will discuss several high level ideas to help you develop a better\\nartistic sensibility. ',\n",
       " ' consider a problem of object recognition from images. if you start machine learning is magical. you give it data and it manages to\\nclassify that data. for many, it can actually outperform a human! but,\\nlike so many problems in the world, there is a signiﬁcant “garbage\\nin, garbage out” aspect to machine learning. if the data you give it is\\ntrash, the learning algorithm is unlikely to be able to overcome it.\\nwith a 100×100 pixel image, a very easy feature representation of\\nthis image is as a 30, 000 dimensional vector, where each dimension\\ncorresponds to the red, green or blue component of some pixel in\\nthe image. so perhaps feature 1 is the amount of red in pixel (1, 1);\\nfeature 2 is the amount of green in that pixel; and so on. this is the\\npixel representation of images. learning objectives:\\n• translate between a problem de-\\nscription and a concrete learning\\nproblem. • perform basic feature engineering on image and text data. • explain how to use cross-validation to tune hyperparameters and esti-\\nmate future performance. • compare and contrast the differ-\\nences between several evaluation\\nmetrics. • explain why feature combinations are important for learning with\\nsome models but not others. • explain the relationship between the three learning techniques you have\\nseen so far. • apply several debugging techniques to learning algorithms. dependencies: chapter 1,chap-\\nter 3,chapter 4 figure 5.1: object recognition in pixels 56 a course in machine learning one thing to keep in mind is that the pixel representation throws away all locality information in the image. learning algorithms don’t\\ncare about features: they only care about feature values. so i can\\npermute all of the features, with no effect on the learning algorithm\\n(so long as i apply the same permutation to all training and test\\nexamples). figure 5.1 shows some images whose pixels have been\\nrandomly permuted (in this case only the pixels are permuted, not\\nthe colors). all of these objects are things that you’ve seen plenty of\\nexamples of; can you identify them? should you expect a machine to\\nbe able to? an alternative representation of images is the patch represen- tation, where the unit of interest is a small rectangular block of an\\nimage, rather than a single pixel. again, permuting the patches has\\nno effect on the classiﬁer. figure 5.2 shows the same images in patch\\nrepresentation. can you identify them? a ﬁnal representation is a\\nshape representation. here, we throw out all color and pixel infor-\\nmation and simply provide a bounding polygon. figure 5.3 shows\\nthe same images in this representation. is this now enough to iden-\\ntify them? (if not, you can ﬁnd the answers in figure 5.15 at the end\\nof the chapter.) in the context of text categorization (for instance, the sentiment\\nrecognition task), one standard representation is the bag of words\\nrepresentation. here, we have one feature for each unique word that\\nappears in a document. for the feature happy, the feature value is\\nthe number of times that the word “happy” appears in the document.\\nthe bag of words (bow) representation throws away all position\\ninformation. table 5.1 shows a bow representation for two chapters\\nof this book. can you tell which is which? ',\n",
       " ' one big difference between learning models is how robust they are to\\nthe addition of noisy or irrelevant features. intuitively, an irrelevant\\nfeature is one that is completely uncorrelated with the prediction\\ntask. a feature f whose expectation does not depend on the label\\ne[ f y] = e[ f ] might be irrelevant. for instance, the presence of\\nthe word “the” might be largely irrelevant for predicting whether a\\ncourse review is positive or negative. a secondary issue is how well these algorithms deal with redun- dant features. two features are redundant if they are highly cor-\\nrelated, regardless of whether they are correlated with the task or\\nnot. for example, having a bright red pixel in an image at position\\n(20, 93) is probably highly redundant with having a bright red pixel\\nat position (21, 93). both might be useful (e.g., for identifying ﬁre hy- figure 5.2: object recognition in patches figure 5.3: object recognition in shapes data learning\\ntraining set\\npredict\\nfea-\\nture function\\ntest machine\\nloss alice tree\\nguess\\nfeatures\\nalgorithm data knn\\ndimensions\\npoints\\nfea-\\nture ﬁgure\\ndecision fea-\\ntures point\\nﬁg training\\nset\\nspace\\nexamples table 5.1: bag of (most frequent) words\\nrepresentation for the decision tree\\nand knn chapters of this book, after\\ndropping high frequency words like\\n“the”. figure 5.4: prac:bow: bow repr of one\\npositive and one negative review practical issues 57 ? is it possible to have a feature f\\nwhose expectation does not depend\\non the label, but is nevertheless still\\nuseful for prediction? 1 you might think it’s absurd to have\\nso many irrelevant features, but the\\ncases you’ve seen so far (bag of words,\\nbag of pixels) are both reasonable\\nexamples of this! how many words,\\nout of the entire english vocabulary\\n(roughly 10, 000 − 100, 000 words), are\\nactually useful for predicting positive\\nand negative course reviews? drants), but because of how images are structured, these two features\\nare likely to co-occur frequently. when thinking about robustness to irrelevant or redundant fea- tures, it is usually not worthwhile thinking of the case where one has\\n999 great features and 1 bad feature. the interesting case is when the\\nbad features outnumber the good features, and often outnumber by a\\nlarge degree. the question is how robust are algorithms in this case.1 for shallow decision trees, the model explicitly selects features that are highly correlated with the label. in particular, by limiting the\\ndepth of the decision tree, one can at least hope that the model will be\\nable to throw away irrelevant features. redundant features are almost\\ncertainly thrown out: once you select one feature, the second feature\\nnow looks mostly useless. the only possible issue with irrelevant\\nfeatures is that even though they’re irrelevant, they happen to correlate\\nwith the class label on the training data, but chance. as a thought experiment, suppose that we have n training ex- amples, and exactly half are positive examples and half are negative\\nexamples. suppose there’s some binary feature, f , that is completely\\nuncorrelated with the label. this feature has a 50/50 chance of ap-\\npearing in any example, regardless of the label. in principle, the deci-\\nsion tree should not select this feature. but, by chance, especially if n\\nis small, the feature might look correlated with the label. this is anal-\\nogous to ﬂipping two coins simultaneously n times. even though the\\ncoins are independent, it’s entirely possible that you will observe a\\nsequence like (h, h), (t, t), (h, h), (h, h), which makes them look\\nentirely correlated! the hope is that as n grows, this becomes less\\nand less likely. in fact, we can explicitly compute how likely this is to\\nhappen. to do this, let’s ﬁx the sequence of n labels. we now ﬂip a coin n times and consider how likely it is that it exactly matches the label.\\nthis is easy: the probability is 0.5n. now, we would also be confused\\nif it exactly matched not the label, which has the same probability. so\\nthe chance that it looks perfectly correlated is 0.5n + 0.5n = 0.5n−1.\\nthankfully, this shrinks down very small (e.g., 10−6) after only 21\\ndata points, meaning that even with a very small training set, the\\nchance that a random feature happens to correlate exactly with the\\nlabel is tiny. this makes us happy. the problem is that we don’t have one irrel-\\nevant feature: we have many! if we randomly pick two irrelevant fea-\\ntures, each has the same probability of perfectly correlating: 0.5n−1.\\nbut since there are two and they’re independent coins, the chance\\nthat either correlates perfectly is 2×0.5n−1 = 0.5n−2. in general,\\nif we have k irrelevant features, all of which are random indepen-\\ndent coins, the chance that at least one of them perfectly correlates is 58 a course in machine learning 0.5n−k. this suggests that if we have a sizeable number k of irrele-\\nvant features, we’d better have at least k + 21 training examples.\\nunfortunately, the situation is actually worse than this. in the above analysis we only considered the case of perfect correlation. we\\ncould also consider the case of partial correlation, which would yield\\neven higher probabilities. sufﬁce it to say that even decision trees can\\nbecome confused. in the case of k-nearest neighbors, the situation is perhaps more dire. since knn weighs each feature just as much as another feature,\\nthe introduction of irrelevant features can completely mess up knn\\nprediction. in fact, as you saw, in high dimensional space, randomly\\ndistributed points all look approximately the same distance apart. if\\nwe add lots and lots of randomly distributed features to a data set,\\nthen all distances still converge. in the case of the perceptron, one can hope that it might learn to assign zero weight to irrelevant features. for instance, consider a\\nbinary feature is randomly one or zero independent of the label. if\\nthe perceptron makes just as many updates for positive examples\\nas for negative examples, there is a reasonable chance this feature\\nweight will be zero. at the very least, it should be small. ',\n",
       " ' in text categorization problems, some words simply do not appear\\nvery often. perhaps the word “groovy”2 appears in exactly one train-\\ning document, which is positive. is it really worth keeping this word\\naround as a feature? it’s a dangerous endeavor because it’s hard to\\ntell with just one training example if it is really correlated with the\\npositive class, or is it just noise. you could hope that your learning\\nalgorithm is smart enough to ﬁgure it out. or you could just remove\\nit. that means that (a) the learning algorithm won’t have to ﬁgure it\\nout, and (b) you’ve reduced the number of dimensions you have, so\\nthings should be more efﬁcient, and less “scary.” this idea of feature pruning is very useful and applied in many\\napplications. it is easiest in the case of binary features. if a binary\\nfeature only appears some small number k times (in the training\\ndata: no fair looking at the test data!), you simply remove it from\\nconsideration. (you might also want to remove features that appear\\nin all-but-k many documents, for instance the word “the” appears in\\npretty much every english document ever written.) typical choices\\nfor k are 1, 2, 5, 10, 20, 50, mostly depending on the size of the data.\\non a text data set with 1000 documents, a cutoff of 5 is probably\\nreasonable. on a text data set the size of the web, a cut of 50 or even\\n100 or 200 is probably reasonable3. figure 5.6 shows the effect of figure 5.5: prac:addirrel: data from\\nhigh dimensional warning, interpolated ? what happens with the perceptron\\nwith truly redundant features (i.e.,\\none is literally a copy of the other)? 2 this is typically positive indicator,\\nor at least it was back in the us in the\\n1970s. figure 5.6: prac:pruning: effect of\\npruning on text data\\n3 according to google, the following\\nwords (among many others) appear\\n200 times on the web: moudlings, agag-\\ngagctg, setgravity, rogov, prosomeric,\\nspunlaid, piyushtwok, telelesson, nes-\\nmysl, brighnasa. for comparison, the\\nword “the” appears 19, 401, 194, 714 (19\\nbillion) times. practical issues 59 math review data statistics: means and variances\\nwe often need to discuss various statistics of a data set. most often, it is enough to consider univariate\\n(one-dimensional) data. suppose we have n real valued numbers z1, z2, . . . , zn. the sample mean (or\\nn ∑n zn. the sample\\njust mean) of these numbers is just their average value, or expected value: µ = 1\\n∑n(zn − µ)2,\\nvariance (or just variance) measures how much they vary around their mean: σ2 = 1\\nn−1\\nwhere µ is the sample mean. the mean and variance have convenient interpretations in terms of prediction. suppose we wanted\\nto choose a single constant value to “predict” the next z, and were minimizing squared error. call\\nthis constant value a. then a = argmina∈r\\n2 is for convenience and does\\n∑n(a − zn)2\\nnot change the answer.) to solve for a, we can take derivatives and set to zero: ∂\\n=\\n∂a\\n∑n(a − zn) = na − ∑n zn; therefore na = ∑n zn and a = µ. this means that the sample mean is\\nthe number that minimizes squared error to the sample. moreover, the variance is propotional to the\\nsquared error of that “predictor.” ∑n(a − zn)2. (here, the 1 1\\n2 1\\n2 pruning on a sentiment analysis task. in the beginning, pruning does\\nnot hurt (and sometimes helps!) but eventually we prune away all the\\ninteresting words and performance suffers. in the case of real-valued features, the question is how to extend\\nthe idea of “does not occur much” to real values. a reasonable def-\\ninition is to look for features with low variance. in fact, for binary\\nfeatures, ones that almost never appear or almost always appear will\\nalso have low variance. figure 5.8 shows the result of pruning low-\\nvariance features on the digit recognition task. again, at ﬁrst pruning\\ndoes not hurt (and sometimes helps!) but eventually we have thrown\\nout all the useful features. it is often useful to normalize the data so that it is consistent in\\nsome way. there are two basic types of normalization: feature nor-\\nmalization and example normalization. in feature normalization,\\nyou go through each feature and adjust it the same way across all\\nexamples. in example normalization, each example is adjusted indi-\\nvidually. the goal of both types of normalization is to make it easier for your learning algorithm to learn. in feature normalization, there are two\\nstandard things to do: 1. centering: moving the entire data set so that it is centered around the origin. 2. scaling: rescaling each feature so that one of the following holds: (a) each feature has variance 1 across the training data. figure 5.7: figure 5.8: prac:variance: effect of\\npruning on vision ? earlier we discussed the problem\\nof scale of features (e.g., millimeters\\nversus centimeters). does this have\\nan impact on variance-based feature\\npruning? figure 5.9: prac:transform: picture 60 a course in machine learning (b) each feature has maximum absolute value 1 across the train- ing data. these transformations are shown geometrically in figure 5.9. the\\ngoal of centering is to make sure that no features are arbitrarily large.\\nthe goal of scaling is to make sure that all features have roughly the\\nsame scale (to avoid the issue of centimeters versus millimeters). these computations are fairly straightforward. here, xn,d refers to the dth feature of example n. since it is very rare to apply scaling\\nwithout previously applying centering, the expressions below for\\nscaling assume that the data is already centered.\\nxn,d ← xn,d − µd\\nxn,d ← xn,d/σd\\nxn,d ← xn,d/rd\\n∑\\nxn,d\\nµd =\\nn centering:\\nvariance scaling:\\nabsolute scaling: where: (5.1)\\n(5.2)\\n(5.3)\\n(5.4) 1\\nn (cid:115) σd = rd = max n 1 n − 1 (cid:12)(cid:12)xn,d (xn,d − µd)2 ∑\\nn (cid:12)(cid:12) (5.5) (5.6) ? for the three models you know\\nabout (knn, dt, perceptron),\\nwhich are most sensitive to center-\\ning? which are most sensitive to\\nscaling? in practice, if the dynamic range of your features is already some\\nsubset of [−2, 2] or [−3, 3], then it is probably not worth the effort of\\ncentering and scaling. (it’s an effort because you have to keep around\\nyour centering and scaling calculations so that you can apply them\\nto the test data as well!) however, if some of your features are orders\\nof magnitude larger than others, it might be helpful. remember that\\nyou might know best: if the difference in scale is actually signiﬁcant\\nfor your problem, then rescaling might throw away useful informa-\\ntion. one thing to be wary of is centering binary data. in many cases, binary data is very sparse: for a given example, only a few of the\\nfeatures are “on.” for instance, out of a vocabulary of 10, 000 or\\n100, 000 words, a given document probably only contains about 100.\\nfrom a storage and computation perspective, this is very useful.\\nhowever, after centering, the data will no longer be sparse and you\\nwill pay dearly with outrageously slow implementations. in example normalization, you view examples one at a time. the most standard normalization is to ensure that the length of each\\nexample vector is one: namely, each example lies somewhere on the\\nunit hypersphere. this is a simple transformation: example normalization: xn ← xn/ xn (5.7) figure 5.10: prac:exnorm: example of\\nexample normalization this transformation is depicted in figure 5.10. the main advantage to example normalization is that it makes\\ncomparisons more straightforward across data sets. if i hand you\\ntwo data sets that differ only in the norm of the feature vectors (i.e.,\\none is just a scaled version of the other), it is difﬁcult to compare the\\nlearned models. example normalization makes this more straightfor-\\nward. moreover, as you saw in the perceptron convergence proof, it is\\noften just mathematically easier to assume normalized data. ',\n",
       " ' you learned in chapter 4 that linear models (like the perceptron)\\ncannot solve the xor problem. you also learned that by performing\\na combinatorial feature explosion, they could. but that came at the\\ncomputational expense of gigantic feature vectors. of the algorithms that you’ve seen so far, the perceptron is the one that has the most to gain by feature combination. and the decision\\ntree is the one that has the least to gain. in fact, the decision tree\\nconstruction is essentially building meta features for you. (or, at\\nleast, it is building meta features constructed purely through “logical\\nands.”) this observation leads to a heuristic for constructing meta features for perceptrons from decision trees. the idea is to train a decision\\ntree on the training data. from that decision tree, you can extract\\nmeta features by looking at feature combinations along branches. you\\ncan then add only those feature combinations as meta features to the\\nfeature set for the perceptron. figure 5.11 shows a small decision tree\\nand a set of meta features that you might extract from it. there is a\\nhyperparameter here of what length paths to extract from the tree: in\\nthis case, only paths of length two are extracted. for bigger trees, or\\nif you have more data, you might beneﬁt from longer paths. in addition to combinatorial transformations, the logarithmic transformation can be quite useful in practice. it seems like a strange\\nthing to be useful, since it doesn’t seem to fundamentally change\\nthe data. however, since many learning algorithms operate by linear\\noperations on the features (both perceptron and knn do this), the\\nlog-transform is a way to get product-like operations. the question is\\nwhich of the following feels more applicable to your data: (1) every\\ntime this feature increases by one, i’m equally more likely to predict\\na positive label; (2) every time this feature doubles, i’m equally more\\nlike to predict a positive label. in the ﬁrst case, you should stick\\nwith linear features and in the second case you should switch to\\na log-transform. this is an important transformation in text data,\\nwhere the presence of the word “excellent” once is a good indicator\\nof a positive review; seeing “excellent” twice is a better indicator; practical issues 61 figure 5.11: prac:dttoperc: turning a\\ndt into a set of meta features figure 5.12: prac:log: performance on\\ntext categ with word counts versus log\\nword counts 62 a course in machine learning but the difference between seeing “excellent” 10 times and seeing it\\n11 times really isn’t a big deal any more. a log-transform achieves\\nthis. experimentally, you can see the difference in test performance\\nbetween word count data and log-word count data in figure 5.12.\\nhere, the transformation is actually xd (cid:55)→ log2(xd + 1) to ensure that\\nzeros remain zero and sparsity is retained. in the case that feature\\nvalues can also be negative, the slightly more complex mapping\\nxd (cid:55)→ log2(xd + 1)sign(xd), where sign(xd) denotes the sign of xd. ',\n",
       " ' so far, our focus has been on classiﬁers that achieve high accuracy.\\nin some cases, this is not what you might want. for instance, if you\\nare trying to predict whether a patient has cancer or not, it might be\\nbetter to err on one side (saying they have cancer when they don’t)\\nthan the other (because then they die). similarly, letting a little spam\\nslip through might be better than accidentally blocking one email\\nfrom your boss. there are two major types of binary classiﬁcation problems. one is “x versus y.” for instance, positive versus negative sentiment.\\nanother is “x versus not-x.” for instance, spam versus non-spam.\\n(the argument being that there are lots of types of non-spam.) or\\nin the context of web search, relevant document versus irrelevant\\ndocument. this is a subtle and subjective decision. but “x versus not-\\nx” problems often have more of the feel of “x spotting” rather than\\na true distinction between x and y. (can you spot the spam? can you\\nspot the relevant documents?) for spotting problems (x versus not-x), there are often more ap-\\npropriate success metrics than accuracy. a very popular one from\\ninformation retrieval is the precision/recall metric. precision asks\\nthe question: of all the x’s that you found, how many of them were\\nactually x’s? recall asks: of all the x’s that were out there, how many\\nof them did you ﬁnd?4 formally, precision and recall are deﬁned as: p = r = i\\ns\\ni\\nt s = number of xs that your system found\\nt = number of xs in the data\\ni = number of correct xs that your system found (5.8) (5.9)\\n(5.10)\\n(5.11)\\n(5.12) here, s is mnemonic for “system,” t is mnemonic for “truth” and i\\nis mnemonic for “intersection.” it is generally accepted that 0/0 = 1\\nin these deﬁnitions. thus, if you system found nothing, your preci- 4 a colleague make the analogy to the\\nus court system’s saying “do you\\npromise to tell the whole truth and\\nnothing but the truth?” in this case, the\\n“whole truth” means high recall and\\n“nothing but the truth” means high\\nprecision.” sion is always perfect; and if there is nothing to ﬁnd, your recall is\\nalways perfect. once you can compute precision and recall, you are often able to produce precision/recall curves. suppose that you are attempting\\nto identify spam. you run a learning algorithm to make predictions\\non a test set. but instead of just taking a “yes/no” answer, you allow\\nyour algorithm to produce its conﬁdence. for instance, in perceptron,\\nyou might use the distance from the hyperplane as a conﬁdence\\nmeasure. you can then sort all of your test emails according to this\\nranking. you may put the most spam-like emails at the top and the\\nleast spam-like emails at the bottom, like in figure 5.13. once you have this sorted list, you can choose how aggressively\\nyou want your spam ﬁlter to be by setting a threshold anywhere on\\nthis list. one would hope that if you set the threshold very high, you\\nare likely to have high precision (but low recall). if you set the thresh-\\nold very low, you’ll have high recall (but low precision). by consider-\\ning every possible place you could put this threshold, you can trace out\\na curve of precision/recall values, like the one in figure 5.14. this\\nallows us to ask the question: for some ﬁxed precision, what sort of\\nrecall can i get. obviously, the closer your curve is to the upper-right\\ncorner, the better. and when comparing learning algorithms a and\\nb you can say that a dominates b if a’s precision/recall curve is\\nalways higher than b’s. precision/recall curves are nice because they allow us to visualize\\nmany ways in which we could use the system. however, sometimes\\nwe like to have a single number that informs us of the quality of the\\nsolution. a popular way of combining precision and recall into a\\nsingle number is by taking their harmonic mean. this is known as\\nthe balanced f-measure (or f-score): f = 2×p×r\\np + r (5.13) the reason that you want to use a harmonic mean rather than an\\narithmetic mean (the one you’re more used to) is that it favors sys-\\ntems that achieve roughly equal precision and recall. in the extreme\\ncase where p = r, then f = p = r. but in the imbalanced case, for\\ninstance p = 0.1 and r = 0.9, the overall f-measure is a modest 0.18.\\ntable 5.2 shows f-measures as a function of precision and recall, so\\nthat you can see how important it is to get balanced values. in some cases, you might believe that precision is more impor-\\ntant than recall. this idea leads to the weighted f-measure, which is\\nparameterized by a weight β ∈ [0, ∞) (beta): fβ = (1 + β2)×p×r β2×p + r (5.14) practical issues 63 figure 5.13: prac:spam: show a bunch\\nof emails spam/nospam sorted by\\nmodel predicion, not perfect\\n? how would you get a conﬁdence\\nout of a decision tree or knn? figure 5.14: prac:prcurve: precision\\nrecall curve0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00 0.8\\n0.000.320.4\\n0.53\\n0.680.801.0\\n0.88\\ntable 5.2: table of f-measures when\\nvarying precision and recall values. 0.6\\n0.00\\n0.30\\n0.48\\n0.60\\n0.68\\n0.74 0.2\\n0.00\\n0.20\\n0.26\\n0.30\\n0.32\\n0.33 0.4\\n0.00\\n0.26\\n0.40\\n0.48\\n0.53\\n0.57 1.0\\n0.00\\n0.33\\n0.57\\n0.74\\n0.88\\n1.00 64 a course in machine learning for β = 1, this reduces to the standard f-measure. for β = 0, it\\nfocuses entirely on recall and for β → ∞ it focuses entirely on preci-\\nsion. the interpretation of the weight is that fβ measures the perfor-\\nmance for a user who cares β times as much about precision as about\\nrecall. one thing to keep in mind is that precision and recall (and hence\\nf-measure) depend crucially on which class is considered the thing\\nyou wish to ﬁnd. in particular, if you take a binary data set if ﬂip\\nwhat it means to be a positive or negative example, you will end\\nup with completely difference precision and recall values. it is not\\nthe case that precision on the ﬂipped task is equal to recall on the\\noriginal task (nor vice versa). consequently, f-measure is also not the\\nsame. for some tasks where people are less sure about what they\\nwant, they will occasionally report two sets of precision/recall/f-\\nmeasure numbers, which vary based on which class is considered the\\nthing to spot. there are other standard metrics that are used in different com-\\nmunities. for instance, the medical community is fond of the sensi-\\ntivity/speciﬁcity metric. a sensitive classiﬁer is one which almost\\nalways ﬁnds everything it is looking for: it has high recall. in fact,\\nsensitivity is exactly the same as recall. a speciﬁc classiﬁer is one\\nwhich does a good job not ﬁnding the things that it doesn’t want to\\nﬁnd. speciﬁcity is precision on the negation of the task at hand. you can compute curves for sensitivity and speciﬁcity much like\\nthose for precision and recall. the typical plot, referred to as the re-\\nceiver operating characteristic (or roc curve) plots the sensitivity\\nagainst 1 − speciﬁcity. given an roc curve, you can compute the\\narea under the curve (or auc) metric, which also provides a mean-\\ningful single number for a system’s performance. unlike f-measures,\\nwhich tend to be low because the require agreement, auc scores\\ntend to be very high, even for not great systems. this is because ran-\\ndom chance will give you an auc of 0.5 and the best possible auc\\nis 1.0. the main message for evaluation metrics is that you should choose whichever one makes the most sense. in many cases, several might\\nmake sense. in that case, you should do whatever is more commonly\\ndone in your ﬁeld. there is no reason to be an outlier without cause. ',\n",
       " ' in chapter 1, you learned about using development data (or held-out\\ndata) to set hyperparameters. the main disadvantage to the develop-\\nment data approach is that you throw out some of your training data,\\njust for estimating one or two hyperparameters. practical issues 65 algorithm 8 crossvalidate(learningalgorithm, data, k)\\n1: ˆ\\x01 ← ∞\\n2: ˆα ← unknown\\n3: for all hyperparameter settings α do\\n4: // store lowest error encountered so far\\n// store the hyperparameter setting that yielded it // keep track of the k-many error estimates err ← [ ]\\nfor k = 1 to k do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: train ← {(xn, yn) ∈ data : n mod k (cid:54)= k − 1}\\ntest ← {(xn, yn) ∈ data : n mod k = k − 1} // test every kth example\\nmodel ← run learningalgorithm on train\\nerr ← err ⊕ error of model on test // add current error to list of errors end for\\navgerr ← mean of set err\\nif avgerr < ˆ\\x01 then ˆ\\x01 ← avgerr\\nˆα ← α // remember these settings\\n// because they’re the best so far end if\\n15:\\n16: end for an alternative is the idea of cross validation. in cross validation, you break your training data up into 10 equally-sized partitions. you\\ntrain a learning algorithm on 9 of them and test it on the remaining\\n1. you do this 10 times, each time holding out a different partition as\\nthe “development” part. you can then average your performance over\\nall ten parts to get an estimate of how well your model will perform\\nin the future. you can repeat this process for every possible choice of\\nhyperparameters to get an estimate of which one performs best. the\\ngeneral k-fold cross validation technique is shown in algorithm 5.6,\\nwhere k = 10 in the preceeding discussion. in fact, the development data approach can be seen as an approxi-\\nmation to cross validation, wherein only one of the k loops (line 5 in\\nalgorithm 5.6) is executed.\\ntypical choices for k are 2, 5, 10 and n − 1. by far the most com- mon is k = 10: 10-fold cross validation. sometimes 5 is used for\\nefﬁciency reasons. and sometimes 2 is used for subtle statistical rea-\\nsons, but that is quite rare. in the case that k = n − 1, this is known\\nas leave-one-out cross validation (or abbreviated as loo cross val-\\nidation). after running cross validation, you have two choices. you\\ncan either select one of the k trained models as your ﬁnal model to\\nmake predictions with, or you can train a new model on all of the\\ndata, using the hyperparameters selected by cross-validation. if you\\nhave the time, the latter is probably a better options. it may seem that loo cross validation is prohibitively expensive\\nto run. this is true for most learning algorithms except for k-nearest\\nneighbors. for knn, leave-one-out is actually very natural. we loop\\nthrough each training point and ask ourselves whether this example\\nwould be correctly classiﬁed for all different possible values of k. 66 a course in machine learning algorithm 9 knn-train-loo(d)\\n1: errk ← 0, ∀1 ≤ k ≤ n − 1\\n2: for n = 1 to n do\\n3: sm ← (cid:104)xn − xm , m(cid:105), ∀m (cid:54)= n\\ns ← sort(s)\\nˆy ← 0\\nfor k = 1 to n − 1 do (cid:104)dist,m(cid:105) ← sk\\nˆy ← ˆy + ym\\nif ˆy (cid:54)= ym then errk ← errk + 1 4: 5: 6: 7: 8: 9: 10: 11: end if\\nend for 12:\\n13: end for\\n14: return argmink errk // errk stores how well you do with knn // compute distances to other points\\n// put lowest-distance objects ﬁrst\\n// current label prediction // let kth closest point vote // one more error for knn // return the k that achieved lowest error this requires only as much computation as computing the k nearest\\nneighbors for the highest value of k. this is such a popular and\\neffective approach for knn classiﬁcation that it is spelled out in\\nalgorithm 5.6. overall, the main advantage to cross validation over develop- ment data is robustness. the main advantage of development data is\\nspeed. one warning to keep in mind is that the goal of both cross valida-\\ntion and development data is to estimate how well you will do in the\\nfuture. this is a question of statistics, and holds only if your test data\\nreally looks like your training data. that is, it is drawn from the same\\ndistribution. in many practical cases, this is not entirely true. for example, in person identiﬁcation, we might try to classify every pixel in an image based on whether it contains a person or not.\\nif we have 100 training images, each with 10, 000 pixels, then we have\\na total of 1m training examples. the classiﬁcation for a pixel in image\\n5 is highly dependent on the classiﬁcation for a neighboring pixel in\\nthe same image. so if one of those pixels happens to fall in training\\ndata, and the other in development (or cross validation) data, your\\nmodel will do unreasonably well. in this case, it is important that\\nwhen you cross validate (or use development data), you do so over\\nimages, not over pixels. the same goes for text problems where you\\nsometimes want to classify things at a word level, but are handed a\\ncollection of documents. the important thing to keep in mind is that\\nit is the images (or documents) that are drawn independently from\\nyour data distribution and not the pixels (or words), which are drawn\\ndependently. ',\n",
       " ' suppose that you’ve presented a machine learning solution to your\\nboss that achieves 7% error on cross validation. your nemesis, gabe,\\ngives a solution to your boss that achieves 6.9% error on cross vali-\\ndation. how impressed should your boss be? it depends. if this 0.1%\\nimprovement was measured over 1000 examples, perhaps not too\\nimpressed. it would mean that gabe got exactly one more example\\nright than you did. (in fact, they probably got 15 more right and 14\\nmore wrong.) if this 0.1% impressed was measured over 1, 000, 000\\nexamples, perhaps this is more impressive. this is one of the most fundamental questions in statistics. you\\nhave a scientiﬁc hypothesis of the form “gabe’s algorithm is better\\nthan mine.” you wish to test whether this hypothesis is true. you\\nare testing it against the null hypothesis, which is that gabe’s algo-\\nrithm is no better than yours. you’ve collected data (either 1000 or\\n1m data points) to measure the strength of this hypothesis. you want\\nto ensure that the difference in performance of these two algorithms\\nis statistically significant: i.e., is probably not just due to random\\nluck. (a more common question statisticians ask is whether one drug\\ntreatment is better than another, where “another” is either a placebo\\nor the competitor’s drug.) there are about ∞-many ways of doing hypothesis testing. like evaluation metrics and the number of folds of cross validation, this is\\nsomething that is very discipline speciﬁc. here, we will discuss two\\npopular tests: the paired t-test and bootstrapping. these tests, and\\nother statistical tests, have underlying assumptions (for instance, as-\\nsumptions about the distribution of observations) and strengths (for\\ninstance, small or large samples). in most cases, the goal of hypoth-\\nesis testing is to compute a p-value: namely, the probability that the\\nobserved difference in performance was by chance. the standard way\\nof reporting results is to say something like “there is a 95% chance\\nthat this difference was not by chance.” the value 95% is arbitrary,\\nand occasionally people use weaker (90%) test or stronger (99.5%)\\ntests. the t-test is an example of a parametric test. it is applicable when\\nthe null hypothesis states that the difference between two responses\\nhas mean zero and unknown variance. the t-test actually assumes\\nthat data is distributed according to a gaussian distribution, which is\\nprobably not true of binary responses. fortunately, for large samples\\n(at least a few hundred), binary samples are well approximated by\\na gaussian distribution. so long as your sample is sufﬁciently large,\\nthe t-test is reasonable either for regression or classiﬁcation problems. suppose that you evaluate two algorithm on n-many examples. practical issues 67 t ≥ 1.28\\n≥ 1.64\\n≥ 1.96\\n≥ 2.58 table 5.3: table of signiﬁcance values\\nfor the t-test. signiﬁcance 90.0%\\n95.0%\\n97.5%\\n99.5% 68 a course in machine learning on each example, you can compute whether the algorithm made\\nthe correct prediction. let a1, . . . , an denote the error of the ﬁrst\\nalgorithm on each example. let b1, . . . , bn denote the error of the\\nsecond algorithm. you can compute µa and µb as the means of a and\\nb, respecitively. finally, center the data as ˆa = a − µa and ˆb = b − µb.\\nthe t-statistic is deﬁned as: (cid:115) t = (µa − µb) n(n − 1)\\n∑n(ˆan − ˆbn)2 (5.15) after computing the t-value, you can compare it to a list of values\\nfor computing conﬁdence intervals. assuming you have a lot of data\\n(n is a few hundred or more), then you can compare your t-value to\\ntable 5.3 to determine the signiﬁcance level of the difference. one disadvantage to the t-test is that it cannot easily be applied\\nto evaluation metrics like f-score. this is because f-score is a com-\\nputed over an entire test set and does not decompose into a set of\\nindividual errors. this means that the t-test cannot be applied. fortunately, cross validation gives you a way around this problem. ? what does it mean for the means\\nµa and µb to become further apart?\\nhow does this affect the t-value?\\nwhat happens if the variance of a\\nincreases? when you do k-fold cross validation, you are able to compute k\\nerror metrics over the same data. for example, you might run 5-fold\\ncross validation and compute f-score for every fold. perhaps the f-\\nscores are 92.4, 93.9, 96.1, 92.2 and 94.4. this gives you an average\\nf-score of 93.8 over the 5 folds. the standard deviation of this set of\\nf-scores is: σ = (cid:115)\\n(cid:114) 1 = 4\\n= 1.595 1 n − 1 (ai − µ)2 ∑\\nn (1.96 + 0.01 + 5.29 + 2.56 + 0.36) (5.16) (5.17)\\n(5.18) you can now assume that the distribution of scores is approximately\\ngaussian. if this is true, then approximately 70% of the proba-\\nbility mass lies in the range [µ − σ, µ + σ]; 95% lies in the range\\n[µ − 2σ, µ + 2σ]; and 99.5% lies in the range [µ − 3σ, µ + 3σ]. so, if we\\nwere comparing our algorithm against one whose average f-score was\\n90.6%, we could be 95% certain that our superior performance was\\nnot due to chance.5 warning: a conﬁdence of 95% does not mean “there is a 95%\\nchance that i am better.” all it means is that if i reran the same ex-\\nperiment 100 times, then in 95 of those experiments i would still win.\\nthese are very different statements. if you say the ﬁrst one, people\\nwho know about statistics will get very mad at you! one disadvantage to cross validation is that it is computationally\\nexpensive. more folds typically leads to better estimates, but every 5 had we run 10-fold cross validation\\nwe might be been able to get tighter\\nconﬁdence intervals. practical issues 69 algorithm 10 bootstrapevaluate(y, ˆy, numfolds)\\n1: scores ← [ ]\\n2: for k = 1 to numfolds do\\ntruth ← [ ]\\n3:\\npred ← [ ]\\nfor n = 1 to n do 4: m ← uniform random value from 1 to n\\ntruth ← truth ⊕ ym\\npred ← pred ⊕ ˆym 5: 6: 7: 8: 9: // list of values we want to predict\\n// list of values we actually predicted // sample a test point\\n// add on the truth\\n// add on our prediction // evaluate end for\\nscores ← scores ⊕ f-score(truth, pred) 10:\\n11: end for\\n12: return (mean(scores), stddev(scores)) new fold requires training a new classiﬁer. this can get very time\\nconsuming. the technique of bootstrapping (and closely related idea\\nof jack-knifing can address this problem. suppose that you didn’t want to run cross validation. all you have is a single held-out test set with 1000 data points in it. you can run\\nyour classiﬁer and get predictions on these 1000 data points. you\\nwould like to be able to compute a metric like f-score on this test set,\\nbut also get conﬁdence intervals. the idea behind bootstrapping is\\nthat this set of 1000 is a random draw from some distribution. we\\nwould like to get multiple random draws from this distribution on\\nwhich to evaluate. we can simulate multiple draws by repeatedly\\nsubsampling from these 1000 examples, with replacement. to perform a single bootstrap, you will sample 1000 random points from your test set of 1000 random points. this sampling must be\\ndone with replacement (so that the same example can be sampled\\nmore than once), otherwise you’ll just end up with your original test\\nset. this gives you a bootstrapped sample. on this sample, you can\\ncompute f-score (or whatever metric you want). you then do this 99\\nmore times, to get a 100-fold bootstrap. for each bootstrapped sam-\\nple, you will be a different f-score. the mean and standard deviation\\nof this set of f-scores can be used to estimate a conﬁdence interval for\\nyour algorithm. the bootstrap resampling procedure is sketched in algorithm 5.7.\\nthis takes three arguments: the true labels y, the predicted labels ˆy\\nand the number of folds to run. it returns the mean and standard\\ndeviation from which you can compute a conﬁdence interval. ',\n",
       " ' learning algorithms are notoriously hard to debug, as you may have\\nalready experienced if you have implemented any of the models 70 a course in machine learning presented so far. the main issue is that when a learning algorithm\\ndoesn’t learn, it’s unclear if this is because there’s a bug or because\\nthe learning problem is too hard (or there’s too much noise, or . . . ).\\nmoreover, sometimes bugs lead to learning algorithms performing\\nbetter than they should: these are especially hard to catch (and always\\na bit disappointing when you do catch them). in order to debug failing learning models, it is useful to revisit the notion of: where can error enter our system? in chapter 2, we con-\\nsidered a typical design process for machine learning in figure 2.4.\\nleaving off the top steps in that are not relevant to machine learning\\nin particular, the basic steps that go into crafting a machine learning\\nsystem are: collect data, choose features, choose model family, choose\\ntraining data, train model, evaluate on test data. in each of these\\nsteps, things can go wrong. below are some strategies for isolating\\nthe cause of error. is the problem with generalization to the test data? we have\\ntalked a lot about training error versus test error. in general, it’s\\nunrealistic to expect to do better on the test data than on the training\\ndata. can your learning system do well on ﬁtting the training data?\\nif so, then the problem is in generalization (perhaps your model\\nfamily is too complicated, you have too many features or not enough\\ndata). if not, then the problem is in representation (you probably\\nneed better features or better data). do you have train/test mismatch? if you can ﬁt the training data, but it doesn’t generalize, it could be because there’s something dif-\\nferent about your test data. try shufﬂing your training data and test\\ndata together and then randomly selecting a new test set. if you do\\nwell in that condition, then probably the test distribution is strange\\nin some way. if reselecting the test data doesn’t help, you have other\\ngeneralization problems. is your learning algorithm implemented correctly? this often means: is it optimizing what you think it’s optimizing. instead\\nof measuring accuracy, try measuring whatever-quantity-your-\\nalgorithm-is-supposedly-optimizing (like log loss or hinge loss) and\\nmake sure that the optimizer is successfully minimizing this quantity.\\nit is usually useful to hand-craft some datasets on which you know\\nthe desired behavior. for instance, you could run knn on the xor\\ndata. or you could run perceptron on some easily linearly separa-\\nble data (for instance positive points along the line x2 = x1 + 1 and\\nnegative points along the line x2 = x1 − 1). or a decision tree on\\nnice axis-aligned data. finally, can you compare against a reference\\nimplementation? do you have an adequate representation? if you cannot even ﬁt the training data, you might not have a rich enough feature set. practical issues 71 the easiest way to try to get a learning algorithm to overﬁt is to add\\na new feature to it. you can call this feature the cheatingisfun\\nfeature. the feature value associated with this feature is +1 if this\\nis a positive example and −1 (or zero) if this is a negative example.\\nin other words, this feature is a perfect indicator of the class of this\\nexample. if you add the cheatingisfun feature and your algorithm\\ndoes not get near 0% training error, this could be because there are\\ntoo many noisy features confusing it. you could either remove a lot\\nof the other features, or make the feature value for cheatingisfun\\neither +100 or −100 so that the algorithm really looks at it. if you\\ndo this and your algorithm still cannot overﬁt then you likely have a\\nbug. (remember to remove the cheatingisfun feature from your\\nﬁnal implementation!) if the cheatingisfun technique gets you\\nnear 0% error, then you need to work on better feature design or pick\\nanother learning model (e.g., decision tree versus linear model). if\\nnot, you probably don’t have enough data or have too many features;\\ntry removing as many features as possible. do you have enough data? try training on 80% of your training\\ndata and look at how much this hurts performance. if it hurts a lot,\\nthen getting more data is likely to help; if it only hurts a little, you\\nmight be data saturated. ',\n",
       " ' because one of the key questions in machine learning is the question\\nof representation, it is common to think about test error in terms of a\\ndecomposition into two terms. let f be the learned classiﬁer, selected\\nfrom a set f of “all possible classiﬁers using a ﬁxed representation,”\\nthen: error( f ) = (cid:123)(cid:122)\\nf ∗∈f error( f ∗)\\nerror( f ) − min estimation error + f ∗∈f error( f )\\nmin (5.19) approximation error (cid:20)\\n(cid:124) (cid:21)\\n(cid:125) (cid:20)\\n(cid:124) (cid:123)(cid:122) (cid:21)\\n(cid:125) here, the second term, the approximation error, measures the qual-\\nity of the model family6. one way of thinking of approximation error\\nis: suppose someone gave me inﬁnite data to train on—how well\\ncould i do with this representation? the ﬁrst term, the estimation\\nerror, measures how far the actual learned classiﬁer f is from the\\noptimal classiﬁer f ∗. you can think of this term as measuring how\\nmuch you have to pay for the fact that you don’t have inﬁnite training\\ndata. unfortunately, it is nearly impossible to compute the estima- tion error and approxiation error, except in constructed cases. this\\ndoesn’t make the decomposition useless. decompositions like this 6 the “model family” (such as depth\\n20 decision trees, or linear classiﬁers)\\nis often refered to as the hypothesis\\nclass. the hypothesis class f denotes\\nthe set of all possible classiﬁers we\\nconsider, such as all linear classiﬁers.\\nan classiﬁer f ∈ f is sometimes called\\na hypothesis, though we generally\\navoid this latter terminology here. 72 a course in machine learning are very useful for designing debugging strategies. for instance, the\\ncheatingisfun strategy is designed explicitly to ensure that the ap-\\nproximation error is zero, and therefore isolating all error into the\\nestimation error. there is a fundamental trade-off between estimation error and ap-\\nproximation error. as you make your representation more complex,\\nyou make f bigger. this will typically cause a decrease in approxi-\\nmation error, because you can now ﬁt more functions. but you run a\\nrisk of increasing the estimation error, because you have added more\\nparameters to ﬁt, and you are likely to suffer from overﬁtting. the trade-off between estimation error and approximation error is often called the bias/variance trade-off, where “approximation\\nerror” is “bias” and “estimation error” is “variance.” to understand\\nthis connection, consider a very simple hypothesis class f that only\\ncontains two functions: the always positive classiﬁer (that returns +1\\nregardless of input) and the always negative classiﬁer. suppose you\\nhave a data generating distribution d that is 60% positive examples\\nand 40% negative examples. you draw a training set of 41 exam-\\nples. there’s about a 90% chance that the majority of these training\\nexamples will be positive, so on this impoverished hypothesis class\\nf, there’s a 90% chance that it will learn the “all positive” classiﬁer.\\nthat is: 90% of the time, regardless of the training set, the learning\\nalgorithm learns the same thing. this is low variance as a function of\\nthe random draw of the training set. on the other hand, the learned\\nclassiﬁer is very insensitive to the input example (in this extreme\\ncase, it’s completely insensitive): it is strongly biased toward predicting\\n+1 even if everything about the input contradicts this. 5.10 further reading todo figure 5.15: object recognition with full\\ninformation learning objectives:\\n• represent complex prediction prob- lems in a formal learning setting. • be able to artiﬁcally “balance” imbalanced data. • understand the positive and neg-\\native aspects of several reductions\\nfrom multiclass classiﬁcation to\\nbinary classiﬁcation. • recognize the difference between\\nregression and ordinal regression. dependencies: different general classiﬁcation methods can give different, but\\nequally plausible, classiﬁcations, so you need an application\\ncontext to choose among them. – karen spärck-jones in the preceeding chapters, you have learned all about a very\\nsimple form of prediction: predicting bits. in the real world, how-\\never, we often need to predict much more complex objects. you may\\nneed to categorize a document into one of several categories: sports,\\nentertainment, news, politics, etc. you may need to rank web pages\\nor ads based on relevance to a query. these problems are all com-\\nmonly encountered, yet fundamentally more complex than binary\\nclassiﬁcation. in this chapter, you will learn how to use everything you already know about binary classiﬁcation to solve these more complicated\\nproblems. you will see that it’s relatively easy to think of a binary\\nclassiﬁer as a black box, which you can reuse for solving these more\\ncomplex problems. this is a very useful abstraction, since it allows us\\nto reuse knowledge, rather than having to build new learning models\\nand algorithms from scratch. ',\n",
       " ' your boss tells you to build a classiﬁer that can identify fraudulent\\ntransactions in credit card histories. fortunately, most transactions\\nare legitimate, so perhaps only 0.1% of the data is a positive in-\\nstance. the imbalanced data problem refers to the fact that for a\\nlarge number of real world problems, the number of positive exam-\\nples is dwarfed by the number of negative examples (or vice versa).\\nthis is actually something of a misnomer: it is not the data that is\\nimbalanced, but the distribution from which the data is drawn. (and\\nsince the distribution is imbalanced, so must the data be.) imbalanced data is a problem because machine learning algo-\\nrithms are too smart for your own good. for most learning algo-\\nrithms, if you give them data that is 99.9% negative and 0.1% posi-\\ntive, they will simply learn to always predict negative. why? because\\nthey are trying to minimize error, and they can achieve 0.1% error by\\ndoing nothing! if a teacher told you to study for an exam with 1000 74 a course in machine learning true/false questions and only one of them is true, it is unlikely you\\nwill study very long. really, the problem is not with the data, but rather with the way that you have deﬁned the learning problem. that is to say, what you\\ncare about is not accuracy: you care about something else. if you\\nwant a learning algorithm to do a reasonable job, you have to tell it\\nwhat you want! most likely, what you want is not to optimize accuracy, but rather\\nto optimize some other measure, like f-score or auc. you want your\\nalgorithm to make some positive predictions, and simply prefer those\\nto be “good.” we will shortly discuss two heuristics for dealing with\\nthis problem: subsampling and weighting. in subsampling, you throw\\nout some of your negative examples so that you are left with a bal-\\nanced data set (50% positive, 50% negative). this might scare you\\na bit since throwing out data seems like a bad idea, but at least it\\nmakes learning much more efﬁcient. in weighting, instead of throw-\\ning out positive examples, we just give them lower weight. if you\\nassign an importance weight of 0.00101 to each of the positive ex-\\namples, then there will be as much weight associated with positive\\nexamples as negative examples. before formally deﬁning these heuristics, we need to have a mech- anism for formally deﬁning supervised learning problems. we will\\nproceed by example, using binary classiﬁcation as the canonical\\nlearning problem. task: binary classification\\ngiven:\\n1. an input space x\\n2. an unknown distribution d over x×{−1, +1}\\n3. a training set d sampled from d\\ncompute: a function f minimizing: e (x,y)∼d(cid:2) f (x) (cid:54)= y(cid:3) as in all the binary classiﬁcation examples you’ve seen, you have\\nsome input space (which has always been rd). there is some distri-\\nbution that produces labeled examples over the input space. you do\\nnot have access to that distribution, but can obtain samples from it.\\nyour goal is to ﬁnd a classiﬁer that minimizes error on that distribu-\\ntion. a small modiﬁcation on this deﬁnition gives a α-weighted classiﬁ-\\ncation problem, where you believe that the positive class is α-times as beyond binary classification 75 algorithm 11 subsamplemap(dweighted, α)\\n1: while true do\\n2: (x, y) ∼ dweighted\\nu ∼ uniform random variable in [0, 1]\\nif y = +1 or u < 1\\nreturn (x, y) α then 3: 4: 5: // draw an example from the weighted distribution end if 6:\\n7: end while algorithm 12 subsampletest( f binary, ˆx)\\n1: return f binary(ˆx) important as the negative class. task: α-weighted binary classification\\ngiven:\\n1. an input space x\\n2. an unknown distribution d over x×{−1, +1}\\n3. a training set d sampled from d compute: a function f minimizing: e (x,y)∼d (cid:104) αy=1(cid:2) f (x) (cid:54)= y(cid:3)(cid:105) the objects given to you in weighted binary classiﬁcation are iden- tical to standard binary classiﬁcation. the only difference is that the\\ncost of misprediction for y = +1 is α, while the cost of misprediction\\nfor y = −1 is 1. in what follows, we assume that α > 1. if it is not,\\nyou can simply swap the labels and use 1/α. the question we will ask is: suppose that i have a good algorithm for solving the binary classification problem. can i turn that into\\na good algorithm for solving the α-weighted binary classification\\nproblem? in order to do this, you need to deﬁne a transformation that maps\\na concrete weighted problem into a concrete unweighted problem.\\nthis transformation needs to happen both at training time and at test\\ntime (though it need not be the same transformation!). algorithm 6.1\\nsketches a training-time sub-sampling transformation and algo-\\nrithm 6.1 sketches a test-time transformation (which, in this case, is\\ntrivial). all the training algorithm is doing is retaining all positive ex-\\namples and a 1/α fraction of all negative examples. the algorithm is\\nexplicitly turning the distribution over weighted examples into a (dif-\\nferent) distribution over binary examples. a vanilla binary classiﬁer 76 a course in machine learning is trained on this induced distribution. aside from the fact that this algorithm throws out a lot of data (especially for large α), it does seem to be doing a reasonable thing.\\nin fact, from a reductions perspective, it is an optimal algorithm. you\\ncan prove the following result: theorem 3 (subsampling optimality). suppose the binary classiﬁer\\ntrained in algorithm 6.1 achieves a binary error rate of \\x01. then the error\\nrate of the weighted predictor is equal to α\\x01. this theorem states that if your binary classiﬁer does well (on the induced distribution), then the learned predictor will also do well\\n(on the original distribution). thus, we have successfully converted\\na weighted learning problem into a plain classiﬁcation problem! the\\nfact that the error rate of the weighted predictor is exactly α times\\nmore than that of the unweighted predictor is unavoidable: the error\\nmetric on which it is evaluated is α times bigger! the proof of this theorem is so straightforward that we will prove it here. it simply involves some algebra on expected values.\\nproof of theorem 3. let dw be the original distribution and let db be\\nthe induced distribution. let f be the binary classiﬁer trained on data\\nfrom db that achieves a binary error rate of \\x01b on that distribution.\\nwe will compute the expected error \\x01w of f on the weighted problem: (x,y)∼dw\\n∑\\ny∈±1 \\x01w = e\\n= ∑\\nx∈x\\n= α ∑\\nx∈x = α ∑\\nx∈x\\n(x,y)∼db = αe\\n= α\\x01b (cid:104)\\nαy=1(cid:2) f (x) (cid:54)= y(cid:3)(cid:105)\\ndw(x, y)αy=1(cid:2) f (x) (cid:54)= y(cid:3)\\n(cid:16)dw(x, +1)(cid:2) f (x) (cid:54)= +1(cid:3) + dw(x,−1)\\n(cid:2) f (x) (cid:54)= −1(cid:3)(cid:17)\\n(cid:16)db(x, +1)(cid:2) f (x) (cid:54)= +1(cid:3) + db(x,−1)(cid:2) f (x) (cid:54)= −1(cid:3)(cid:17)\\n(cid:2) f (x) (cid:54)= y(cid:3) 1\\nα (6.1)\\n(6.2) (6.3)\\n(6.4) (6.5)\\n(6.6) ? why is it unreasonable to expect\\n√\\nto be able to achieve, for instance,\\nan error of\\nsublinear in α? α\\x01, or anything that is and we’re done! (we implicitly assumed x is discrete. in the case\\nof continuous data, you need to replace all the sums over x with\\nintegrals over x, but the result still holds.) instead of subsampling the low-cost class, you could alternatively oversample the high-cost class. the easiest case is when α is an in-\\nteger, say 5. now, whenever you get a positive point, you include 5\\ncopies of it in the induced distribution. whenever you get a negative\\npoint, you include a single copy. ? how can you handle non-integral α,\\nfor instance 5.5? beyond binary classification 77 this oversampling algorithm achieves exactly the same theoretical\\nresult as the subsampling algorithm. the main advantage to the over-\\nsampling algorithm is that it does not throw out any data. the main\\nadvantage to the subsampling algorithm is that it is more computa-\\ntionally efﬁcient. you might be asking yourself: intuitively, the oversampling algo- rithm seems like a much better idea than the subsampling algorithm,\\nat least if you don’t care about computational efﬁciency. but the the-\\nory tells us that they are the same! what is going on? of course the\\ntheory isn’t wrong. it’s just that the assumptions are effectively dif-\\nferent in the two cases. both theorems state that if you can get error\\nof \\x01 on the binary problem, you automatically get error of α\\x01 on the\\nweighted problem. but they do not say anything about how possible\\nit is to get error \\x01 on the binary problem. since the oversampling al-\\ngorithm produces more data points than the subsampling algorithm\\nit is very concievable that you could get lower binary error with over-\\nsampling than subsampling. the primary drawback to oversampling is computational inefﬁ- ciency. however, for many learning algorithms, it is straightforward\\nto include weighted copies of data points at no cost. the idea is to\\nstore only the unique data points and maintain a counter saying how\\nmany times they are replicated. this is not easy to do for the percep-\\ntron (it can be done, but takes work), but it is easy for both decision\\ntrees and knn. for example, for decision trees (recall algorithm 1.3),\\nthe only changes are to: (1) ensure that line 1 computes the most fre-\\nquent weighted answer, and (2) change lines 10 and 11 to compute\\nweighted errors. ',\n",
       " ' multiclass classiﬁcation is a natural extension of binary classiﬁcation.\\nthe goal is still to assign a discrete label to examples (for instance,\\nis a document about entertainment, sports, ﬁnance or world news?).\\nthe difference is that you have k > 2 classes to choose from. ? modify the proof of optimality\\nfor the subsampling algorithm so\\nthat it applies to the oversampling\\nalgorithm. ? why is it hard to change the per-\\nceptron? (hint: it has to do with the\\nfact that perceptron is online.) ? how would you modify knn to\\ntake into account weights? 78 a course in machine learning algorithm 13 oneversusalltrain(dmulticlass, binarytrain)\\n1: for i = 1 to k do\\n2: dbin ← relabel dmulticlass so class i is positive and ¬i is negative fi ← binarytrain(dbin) 3:\\n4: end for\\n5: return f1, . . . , fk algorithm 14 oneversusalltest( f1, . . . , fk, ˆx)\\n1: score ← (cid:104)0, 0, . . . , 0(cid:105)\\n2: for i = 1 to k do\\n3: y ← fi(ˆx)\\nscorei ← scorei + y 4:\\n5: end for\\n6: return argmaxk scorek // initialize k-many scores to zero task: multiclass classification\\ngiven:\\n1. an input space x and number of classes k\\n2. an unknown distribution d over x×[k]\\n3. a training set d sampled from d\\ncompute: a function f minimizing: e (x,y)∼d(cid:2) f (x) (cid:54)= y(cid:3) note that this is identical to binary classiﬁcation, except for the presence of k classes. (in the above, [k] = {1, 2, 3, . . . , k}.) in fact, if\\nyou set k = 2 you exactly recover binary classiﬁcation. the game we play is the same: someone gives you a binary classi-\\nﬁer and you have to use it to solve the multiclass classiﬁcation prob-\\nlem. a very common approach is the one versus all technique (also\\ncalled ova or one versus rest). to perform ova, you train k-many\\nbinary classiﬁers, f1, . . . , fk. each classiﬁer sees all of the training\\ndata. classiﬁer fi receives all examples labeled class i as positives\\nand all other examples as negatives. at test time, whichever classiﬁer\\npredicts “positive” wins, with ties broken randomly. the training and test algorithms for ova are sketched in algo- rithms 6.2 and 6.2. in the testing procedure, the prediction of the ith\\nclassiﬁer is added to the overall score for class i. thus, if the predic-\\ntion is positive, class i gets a vote; if the prdiction is negative, every-\\none else (implicitly) gets a vote. (in fact, if your learning algorithm\\ncan output a conﬁdence, as discussed in section ??, you can often do\\nbetter by using the conﬁdence as y, rather than a simple ±1.) ova is quite natural and easy to implement. it also works very ? suppose that you have n data\\npoints in k classes, evenly divided.\\nhow long does it take to train an\\nova classiﬁer, if the base binary\\nclassiﬁer takes o(n) time to train?\\nwhat if the base classiﬁer takes\\no(n2) time? ? why would using a conﬁdence\\nhelp? beyond binary classification 79 well in practice, so long as you do a good job choosing a good binary\\nclassiﬁcation algorithm tuning its hyperparameters well. its weakness\\nis that it can be somewhat brittle. intuitively, it is not particularly\\nrobust to errors in the underlying classiﬁers. if one classiﬁer makes a\\nmistake, it is possible that the entire prediction is erroneous. in fact,\\nit is entirely possible that none of the k classiﬁers predicts positive\\n(which is actually the worst-case scenario from a theoretical perspec-\\ntive)! this is made explicit in the ova error bound below. theorem 4 (ova error bound). suppose the average binary error of the\\nk binary classiﬁers is \\x01. then the error rate of the ova multiclass predictor\\nis at most (k − 1)\\x01.\\nproof of theorem 4. the key question is how erroneous predictions\\nfrom the binary classiﬁers lead to multiclass errors. we break it down\\ninto false negatives (predicting -1 when the truth is +1) and false\\npositives (predicting +1 when the truth is -1). when a false negative occurs, then the testing procedure chooses\\nrandomly between available options, which is all labels. this gives a\\n(k − 1)/k probability of multiclass error. since only one binary error\\nis necessary to make this happen, the efﬁciency of this error mode is\\n[(k − 1)/k]/1 = (k − 1)/k. multiple false positives can occur simultaneously. suppose there\\nare m false positives. if there is simultaneously a false negative, the\\nerror is 1. in order for this to happen, there have to be m + 1 errors,\\nso the efﬁciency is 1/(m + 1). in the case that there is not a simulta-\\nneous false negative, the error probability is m/(m + 1). this requires\\nm errors, leading to an efﬁciency of 1/(m + 1).\\nthe worse case, therefore, is the false negative case, which gives an\\nefﬁciency of (k − 1)/k. since we have k-many opportunities to err,\\nwe multiply this by k and get a bound of (k − 1)\\x01. the constants in this are relatively unimportant: the aspect that matters is that this scales linearly in k. that is, as the number of\\nclasses grows, so does your expected error. to develop alternative approaches, a useful way to think about\\nturning multiclass classiﬁcation problems into binary classiﬁcation\\nproblems is to think of them like tournaments (football, soccer–aka\\nfootball, cricket, tennis, or whatever appeals to you). you have k\\nteams entering a tournament, but unfortunately the sport they are\\nplaying only allows two to compete at a time. you want to set up a\\nway of pairing the teams and having them compete so that you can\\nﬁgure out which team is best. in learning, the teams are now the\\nclasses and you’re trying to ﬁgure out which class is best.1 one natural approach is to have every team compete against ev- ery other team. the team that wins the majority of its matches is 1 the sporting analogy breaks down\\na bit for ova: k games are played,\\nwherein each team will play simultane-\\nously against all other teams. 80 a course in machine learning algorithm 15 allversusalltrain(dmulticlass, binarytrain) fij ← ∅,∀1 ≤ i < j ≤ k 1:\\n2: for i = 1 to k-1 do\\n3: dpos ← all x ∈ dmulticlass labeled i for j = i+1 to k do dneg ← all x ∈ dmulticlass labeled j\\ndbin ← {(x, +1) : x ∈ dpos} ∪ {(x,−1) : x ∈ dneg}\\nfij ← binarytrain(dbin) algorithm 16 allversusalltest(all fij, ˆx)\\n1: score ← (cid:104)0, 0, . . . , 0(cid:105)\\n2: for i = 1 to k-1 do\\n3: // initialize k-many scores to zero 4: 5: 6: 7: 4: 5: 6: end for 8:\\n9: end for\\n10: return all fijs for j = i+1 to k do\\ny ← fij(ˆx)\\nscorei ← scorei + y\\nscorej ← scorej - y end for 7:\\n8: end for\\n9: return argmaxk scorek declared the winner. this is the all versus all (or ava) approach\\n(sometimes called all pairs). the most natural way to think about it\\n2) classiﬁers. say fij for 1 ≤ i < j ≤ k is the classiﬁer\\nis as training (k\\nthat pits class i against class j. this classiﬁer receives all of the class i\\nexamples as “positive” and all of the class j examples as “negative.”\\nwhen a test point arrives, it is run through all fij classiﬁers. every\\ntime fij predicts positive, class i gets a point; otherwise, class j gets a\\npoint. after running all (k\\n2) classiﬁers, the class with the most votes\\nwins. the training and test algorithms for ava are sketched in algo- rithms 6.2 and 6.2. in theory, the ava mapping is more complicated\\nthan the weighted binary case. the result is stated below, but the\\nproof is omitted. theorem 5 (ava error bound). suppose the average binary error of\\nthe (k\\n2) binary classiﬁers is \\x01. then the error rate of the ava multiclass\\npredictor is at most 2(k − 1)\\x01. at this point, you might be wondering if it’s possible to do bet-\\nter than something linear in k. fortunately, the answer is yes! the\\nsolution, like so much in computer science, is divide and conquer.\\nthe idea is to construct a binary tree of classiﬁers. the leaves of this\\ntree correspond to the k labels. since there are only log2 k decisions\\nmade to get from the root to a leaf, then there are only log2 k chances ? suppose that you have n data\\npoints in k classes, evenly divided.\\nhow long does it take to train an\\nava classiﬁer, if the base binary\\nclassiﬁer takes o(n) time to train?\\nwhat if the base classiﬁer takes\\no(n2) time? how does this com-\\npare to ova? the bound for ava is 2(k − 1)\\x01; the\\nbound for ova is (k − 1)\\x01. does\\nthis mean that ova is necessarily\\nbetter than ava? why or why not? ? beyond binary classification 81 figure 6.2: example classiﬁcation tree\\nfor k = 8 to make an error.\\nan example of a classiﬁcation tree for k = 8 classes is shown in\\nfigure 6.2. at the root, you distinguish between classes {1, 2, 3, 4}\\nand classes {5, 6, 7, 8}. this means that you will train a binary clas-\\nsiﬁer whose positive examples are all data points with multiclass\\nlabel {1, 2, 3, 4} and whose negative examples are all data points with\\nmulticlass label {5, 6, 7, 8}. based on what decision is made by this\\nclassiﬁer, you can walk down the appropriate path in the tree. when\\nk is not a power of 2, the tree will not be full. this classiﬁcation tree\\nalgorithm achieves the following bound. theorem 6 (tree error bound). suppose the average binary classiﬁers\\nerror is \\x01. then the error rate of the tree classiﬁer is at most (cid:100)log2 k(cid:101) \\x01.\\nproof of theorem 6. a multiclass error is made if any classiﬁer on\\nthe path from the root to the correct leaf makes an error. each has\\nprobability \\x01 of making an error and the path consists of at most\\n(cid:100)log2 k(cid:101) binary decisions. one thing to keep in mind with tree classiﬁers is that you have control over how the tree is deﬁned. in ova and ava you have no\\nsay in what classiﬁcation problems are created. in tree classiﬁers,\\nthe only thing that matters is that, at the root, half of the classes are\\nconsidered positive and half are considered negative. you want to\\nsplit the classes in such a way that this classiﬁcation decision is as\\neasy as possible. you can use whatever you happen to know about\\nyour classiﬁcation problem to try to separate the classes out in a\\nreasonable way. can you do better than (cid:100)log2 k(cid:101) \\x01? it turns out the answer is yes,\\nbut the algorithms to do so are relatively complicated. you can actu-\\nally do as well as 2\\x01 using the idea of error-correcting tournaments.\\nmoreover, you can prove a lower bound that states that the best you\\ncould possible do is \\x01/2. this means that error-correcting tourna-\\nments are at most a factor of four worse than optimal. ',\n",
       " ' you start a new web search company called goohooing. like other\\nsearch engines, a user inputs a query and a set of documents is re-\\ntrieved. your goal is to rank the resulting documents based on rel-\\nevance to the query. the ranking problem is to take a collection of\\nitems and sort them according to some notion of preference. one of\\nthe trickiest parts of doing ranking through learning is to properly\\ndeﬁne the loss function. toward the end of this section you will see a\\nvery general loss function, but before that let’s consider a few special\\ncases. 82 a course in machine learning algorithm 17 naiveranktrain(rankingdata, binarytrain)\\n1: d ← [ ]\\n2: for n = 1 to n do\\n3: for all i, j = 1 to m and i (cid:54)= j do if i is prefered to j on query n then 4: 5: 6: 7: 8: d ← d ⊕ (xnij, +1)\\nd ← d ⊕ (xnij,−1) else if j is prefered to i on query n then end if\\nend for 9:\\n10: end for\\n11: return binarytrain(d) continuing the web search example, you are given a collection of queries. for each query, you are also given a collection of documents,\\ntogether with a desired ranking over those documents. in the follow-\\ning, we’ll assume that you have n-many queries and for each query\\nyou have m-many documents. (in practice, m will probably vary\\nby query, but for ease we’ll consider the simpliﬁed case.) the goal is\\nto train a binary classiﬁer to predict a preference function. given a\\nquery q and two documents di and dj, the classiﬁer should predict\\nwhether di should be preferred to dj with respect to the query q. as in all the previous examples, there are two things we have to\\ntake care of: (1) how to train the classiﬁer that predicts preferences;\\n(2) how to turn the predicted preferences into a ranking. unlike the\\nprevious examples, the second step is somewhat complicated in the\\nranking case. this is because we need to predict an entire ranking of\\na large number of documents, somehow assimilating the preference\\nfunction into an overall permutation. for notationally simplicity, let xnij denote the features associated\\nwith comparing document i to document j on query n. training is\\nfairly straightforward. for every n and every pair i (cid:54)= j, we will\\ncreate a binary classiﬁcation example based on features xnij. this\\nexample is positive if i is preferred to j in the true ranking. it is neg-\\native if j is preferred to i. (in some cases the true ranking will not\\nexpress a preference between two objects, in which case we exclude\\nthe i, j and j, i pair from training.) now, you might be tempted to evaluate the classiﬁcation perfor- mance of this binary classiﬁer on its own. the problem with this\\napproach is that it’s impossible to tell—just by looking at its output\\non one i, j pair—how good the overall ranking is. this is because\\nthere is the intermediate step of turning these pairwise predictions\\ninto a coherent ranking. what you need to do is measure how well\\nthe ranking based on your predicted preferences compares to the true\\nordering. algorithms 6.3 and 6.3 show naive algorithms for training algorithm 18 naiveranktest( f , ˆx)\\n1: score ← (cid:104)0, 0, . . . , 0(cid:105)\\n2: for all i, j = 1 to m and i (cid:54)= j do 3: 4: y ← f (ˆxij)\\nscorei ← scorei + y\\nscorej ← scorej - y 5:\\n6: end for\\n7: return argsort(score) beyond binary classification 83 // initialize m-many scores to zero // get predicted ranking of i and j // return queries sorted by score and testing a ranking function. these algorithms actually work quite well in the case of bipartite\\nranking problems. a bipartite ranking problem is one in which you\\nare only ever trying to predict a binary response, for instance “is this\\ndocument relevant or not?” but are being evaluated according to a\\nmetric like auc. this is essentially because the only goal in bipartite\\nproblems is to ensure that all the relevant documents are ahead of\\nall the irrelevant documents. there is no notion that one relevant\\ndocument is more relevant than another. for non-bipartite ranking problems, you can do better. first, when the preferences that you get at training time are more nuanced than\\n“relevant or not,” you can incorporate these preferences at training\\ntime. effectively, you want to give a higher weight to binary prob-\\nlems that are very different in terms of preference than others. sec-\\nond, rather than producing a list of scores and then calling an arbi-\\ntrary sorting algorithm, you can actually use the preference function\\nas the sorting function inside your own implementation of quicksort.\\nwe can now formalize the problem. deﬁne a ranking as a function σ that maps the objects we are ranking (documents) to the desired\\nposition in the list, 1, 2, . . . m. if σu < σv then u is preferred to v (i.e.,\\nappears earlier on the ranked document list). given data with ob-\\nserved rankings σ, our goal is to learn to predict rankings for new\\nobjects, ˆσ. we deﬁne σm as the set of all ranking functions over m\\nobjects. we also wish to express the fact that making a mistake on\\nsome pairs is worse than making a mistake on others. this will be\\nencoded in a cost function ω (omega), where ω(i, j) is the cost for\\naccidentally putting something in position j when it should have\\ngone in position i. to be a valid cost function, ω must be (1) symmet-\\nric, (2) monotonic and (3) satisfy the triangle inequality. namely: (1)\\nω(i, j) = ω(j, i); (2) if i < j < k or i > j > k then ω(i, j) ≤ ω(i, k);\\n(3) ω(i, j) + ω(j, k) ≥ ω(i, k). with these deﬁnitions, we can properly\\ndeﬁne the ranking problem. 84 a course in machine learning task: ω-ranking\\ngiven:\\n1. an input space x\\n2. an unknown distribution d over x×σm\\n3. a training set d sampled from d\\n(cid:35)\\ncompute: a function f : x → σm minimizing: (cid:34) e (x,σ)∼d [σu < σv] [ˆσv < ˆσu] ω(σu, σv) ∑\\nu(cid:54)=v where ˆσ = f (x) (6.7) in this deﬁnition, the only complex aspect is the loss function 6.7.\\nthis loss sums over all pairs of objects u and v. if the true ranking (σ)\\nprefers u to v, but the predicted ranking (ˆσ) prefers v to u, then you\\nincur a cost of ω(σu, σv).\\ndepending on the problem you care about, you can set ω to many\\n“standard” options. if ω(i, j) = 1 whenever i (cid:54)= j, then you achieve\\nthe kemeny distance measure, which simply counts the number of\\npairwise misordered items. in many applications, you may only care\\nabout getting the top k predictions correct. for instance, your web\\nsearch algorithm may only display k = 10 results to a user. in this\\ncase, you can deﬁne: ω(i, j) = if min{i, j} ≤ k and i (cid:54)= j 1\\n0 otherwise (6.8) (cid:40) in this case, only errors in the top k elements are penalized. swap-\\nping items 55 and 56 is irrelevant (for k < 55). finally, in the bipartite ranking case, you can express the area under the curve (auc) metric as: \\uf8f1\\uf8f4\\uf8f2\\uf8f4\\uf8f3 1 ω(i, j) = (m\\n2 ) m+(m − m+) × if i ≤ m+ and j > m+\\n1 if j ≤ m+ and i > m+\\n0 otherwise (6.9) here, m is the total number of objects to be ranked and m+ is the\\nnumber that are actually “good.” (hence, m − m+ is the number\\nthat are actually “bad,” since this is a bipartite problem.) you are\\nonly penalized if you rank a good item in position greater than m+\\nor if you rank a bad item in a position less than or equal to m+. in order to solve this problem, you can follow a recipe similar to the naive approach sketched earlier. at training time, the biggest beyond binary classification 85 algorithm 19 ranktrain(drank, ω, binarytrain)\\n1: dbin ← [ ]\\n2: for all (x, σ) ∈ drank do\\nfor all u (cid:54)= v do\\ny ← sign(σv - σu)\\nw ← ω(σu, σv)\\ndbin ← dbin ⊕ (y, w, xuv) 3: 6: 4: 5: end for 7:\\n8: end for\\n9: return binarytrain(dbin) // y is +1 if u is prefered to v\\n// w is the cost of misclassiﬁcation algorithm 20 ranktest( f , ˆx, obj)\\n1: if obj contains 0 or 1 elements then\\n2:\\n3: else\\n4: return obj\\np ← randomly chosen object in obj\\nleft ← [ ]\\nright ← [ ]\\nfor all u ∈ obj \\\\{p} do left ← left ⊕ u\\nright ← right ⊕ u else end if\\nend for\\nleft ← ranktest( f , ˆx, left)\\nright ← ranktest( f , ˆx, right)\\nreturn left ⊕ (cid:104)p(cid:105) ⊕ right 17:\\n18: end if 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: // pick pivot\\n// elements that seem smaller than p\\n// elements that seem larger than p ˆy ← f (xup)\\nif uniform random variable < ˆy then // what is the probability that u precedes p // sort earlier elements\\n// sort later elements change is that you can weight each training example by how bad it\\nwould be to mess it up. this change is depicted in algorithm 6.3,\\nwhere the binary classiﬁcation data has weights w provided for saying\\nhow important a given example is. these weights are derived from\\nthe cost function ω. at test time, instead of predicting scores and then sorting the list,\\nyou essentially run the quicksort algorithm, using f as a comparison\\nfunction. at each step in algorithm 6.3, a pivot p is chosen. every\\nother object u is compared to p using f . if f thinks u is better, then it\\nis sorted on the left; otherwise it is sorted on the right. there is one\\nmajor difference between this algorithm and quicksort: the compar-\\nison function is allowed to be probabilistic. if f outputs probabilities,\\nfor instance it predicts that u has an 80% probability of being better\\nthan p, then it puts it on the left with 80% probability and on the\\nright with 20% probability. (the pseudocode is written in such a way\\nthat even if f just predicts −1, +1, the algorithm still works.) 86 a course in machine learning this algorithm is better than the naive algorithm in at least two\\nways. first, it only makes o(m log2 m) calls to f (in expectation),\\nrather than o(m2) calls in the naive case. second, it achieves a better\\nerror bound, shown below: theorem 7 (rank error bound). suppose the average binary error of f\\nis \\x01. then the ranking algorithm achieves a test error of at most 2\\x01 in the\\ngeneral case, and \\x01 in the bipartite case. 6.4 further reading todo further reading learning objectives:\\n• deﬁne and plot four surrogate loss functions: squared loss, logistic loss,\\nexponential loss and hinge loss. • compare and contrast the optimiza- tion of 0/1 loss and surrogate loss\\nfunctions. • solve the optimization problem for squared loss with a quadratic\\nregularizer in closed form. • implement and debug gradient descent and subgradient descent. dependencies: the essence of mathematics is not to make simple things compli-\\ncated, but to make complicated things simple.\\n– stanley gudder in chapter 4, you learned about the perceptron algorithm for\\nlinear classiﬁcation. this was both a model (linear classiﬁer) and al-\\ngorithm (the perceptron update rule) in one. in this section, we will\\nseparate these two, and consider general ways for optimizing lin-\\near models. this will lead us into some aspects of optimization (aka\\nmathematical programming), but not very far. at the end of this\\nchapter, there are pointers to more literature on optimization for\\nthose who are interested. the basic idea of the perceptron is to run a particular algorithm\\nuntil a linear separator is found. you might ask: are there better al-\\ngorithms for ﬁnding such a linear separator? we will follow this idea\\nand formulate a learning problem as an explicit optimization prob-\\nlem: ﬁnd me a linear separator that is not too complicated. we will\\nsee that ﬁnding an “optimal” separator is actually computationally\\nprohibitive, and so will need to “relax” the optimality requirement.\\nthis will lead us to a convex objective that combines a loss func-\\ntion (how well are we doing on the training data?) and a regularizer\\n(how complicated is our learned model?). this learning framework\\nis known as both tikhonov regularization and structural risk mini-\\nmization. ',\n",
       " ' you have already seen the perceptron as a way of ﬁnding a weight\\nvector w and bias b that do a good job of separating positive train-\\ning examples from negative training examples. the perceptron is a\\nmodel and algorithm in one. here, we are interested in separating\\nthese issues. we will focus on linear models, like the perceptron.\\nbut we will think about other, more generic ways of ﬁnding good\\nparameters of these models. the goal of the perceptron was to ﬁnd a separating hyperplane\\nfor some training data set. for simplicity, you can ignore the issue\\nof overﬁtting (but just for now!). not all data sets are linearly sepa- you should remember the yw ·\\ntrick from the perceptron discus-\\nsion. if not, re-convince yourself\\nthat this is doing the right thing. ? x 88 a course in machine learning rable. in the case that your training data isn’t linearly separable, you\\nmight want to ﬁnd the hyperplane that makes the fewest errors on\\nthe training data. we can write this down as a formal mathematics\\noptimization problem as follows: min\\nw,b ∑\\nn 1[yn(w · xn + b) > 0] (7.1) in this expression, you are optimizing over two variables, w and b.\\nthe objective function is the thing you are trying to minimize. in\\nthis case, the objective function is simply the error rate (or 0/1 loss) of\\nthe linear classiﬁer parameterized by w, b. in this expression, 1[·] is\\nthe indicator function: it is one when (·) is true and zero otherwise. we know that the perceptron algorithm is guaranteed to ﬁnd parameters for this model if the data is linearly separable. in other\\nwords, if the optimum of eq (7.1) is zero, then the perceptron will\\nefﬁciently ﬁnd parameters for this model. the notion of “efﬁciency”\\ndepends on the margin of the data for the perceptron. you might ask: what happens if the data is not linearly separable? is there an efﬁcient algorithm for ﬁnding an optimal setting of the\\nparameters? unfortunately, the answer is no. there is no polynomial\\ntime algorithm for solving eq (7.1), unless p=np. in other words,\\nthis problem is np-hard. sadly, the proof of this is quite complicated\\nand beyond the scope of this book, but it relies on a reduction from a\\nvariant of satisﬁability. the key idea is to turn a satisﬁability problem\\ninto an optimization problem where a clause is satisﬁed exactly when\\nthe hyperplane correctly separates the data. you might then come back and say: okay, well i don’t really need an exact solution. i’m willing to have a solution that makes one or\\ntwo more errors than it has to. unfortunately, the situation is really\\nbad. zero/one loss is np-hard to even appproximately minimize. in\\nother words, there is no efﬁcient algorithm for even ﬁnding a solution\\nthat’s a small constant worse than optimal. (the best known constant\\nat this time is 418/415 ≈ 1.007.) however, before getting too disillusioned about this whole enter-\\nprise (remember: there’s an entire chapter about this framework, so\\nit must be going somewhere!), you should remember that optimizing\\neq (7.1) perhaps isn’t even what you want to do! in particular, all it\\nsays is that you will get minimal training error. it says nothing about\\nwhat your test error will be like. in order to try to ﬁnd a solution that\\nwill generalize well to test data, you need to ensure that you do not\\noverﬁt the data. to do this, you can introduce a regularizer over the\\nparameters of the model. for now, we will be vague about what this\\nregularizer looks like, and simply call it an arbitrary function r(w, b). this leads to the following, regularized objective: min\\nw,b ∑\\nn 1[yn(w · xn + b) > 0] + λr(w, b) (7.2) in eq (7.2), we are now trying to optimize a trade-off between a so-\\nlution that gives low training error (the ﬁrst term) and a solution\\nthat is “simple” (the second term). you can think of the maximum\\ndepth hyperparameter of a decision tree as a form of regularization\\nfor trees. here, r is a form of regularization for hyperplanes. in this\\nformulation, λ becomes a hyperparameter for the optimization. the key remaining questions, given this formalism, are: • how can we adjust the optimization problem so that there are efﬁcient algorithms for solving it? • what are good regularizers r(w, b) for hyperplanes? • assuming we can adjust the optimization problem appropriately,\\nwhat algorithms exist for efﬁciently solving this regularized opti-\\nmization problem? we will address these three questions in the next sections. ',\n",
       " ' you might ask: why is optimizing zero/one loss so hard? intuitively,\\none reason is that small changes to w, b can have a large impact on\\nthe value of the objective function. for instance, if there is a positive\\ntraining example with w, x · +b = −0.0000001, then adjusting b up-\\nwards by 0.00000011 will decrease your error rate by 1. but adjusting\\nit upwards by 0.00000009 will have no effect. this makes it really\\ndifﬁcult to ﬁgure out good ways to adjust the parameters. to see this more clearly, it is useful to look at plots that relate margin to loss. such a plot for zero/one loss is shown in figure 7.1.\\nin this plot, the horizontal axis measures the margin of a data point\\nand the vertical axis measures the loss associated with that margin.\\nfor zero/one loss, the story is simple. if you get a positive margin\\n(i.e., y(w · x + b) > 0) then you get a loss of zero. otherwise you get\\na loss of one. by thinking about this plot, you can see how changes\\nto the parameters that change the margin just a little bit can have an\\nenormous effect on the overall loss. you might decide that a reasonable way to address this problem is to replace the non-smooth zero/one loss with a smooth approxima-\\ntion. with a bit of effort, you could probably concoct an “s”-shaped\\nfunction like that shown in figure 7.2. the beneﬁt of using such an\\ns-function is that it is smooth, and potentially easier to optimize. the\\ndifﬁculty is that it is not convex. linear models 89 ? assuming r does the “right thing,”\\nwhat value(s) of λ will lead to over-\\nﬁtting? what value(s) will lead to\\nunderﬁtting? figure 7.1: plot of zero/one versus\\nmargin figure 7.2: plot of zero/one versus\\nmargin and an s version of it 90 a course in machine learning if you remember from calculus, a convex function is one that looks\\nlike a happy face ((cid:94)). (on the other hand, a concave function is one\\nthat looks like a sad face ((cid:95)); an easy mnemonic is that you can hide\\nunder a concave function.) there are two equivalent deﬁnitions of\\na convex function. the ﬁrst is that it’s second derivative is always\\nnon-negative. the second, more geometric, deﬁtion is that any chord\\nof the function lies above it. this is shown in figure 7.3. there you\\ncan see a convex function and a non-convex function, both with two\\nchords drawn in. in the case of the convex function, the chords lie\\nabove the function. in the case of the non-convex function, there are\\nparts of the chord that lie below the function. convex functions are nice because they are easy to minimize. intu-\\nitively, if you drop a ball anywhere in a convex function, it will even-\\ntually get to the minimum. this is not true for non-convex functions.\\nfor example, if you drop a ball on the very left end of the s-function\\nfrom figure 7.2, it will not go anywhere. this leads to the idea of convex surrogate loss functions. since\\nzero/one loss is hard to optimize, you want to optimize something\\nelse, instead. since convex functions are easy to optimize, we want\\nto approximate zero/one loss with a convex function. this approxi-\\nmating function will be called a surrogate loss. the surrogate losses\\nwe construct will always be upper bounds on the true loss function:\\nthis guarantees that if you minimize the surrogate loss, you are also\\npushing down the real loss. there are four common surrogate loss functions, each with their own properties: hinge loss, logistic loss, exponential loss and\\nsquared loss. these are shown in figure 7.4 and deﬁned below.\\nthese are deﬁned in terms of the true label y (which is just {−1, +1})\\nand the predicted value ˆy = w · x + b. zero/one:\\nhinge: logistic: exponential:\\nsquared: (cid:96)(0/1)(y, ˆy) = 1[y ˆy ≤ 0]\\n(cid:96)(hin)(y, ˆy) = max{0, 1 − y ˆy}\\n(cid:96)(log)(y, ˆy) =\\n(cid:96)(exp)(y, ˆy) = exp[−y ˆy]\\n(cid:96)(sqr)(y, ˆy) = (y − ˆy)2 log 2 1 log (1 + exp[−y ˆy]) (7.3)\\n(7.4)\\n(7.5) (7.6)\\n(7.7) 1\\nlog 2 term out front is there sim- in the deﬁnition of logistic loss, the\\nply to ensure that (cid:96)(log)(y, 0) = 1. this ensures, like all the other\\nsurrogate loss functions, that logistic loss upper bounds the zero/one\\nloss. (in practice, people typically omit this constant since it does not\\naffect the optimization.) there are two big differences in these loss functions. the ﬁrst difference is how “upset” they get by erroneous predictions. in the figure 7.3: plot of convex and non-\\nconvex functions with two chords each figure 7.4: surrogate loss fns linear models 91 case of hinge loss and logistic loss, the growth of the function as ˆy\\ngoes negative is linear. for squared loss and exponential loss, it is\\nsuper-linear. this means that exponential loss would rather get a few\\nexamples a little wrong than one example really wrong. the other\\ndifference is how they deal with very conﬁdent correct predictions.\\nonce y ˆy > 1, hinge loss does not care any more, but logistic and\\nexponential still think you can do better. on the other hand, squared\\nloss thinks it’s just as bad to predict +3 on a positive example as it is\\nto predict −1 on a positive example. ',\n",
       " ' in our learning objective, eq (7.2), we had a term correspond to the\\nzero/one loss on the training data, plus a regularizer whose goal\\nwas to ensure that the learned function didn’t get too “crazy.” (or,\\nmore formally, to ensure that the function did not overﬁt.) if you re-\\nplace to zero/one loss with a surrogate loss, you obtain the following\\nobjective: min\\nw,b ∑\\nn (cid:96)(yn, w · xn + b) + λr(w, b) (7.8) the question is: what should r(w, b) look like? from the discussion of surrogate loss function, we would like to ensure that r is convex. otherwise, we will be back to the point\\nwhere optimization becomes difﬁcult. beyond that, a common desire\\nis that the components of the weight vector (i.e., the wds) should be\\nsmall (close to zero). this is a form of inductive bias. why are small values of wd good? or, more precisely, why do small values of wd correspond to simple functions? suppose that we\\nhave an example x with label +1. we might believe that other ex-\\namples, x(cid:48) that are nearby x should also have label +1. for example,\\nif i obtain x(cid:48) by taking x and changing the ﬁrst component by some\\nsmall value \\x01 and leaving the rest the same, you might think that the\\nclassiﬁcation would be the same. if you do this, the difference be-\\ntween ˆy and ˆy(cid:48) will be exactly \\x01w1. so if w1 is reasonably small, this\\nis unlikely to have much of an effect on the classiﬁcation decision. on\\nthe other hand, if w1 is large, this could have a large effect.\\nanother way of saying the same thing is to look at the derivative\\nof the predictions as a function of w1. the derivative of w · x + b with\\nrespect to w1 is:\\n∂ [w · x + b] ∂ [∑d wdxd + b] (7.9) = ∂w1 ∂w1 = x1 interpreting the derivative as the rate of change, we can see that\\nthe rate of change of the prediction function is proportional to the 92 a course in machine learning (cid:113) individual weights. so if you want the function to change slowly, you\\nwant to ensure that the weights stay small. one way to accomplish this is to simply use the norm of the ∑d w2 weight vector. namely r(norm)(w, b) = w =\\nis convex and smooth, which makes it easy to minimize. in prac-\\ntice, it’s often easier to use the squared norm, namely r(sqr)(w, b) =\\nw2 = ∑d w2\\nremains convex. an alternative to using the sum of squared weights\\nis to use the sum of absolute weights: r(abs)(w, b) = ∑d wd. both of\\nthese norms are convex. d because it removes the ugly square root term and d. this function in addition to small weights being good, you could argue that zero weights are better. if a weight wd goes to zero, then this means that\\nfeature d is not used at all in the classiﬁcation decision. if there are a\\nlarge number of irrelevant features, you might want as many weights\\nto go to zero as possible. this suggests an alternative regularizer:\\nr(cnt)(w, b) = ∑d 1[xd (cid:54)= 0]. this line of thinking leads to the general concept of p-norms. (technically these are called (cid:96)p (or “ell p”) norms, but this notation\\nclashes with the use of (cid:96) for “loss.”) this is a family of norms that all\\nhave the same general ﬂavor. we write wp to denote the p-norm of\\nw. (cid:32) (cid:33) 1 p wp = wdp ∑\\nd (7.10) you can check that the 2-norm exactly corresponds to the usual eu-\\nclidean norm, and that the 1-norm corresponds to the “absolute”\\nregularizer described above. when p-norms are used to regularize weight vectors, the interest- ing aspect is how they trade-off multiple features. to see the behavior\\nof p-norms in two dimensions, we can plot their contour (or level-\\nset). figure 7.5 shows the contours for the same p norms in two\\ndimensions. each line denotes the two-dimensional vectors to which\\nthis norm assignes a total value of 1. by changing the value of p, you\\ncan interpolate between a square (the so-called “max norm”), down\\nto a circle (2-norm), diamond (1-norm) and pointy-star-shaped-thing\\n(p < 1 norm). in general, smaller values of p “prefer” sparser vectors. you can see this by noticing that the contours of small p-norms “stretch”\\nout along the axes. it is for this reason that small p-norms tend to\\nyield weight vectors with many zero entries (aka sparse weight vec-\\ntors). unfortunately, for p < 1 the norm becomes non-convex. as\\nyou might guess, this means that the 1-norm is a popular choice for\\nsparsity-seeking applications. ? why do we not regularize the bias\\nterm b? ? why might you not want to use\\nr(cnt) as a regularizer? ? you can actually identify the r(cnt)\\nregularizer with a p-norm as well.\\nwhich value of p gives it to you?\\n(hint: you may have to take a limit.) figure 7.5: loss:norms2d: level sets of\\nthe same p-norms ? the max norm corresponds to\\nlimp→∞. why is this called the max\\nnorm? linear models 93 math review gradients\\na gradient is a multidimensional generalization of a derivative. suppose you have a function\\n: rd → r that takes a vector x = (cid:104)x1, x2, . . . , xd(cid:105) as input and produces a scalar value as output.\\nf\\nyou can differentite this function according to any one of the inputs; for instance, you can compute ∂ f\\n∂x5\\nto get the derivative with respect to the ﬁfth input. the gradient of f is just the vector consisting of the\\nderivative f with respect to each of its input coordinates independently, and is denoted ∇ f , or, when\\nthe input to f is ambiguous, ∇x f . this is deﬁned as: ∇x f = , ∂ f\\n∂x2 , . . . , ∂ f\\n∂xd (cid:29) (cid:28) ∂ f\\n(cid:68) ∂x1 (7.11) 1 + 5x1x2 − 3x2x2 3. the gradient is: for example, consider the function f (x1, x2, x3) = x3\\n∇x f = 1 + 5x2 , 5x1 − 3x2\\n3x2 3 , − 6x2x3 (cid:69) (7.12)\\nnote that if f : rd → r, then ∇ f : rd → rd. if you evaluate ∇ f (x), this will give you the gradient at\\nx, a vector in rd. this vector can be interpreted as the direction of steepest ascent: namely, if you were\\nto travel an inﬁnitesimal amount in the direction of the gradient, you would go uphill (i.e., increase f )\\nthe most. ',\n",
       " ' figure 7.6: envision the following problem. you’re taking up a new hobby:\\nblindfolded mountain climbing. someone blindfolds you and drops\\nyou on the side of a mountain. your goal is to get to the peak of the\\nmountain as quickly as possible. all you can do is feel the mountain\\nwhere you are standing, and take steps. how would you get to the\\ntop of the mountain? perhaps you would feel to ﬁnd out what direc-\\ntion feels the most “upward” and take a step in that direction. if you\\ndo this repeatedly, you might hope to get the the top of the moun-\\ntain. (actually, if your friend promises always to drop you on purely\\nconcave mountains, you will eventually get to the peak!) the idea of gradient-based methods of optimization is exactly the same. suppose you are trying to ﬁnd the maximum of a function\\nf (x). the optimizer maintains a current estimate of the parameter of\\ninterest, x. at each step, it measures the gradient of the function it is\\ntrying to optimize. this measurement occurs at the current location,\\nx. call the gradient g. it then takes a step in the direction of the\\ngradient, where the size of the step is controlled by a parameter η\\n(eta). the complete step is x ← x + ηg. this is the basic idea of\\ngradient ascent. the opposite of gradient ascent is gradient descent. all of our 94 a course in machine learning algorithm 21 gradientdescent(f, k, η1, . . . )\\n1: z(0) ← (cid:104)0, 0, . . . , 0(cid:105)\\n2: for k = 1 . . . k do\\ng(k) ← ∇zfz(k-1)\\n3:\\nz(k) ← z(k-1) − η(k)g(k) // initialize variable we are optimizing // compute gradient at current location\\n// take a step down the gradient 4:\\n5: end for\\n6: return z(k) learning problems will be framed as minimization problems (trying\\nto reach the bottom of a ditch, rather than the top of a hill). there-\\nfore, descent is the primary approach you will use. one of the major\\nconditions for gradient ascent being able to ﬁnd the true, global min-\\nimum, of its objective function is convexity. without convexity, all is\\nlost.\\nthe gradient descent algorithm is sketched in algorithm 7.4.\\nthe function takes as arguments the function f to be minimized,\\nthe number of iterations k to run and a sequence of learning rates\\nη1, . . . , ηk. (this is to address the case that you might want to start\\nyour mountain climbing taking large steps, but only take small steps\\nwhen you are close to the peak.) the only real work you need to do to apply a gradient descent method is be able to compute derivatives. for concreteness, suppose\\nthat you choose exponential loss as a loss function and the 2-norm as\\na regularizer. then, the regularized objective function is: exp(cid:2) − yn(w · xn + b)(cid:3) + l(w, b) = ∑\\nn w2 λ\\n2 (7.13) the only “strange” thing in this objective is that we have replaced\\nλ with λ\\n2 . the reason for this change is just to make the gradients\\ncleaner. we can ﬁrst compute derivatives with respect to b: ∂l\\n∂b exp(cid:2) − yn(w · xn + b)(cid:3) +\\nexp(cid:2) − yn(w · xn + b)(cid:3) + 0\\n(cid:19)\\n(cid:18) ∂\\nyn exp(cid:2) − yn(w · xn + b)(cid:3) − yn(w · xn + b) ∂b = ∑\\nn\\n∂\\n∂b ∂\\n∂b\\n= ∑\\nn\\n= ∑\\nn\\n= − ∑\\nn ∂\\n∂b w2 λ\\n2 exp(cid:2) − yn(w · xn + b)(cid:3) (7.14) (7.15) (7.16) (7.17) before proceeding, it is worth thinking about what this says. from a\\npractical perspective, the optimization will operate by updating b ←\\nb − η ∂l\\n∂b . consider positive examples: examples with yn = +1. we\\nwould hope for these examples that the current prediction, w · xn + b,\\nis as large as possible. as this value tends toward ∞, the term in the\\nexp[] goes to zero. thus, such points will not contribute to the step. linear models 95 ? this considered the case of posi-\\ntive examples. what happens with\\nnegative examples? however, if the current prediction is small, then the exp[] term will\\nbe positive and non-zero. this means that the bias term b will be\\nincreased, which is exactly what you would want. moreover, once all\\npoints are very well classiﬁed, the derivative goes to zero. now that we have done the easy case, let’s do the gradient with n exp(cid:2) − yn(w · xn + b)(cid:3) + ∇w\\n(∇w − yn(w · xn + b)) exp(cid:2) − yn(w · xn + b)(cid:3) + λw\\nynxn exp(cid:2) − yn(w · xn + b)(cid:3) + λw w2 λ\\n2 (7.18) (7.19)\\n(7.20) respect to w. ∇wl = ∇w ∑ = ∑\\nn = − ∑\\nn now you can repeat the previous exercise. the update is of the form\\nw ← w − η∇wl. for well classiﬁed points (ones that tend toward\\nyn∞), the gradient is near zero. for poorly classiﬁed points, the gra-\\ndient points in the direction −ynxn, so the update is of the form\\nw ← w + cynxn, where c is some constant. this is just like the per-\\nceptron update! note that c is large for very poorly classiﬁed points\\nand small for relatively well classiﬁed points.\\nby looking at the part of the gradient related to the regularizer,\\nthe update says: w ← w − λw = (1 − λ)w. this has the effect of\\nshrinking the weights toward zero. this is exactly what we expect the\\nregulaizer to be doing! the success of gradient descent hinges on appropriate choices\\nfor the step size. figure 7.7 shows what can happen with gradient\\ndescent with poorly chosen step sizes. if the step size is too big, you\\ncan accidentally step over the optimum and end up oscillating. if the\\nstep size is too small, it will take way too long to get to the optimum.\\nfor a well-chosen step size, you can show that gradient descent will\\napproach the optimal value at a fast rate. the notion of convergence\\nhere is that the objective value converges to the true minimum. figure 7.7: good and bad step sizes theorem 8 (gradient descent convergence). under suitable condi-\\ntions1, for an appropriately chosen constant step size (i.e., η1 = η2,· · · =\\nη), the convergence rate of gradient descent is o(1/k). more speciﬁ-\\ncally, letting z∗ be the global minimum of f, we have: f (z(k)) − f (z∗) ≤\\n2z(0)−z∗2 ηk . 1 speciﬁcally the function to be opti-\\nmized needs to be strongly convex.\\nthis is true for all our problems, pro-\\nvided λ > 0. for λ = 0 the rate could\\nbe as bad as o(1/ √ k). the proof of this theorem is a bit complicated because it makes heavy use of some linear algebra. the key is to set the learning rate\\nto 1/l, where l is the maximum curvature of the function that is\\nbeing optimized. the curvature is simply the “size” of the second\\nderivative. functions with high curvature have gradients that change ? a naive reading of this theorem\\nseems to say that you should choose\\nhuge values of η. it should be obvi-\\nous that this cannot be right. what\\nis missing? 96 a course in machine learning quickly, which means that you need to take small steps to avoid\\noverstepping the optimum. this convergence result suggests a simple approach to decid- ing when to stop optimizing: wait until the objective function stops\\nchanging by much. an alternative is to wait until the parameters stop\\nchanging by much. a ﬁnal example is to do what you did for percep-\\ntron: early stopping. every iteration, you can check the performance\\nof the current model on some held-out data, and stop optimizing\\nwhen performance plateaus. ',\n",
       " ' as a good exercise, you should try deriving gradient descent update\\nrules for the different loss functions and different regularizers you’ve\\nlearned about. however, if you do this, you might notice that hinge\\nloss and the 1-norm regularizer are not differentiable everywhere! in\\nparticular, the 1-norm is not differentiable around wd = 0, and the\\nhinge loss is not differentiable around y ˆy = 1. the solution to this is to use subgradient optimization. one way\\nto think about subgradients is just to not think about it: you essen-\\ntially need to just ignore the fact that you forgot that your function\\nwasn’t differentiable, and just try to apply gradient descent anyway.\\nz}. this function is differentiable for z > 1 and differentiable for\\nz < 1, but not differentiable at z = 1. you can derive this using\\ndifferentiation by parts: to be more concrete, consider the hinge function f (z) = max{0, 1− ∂\\n∂z 0\\n1 − z (cid:40)\\n(cid:40) ∂\\n∂z 0\\n(cid:40)\\n∂z (1 − z)\\n∂\\nif z ≥ 1\\n0\\n−1 if z < 1 if z > 1\\nif z < 1 if z > 1\\nif z < 1 ∂\\n∂z f (z) = = = (7.21) (7.22) (7.23) thus, the derivative is zero for z < 1 and −1 for z > 1, matching intuition from the figure. at the non-differentiable point, z = 1,\\nwe can use a subderivative: a generalization of derivatives to non-\\ndifferentiable functions. intuitively, you can think of the derivative\\nof f at z as the tangent line. namely, it is the line that touches f at\\nz that is always below f (for convex functions). the subderivative,\\ndenoted ∂∂∂ f , is the set of all such lines. at differentiable positions,\\nthis set consists just of the actual derivative. at non-differentiable\\npositions, this contains all slopes that deﬁne lines that always lie\\nunder the function and make contact at the operating point. this is figure 7.8: hinge loss with sub linear models 97 , algorithm 22 hingeregularizedgd(d, λ, maxiter)\\n1: w ← (cid:104)0, 0, . . . 0(cid:105)\\n2: for iter = 1 . . . maxiter do\\ng ← (cid:104)0, 0, . . . 0(cid:105)\\n,\\n3:\\nfor all (x,y) ∈ d do b ← 0\\ng ← 0\\nif y(w · x + b) ≤ 1 then 4: // initialize weights and bias // initialize gradient of weights and bias 5: 6: 7: g ← g + y x\\ng ← g + y 9: 8: 10: end if\\nend for\\ng ← g − λw\\n11: w ← w + ηg\\nb ← b + ηg 12:\\n13: end for\\n14: return w, b // update weight gradient\\n// update bias derivative // add in regularization term\\n// update weights\\n// update bias shown pictorally in figure 7.8, where example subderivatives are\\nshown for the hinge loss function. in the particular case of hinge loss,\\nany value between 0 and −1 is a valid subderivative at z = 0. in fact,\\nthe subderivative is always a closed set of the form [a, b], where a and\\nb can be derived by looking at limits from the left and right. this gives you a way of computing derivative-like things for non-\\ndifferentiable functions. take hinge loss as an example. for a given\\nexample n, the subgradient of hinge loss can be computed as: (cid:40) 0\\n1 − yn(w · xn + b) otherwise = ∂∂∂w ∂∂∂w max{0, 1 − yn(w · xn + b)}\\n(cid:40)\\n(cid:40) ∂∂∂w0\\n∂∂∂w1 − yn(w · xn + b) otherwise\\n0\\n−ynxn otherwise if yn(w · xn + b) > 1 = = if yn(w · xn + b) > 1 if yn(w · xn + b) > 1 (7.24) (7.25) (7.26) (7.27) if you plug this subgradient form into algorithm 7.4, you obtain\\nalgorithm 7.5. this is the subgradient descent for regularized hinge\\nloss (with a 2-norm regularizer). ',\n",
       " ' although gradient descent is a good, generic optimization algorithm,\\nthere are cases when you can do better. an example is the case of a\\n2-norm regularizer and squared error loss function. for this, you can\\nactually obtain a closed form solution for the optimal weights. how-\\never, to obtain this, you need to rewrite the optimization problem in\\nterms of matrix operations. for simplicity, we will only consider the 98 a course in machine learning math review matrix multiplication and inversion\\nif a and b are matrices, and a is n×k and b is k×m (the inner dimensions must match), then the ma-\\ntrix product ab is a matrix c that is n× m, with cn,m = ∑k an,kbk,m. if v is a vector in rd, we will\\ntreat is as a column vector, or a matrix of size d×1. thus, av is well deﬁned if a is d×m, and the result-\\ning product is a vector u with um = ∑d ad,mvd. aside from matrix product, a fundamental matrix operation is inversion. we will often encounter a\\nform like ax = y, where a and y are known and we want to solve for a. if a is square of size n×n,\\nthen the inverse of a, denoted a−1, is also a square matrix of size n×n, such that aa−1 = in = a−1a.\\ni.e., multiplying a matrix by its inverse (on either side) gives back the identity matrix. using this, we\\ncan solve ax = y by multiplying both sides by a−1 on the left (recall that order matters in matrix mul-\\ntiplication), yielding a−1ax = a−1y from which we can conclude x = a−1y. note that not all square\\nmatrices are invertible. for instance, the all zeros matrix does not have an inverse (in the same way\\nthat 1/0 is not deﬁned for scalars). however, there are other matrices that do not have inverses; such\\nmatrices are called singular. figure 7.9: unbiased version, but the extension is exercise ??. this is precisely the\\nlinear regression setting.\\nyou can think of the training data as a large matrix x of size n×d, where xn,d is the value of the dth feature on the nth example. you\\ncan think of the labels as a column (“tall”) vector y of dimension n.\\nfinally, you can think of the weights as a column vector w of size\\nd. thus, the matrix-vector product a = xw has dimension n. in\\nparticular: an = [xw]n = ∑ d xn,dwd (7.28) this means, in particular, that a is actually the predictions of the\\nmodel. instead of calling this a, we will call it ˆy. the squared error\\n∑n( ˆyn − yn)2, which can be written\\nsays that we should minimize 1\\n2\\nin vector form as a minimization of 1\\n2\\nthis can be expanded visually as: ? verify that the squared error can\\nactually be written as this vector\\nnorm. \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 (cid:124) x1,1\\nx2,1\\n...\\nxn,1 . . .\\n. . .\\n...\\n. . . x1,d\\nx2,d\\n...\\nxn,d x1,2\\nx2,2\\n...\\nxn,2 (cid:123)(cid:122) x (cid:12)(cid:12)(cid:12)(cid:12) ˆy − y(cid:12)(cid:12)(cid:12)(cid:12)2.\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb = (cid:125) (cid:124) ∑d x1,dwd\\n∑d x2,dwd ∑d xn,dwd ...\\n(cid:123)(cid:122) ˆy \\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb (cid:125) \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 (cid:124) w1\\nw2\\n...\\n(cid:123)(cid:122)\\nwd\\nw \\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb (cid:125) ≈ \\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 y1\\ny2\\n...\\n(cid:124) (cid:123)(cid:122) (cid:125)\\nyn ˆy\\n(7.29) linear models 99 so, compactly, our optimization problem can be written as: min\\nw l(w) = 1\\n2 xw − y2 + w2 λ\\n2 (7.30) if you recall from calculus, you can minimize a function by setting its\\nderivative to zero. we start with the weights w and take gradients: ∇wl(w) = x(cid:62) (xw − y) + λw\\n(cid:16)\\n= x(cid:62)xw − x(cid:62)y + λw\\nw − x(cid:62)y\\n= (cid:17) x(cid:62)x + λi\\n(cid:17)\\nx(cid:62)x + λi\\n(cid:16)\\nx(cid:62)x + λid (cid:17)\\nw − x(cid:62)y = 0\\nw = x(cid:62)y\\nx(cid:62)x + λid (cid:17)−1x(cid:62)y we can equate this to zero and solve, yielding: (cid:16)\\n⇐⇒ (cid:16) ⇐⇒ w = (7.31)\\n(7.32)\\n(7.33) (7.34)\\n(7.35)\\n(7.36) ? for those who are keen on linear\\nalgebra, you might be worried that\\nthe matrix you must invert might\\nnot be invertible. is this actually a\\nproblem? thus, the optimal solution of the weights can be computed by a few\\nmatrix multiplications and a matrix inversion. as a sanity check,\\nyou can make sure that the dimensions match. the matrix x(cid:62)x has\\ndimension d×d, and therefore so does the inverse term. the inverse\\nis d×d and x(cid:62) is d×n, so that product is d×n. multiplying through\\nby the n×1 vector y yields a d×1 vector, which is precisely what we\\nwant for the weights. note that this gives an exact solution, modulo numerical innacu- racies with computing matrix inverses. in contrast, gradient descent\\nwill give you progressively better solutions and will “eventually”\\nconverge to the optimum at a rate of 1/k. this means that if you\\nwant an answer that’s within an accuracy of \\x01 = 10−4, you will need\\nsomething on the order of one thousand steps.\\nthe question is whether getting this exact solution is always more\\nefﬁcient. to run gradient descent for one step will take o(nd) time,\\nwith a relatively small constant. you will have to run k iterations,\\nyielding an overall runtime of o(knd). on the other hand, the\\nclosed form solution requires constructing x(cid:62)x, which takes o(d2n)\\ntime. the inversion take o(d3) time using standard matrix inver-\\nsion routines. the ﬁnal multiplications take o(nd) time. thus, the\\noverall runtime is on the order o(d3 + d2n). in most standard cases\\n(though this is becoming less true over time), n > d, so this is domi-\\nnated by o(d2n). thus, the overall question is whether you will need to run more\\nthan d-many iterations of gradient descent. if so, then the matrix\\ninversion will be (roughly) faster. otherwise, gradient descent will\\nbe (roughly) faster. for low- and medium-dimensional problems (say, 100 a course in machine learning d ≤ 100), it is probably faster to do the closed form solution via\\nmatrix inversion. for high dimensional problems (d ≥ 10, 000), it is\\nprobably faster to do gradient descent. for things in the middle, it’s\\nhard to say for sure. ',\n",
       " ' at the beginning of this chapter, you may have looked at the convex\\nsurrogate loss functions and asked yourself: where did these come\\nfrom?! they are all derived from different underlying principles,\\nwhich essentially correspond to different inductive biases. let’s start by thinking back to the original goal of linear classiﬁers: to ﬁnd a hyperplane that separates the positive training examples\\nfrom the negative ones. figure 7.10 shows some data and three po-\\ntential hyperplanes: red, green and blue. which one do you like best? most likely you chose the green hyperplane. and most likely you\\nchose it because it was furthest away from the closest training points.\\nin other words, it had a large margin. the desire for hyperplanes\\nwith large margins is a perfect example of an inductive bias. the data\\ndoes not tell us which of the three hyperplanes is best: we have to\\nchoose one using some other source of information. following this line of thinking leads us to the support vector ma- chine (svm). this is simply a way of setting up an optimization\\nproblem that attempts to ﬁnd a separating hyperplane with as large\\na margin as possible. it is written as a constrained optimization\\nproblem: figure 7.10: picture of data points with\\nthree hyperplanes, rgb with g the best 1 γ(w, b) min\\nw,b subj. to yn (w · xn + b) ≥ 1 (7.37) (∀n) in this optimization, you are trying to ﬁnd parameters that maximize\\nthe margin, denoted γ, (i.e., minimize the reciprocal of the margin)\\nsubject to the constraint that all training examples are correctly classi-\\nﬁed. the “odd” thing about this optimization problem is that we re- quire the classiﬁcation of each point to be greater than one rather than\\nsimply greater than zero. however, the problem doesn’t fundamen-\\ntally change if you replace the “1” with any other positive constant\\n(see exercise ??). as shown in figure 7.11, the constant one can be\\ninterpreted visually as ensuring that there is a non-trivial margin\\nbetween the positive points and negative points. the difﬁculty with the optimization problem in eq (7.37) is what\\nhappens with data that is not linearly separable. in that case, there\\nis no set of parameters w, b that can simultaneously satisfy all the figure 7.11: hyperplane with margins\\non sides linear models 101 figure 7.12: one bad point with slack constraints. in optimization terms, you would say that the feasible\\nregion is empty. (the feasible region is simply the set of all parame-\\nters that satify the constraints.) for this reason, this is refered to as\\nthe hard-margin svm, because enforcing the margin is a hard con-\\nstraint. the question is: how to modify this optimization problem so\\nthat it can handle inseparable data. the key idea is the use of slack parameters. the intuition behind\\nslack parameters is the following. suppose we ﬁnd a set of param-\\neters w, b that do a really good job on 9999 data points. the points\\nare perfectly classifed and you achieve a large margin. but there’s\\none pesky data point left that cannot be put on the proper side of the\\nmargin: perhaps it is noisy. (see figure 7.12.) you want to be able\\nto pretend that you can “move” that point across the hyperplane on\\nto the proper side. you will have to pay a little bit to do so, but as\\nlong as you aren’t moving a lot of points around, it should be a good\\nidea to do this. in this picture, the amount that you move the point is\\ndenoted ξ (xi). by introducing one slack parameter for each training example,\\nand penalizing yourself for having to use slack, you can create an\\nobjective function like the following, soft-margin svm: + c ∑\\nn ξn γ(w, b) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\\nsubj. to yn (w · xn + b) ≥ 1 − ξn large margin small slack 1 min\\nw,b,ξ ξn ≥ 0 (7.38) (∀n)\\n(∀n) the goal of this objective function is to ensure that all points are\\ncorrectly classiﬁed (the ﬁrst constraint). but if a point n cannot be\\ncorrectly classiﬁed, then you can set the slack ξn to something greater\\nthan zero to “move” it in the correct direction. however, for all non-\\nzero slacks, you have to pay in the objective function proportional to\\nthe amount of slack. the hyperparameter c > 0 controls overﬁtting\\nversus underﬁtting. the second constraint simply says that you must\\nnot have negative slack. one major advantage of the soft-margin svm over the original\\nhard-margin svm is that the feasible region is never empty. that is,\\nthere is always going to be some solution, regardless of whether your\\ntraining data is linearly separable or not. it’s one thing to write down an optimization problem. it’s another thing to try to solve it. there are a very large number of ways to\\noptimize svms, essentially because they are such a popular learning\\nmodel. here, we will talk just about one, very simple way. more\\ncomplex methods will be discussed later in this book once you have a\\nbit more background. ? what values of c will lead to over-\\nﬁtting? what values will lead to\\nunderﬁtting? ? suppose i give you a data set.\\nwithout even looking at the data,\\nconstruct for me a feasible solution\\nto the soft-margin svm. what is\\nthe value of the objective for this\\nsolution? 102 a course in machine learning by this observation, there is some positive example that that lies to make progress, you need to be able to measure the size of the\\nmargin. suppose someone gives you parameters w, b that optimize\\nthe hard-margin svm. we wish to measure the size of the margin.\\nthe ﬁrst observation is that the hyperplane will lie exactly halfway\\nbetween the nearest positive point and nearest negative point. if not,\\nthe margin could be made bigger by simply sliding it one way or the\\nother by adjusting the bias b.\\nexactly 1 unit from the hyperplane. call it x+, so that w · x+ + b = 1.\\nsimilarly, there is some negative example, x−, that lies exactly on\\nthe other side of the margin: for which w · x− + b = −1. these two\\npoints, x+ and x− give us a way to measure the size of the margin.\\nas shown in figure 7.11, we can measure the size of the margin by\\nlooking at the difference between the lengths of projections of x+\\nand x− onto the hyperplane. since projection requires a normalized\\nvector, we can measure the distances as: figure 7.13: copy of ﬁgure from p5 of\\ncs544 svm tutorial d+ =\\nd− = − 1 1 w w · x+ + b − 1\\nw w · x− − b + 1\\n(cid:2)d+ − d−(cid:3)\\n(cid:20) 1\\nw w · x−(cid:21)\\n(cid:20) 1\\nw w · x− − b + 1\\nw w · x+ + b − 1 − 1\\n(cid:20) 1\\n(cid:21)\\nw w · x+ − 1\\nw (+1) − 1 w (−1) we can then compute the margin by algebra: γ = = = = = 1\\n2\\n1\\n2\\n1\\n2\\n1\\n2\\n1\\nw (cid:21) (7.39) (7.40) (7.41) (7.42) (7.43) (7.44) (7.45) this is a remarkable conclusion: the size of the margin is inversely\\nproportional to the norm of the weight vector. thus, maximizing the\\nmargin is equivalent to minimizing w! this serves as an addi-\\ntional justiﬁcation of the 2-norm regularizer: having small weights\\nmeans having large margins! however, our goal wasn’t to justify the regularizer: it was to un-\\nderstand hinge loss. so let us go back to the soft-margin svm and\\nplug in our new knowledge about margins: min\\nw,b,ξ w2 (cid:124) (cid:123)(cid:122) (cid:125) 1\\n2\\nlarge margin + c ∑\\nn (cid:124) (cid:123)(cid:122) (cid:125) ξn small slack (7.46) linear models 103 subj. to yn (w · xn + b) ≥ 1 − ξn ξn ≥ 0 (∀n)\\n(∀n) now, let’s play a thought experiment. suppose someone handed\\nyou a solution to this optimization problem that consisted of weights\\n(w) and a bias (b), but they forgot to give you the slacks. could you\\nrecover the slacks from the information you have? in fact, the answer is yes! for simplicity, let’s consider positive examples. suppose that you look at some positive example xn. you\\nneed to ﬁgure out what the slack, ξn, would have been. there are two\\ncases. either w · xn + b is at least 1 or it is not. if it’s large enough,\\nthen you want to set ξn = 0. why? it cannot be less than zero by the\\nsecond constraint. moreover, if you set it greater than zero, you will\\n“pay” unnecessarily in the objective. so in this case, ξn = 0. next,\\nsuppose that w · xn + b = 0.2, so it is not big enough. in order to\\nsatisfy the ﬁrst constraint, you’ll need to set ξn ≥ 0.8. but because\\nof the objective, you’ll not want to set it any larger than necessary, so\\nyou’ll set ξn = 0.8 exactly. following this argument through for both positive and negative points, if someone gives you solutions for w, b, you can automatically\\ncompute the optimal ξ variables as: (cid:40) ξn = 0\\n1 − yn(w · xn + b) otherwise if yn(w · xn + b) ≥ 1 (7.47) in other words, the optimal value for a slack variable is exactly the\\nhinge loss on the corresponding example! thus, we can write the\\nsvm objective as an unconstrained optimization problem: min\\nw,b w2 (cid:124) (cid:123)(cid:122) (cid:125) 1\\n2\\nlarge margin + c ∑\\nn (cid:124) (cid:125)\\n(cid:96)(hin)(yn, w · xn + b) (cid:123)(cid:122) small slack (7.48) multiplying this objective through by λ/c, we obtain exactly the reg-\\nularized objective from eq (7.8) with hinge loss as the loss function\\nand the 2-norm as the regularizer! 7.8 further reading todo further reading science and everyday life cannot\\nand should not be separated. – rosalind franklin learning objectives:\\n• at the end of chapter 1, you saw the “russian tank” example of\\na biased data set leading to a classiﬁer that seemed like it was doing\\nwell, but was really relying on some artifact of the data collection\\nprocess. as machine learning algorithms have a greater and greater\\nimpact on the real world, it is crucially important to ensure that they\\nare making decisions based on the “right” aspects of the input, rather\\nthan exploiting arbitrary idiosyncracies of a particular training set.\\nfor the rest of this chapter, we will consider two real world ex-\\namples of bias issues that have had signiﬁcant impact: the effect of\\ngender in speech recognition systems and the effect of race in pre-\\ndicting criminal recidivism (i.e., will a convicted criminal commit\\nfurther crimes if released).1 the gender issue is that early speech\\nrecognition systems in cars failed to recognized the voices of many\\npeople who were not men. the race issue is that a speciﬁc recidivism\\npredictor based on standard learning algorithms was biased against\\nminorities. ',\n",
       " ' one of the most common issues in bias is a mismatch between the\\ntraining distribution and the testing distribution. in the running\\nexample of speech recognition failing to work on many non-men\\nspeakers, a large part of this happened because most of the training\\ndata on which the speech recognition system was trained was spoken\\nby men. the learning algorithm learned—very well—how to recog-\\nnize men’s speech, but its accuracy dropped signiﬁcantly when faced\\nwith a different distribution of test data. to understand why this happens, recall the bayes optimal clas-\\nsiﬁer from chapter 2. this was the classiﬁer than magically know\\nthe data distribution d, and then when presented with an example x\\npredicted argmaxy d(x, y). this was optimal, but only because d was\\nthe correct distribution. even if one has access to the true distribu-\\ntion for male speakers, say d(male), the bayes optimal classiﬁer under dependencies: chapter 1,chap-\\nter 3,chapter 4,chapter 5\\n1 see autoblog and propublica for press\\ncoverage of these two issues. bias and fairness 105 d(male) will generally not be optimal under the distribution for any\\nother gender. another example occurs in sentiment analysis. it is common to train sentiment analysis systems on data collected from reviews:\\nproduct reviews, restaurant reviews, movie reviews, etc. this data is\\nconvenient because it includes both text (the review itself) and also\\na rating (typically one to ﬁve stars). this yields a “free” dataset for\\ntraining a model to predict rating given text. however, one should\\nbe wary when running such a sentiment classiﬁer on text other than\\nthe types of reviews it was trained on. one can easily imagine that\\na sentiment analyzer trained on movie reviews may not work so\\nwell when trying to detect sentiment in a politician’s speech. even\\nmoving between one type of product and another can fail wildly:\\n“very small” typically expresses positive sentiment for usb drives,\\nbut not for hotel rooms. the issue of train/test mismatch has been widely studied under many different names: covariate shift (in statistics, the input features\\nare called “covariates”), sample selection bias and domain adapta-\\ntion are the most common. we will refer to this problem simply as\\nadaptation. the adaptation challenge is: given training data from one\\n“old” distribution, learn a classiﬁer that does a good job on another\\nrelated, but different, “new” distribution. it’s important to recognize that in general, adaptation is im-\\npossible. for example, even if the task remains the same (posi-\\ntive/negative sentiment), if the old distribution is text reviews and\\nthe new distribution is images of text reviews, it’s hard to imagine\\nthat doing well on the old distribution says anything about the new.\\nas a less extreme example, if the old distribution is movie reviews in\\nenglish and the new distribution is movie reviews in mandarin, it’s\\nunlikely that adaptation will be easy. these examples give rise to the tension in adaptation problems. 1. what does it mean for two distributions to be related? we might believe that “reviews of dvds” and “reviews of movies” are\\n“highly related” (though somewhat different: dvd reviews of-\\nten discuss bonus features, quality, etc.); while “reviews of hotels”\\nand “reviews of political policies” are “less related.” but how can\\nwe formalize this? 2. when two distributions are related, how can we build models that effectively share information between them? 106 a course in machine learning ',\n",
       " ' the ﬁrst type of adaptation we will cover is unsupervised adapta-\\ntion. the setting is the following. there are two distributions, dold\\nand dnew. we have labeled training data from dold, say (x1, y1), . . . , (xn, yn)\\ntotalling n examples. we also have m many unlabeled examples from\\ndnew: z1, . . . , zm. we assume that the examples live in the same\\nspace, rd. this is called unsupervised adaptation because we do not\\nhave access to any labels in the new distribution.2\\nour goal is to learn a classiﬁer f that achieves low expected loss\\nunder the new distribution, dnew. the challenge is that we do not have\\naccess to any labeled data from dnew. as a warm-up, let’s suppose\\nthat we have a black box machine learning algorithm a that takes\\nin weighted examples and produces a classiﬁer. at the very least,\\nthis can be achieved using either undersampling or oversampling\\n(see section 6.1). we’re going to attempt to reweigh the (old distri-\\nbution) labeled examples based on how similar they are to the new\\ndistribution. this is justiﬁed using the importance sampling trick for\\nswitching expectations: 2 sometimes this is called semi-super-\\nvised adaptation in the literature. test loss\\n= e\\n= ∑\\n(x,y)\\n= ∑\\n(x,y)\\n= ∑\\n(x,y) (x,y)∼dnew [(cid:96)(y, f (x))]\\ndnew(x, y)(cid:96)(y, f (x))\\ndold(x, y)\\ndold(x, y)\\ndnew(x, y)\\ndold(x, y) dnew(x, y) dold(x, y) (cid:20)dnew(x, y) dold(x, y) = e (x,y)∼dold deﬁnition expand expectation (8.1)\\n(8.2)\\n(8.3) (cid:96)(y, f (x)) times one (8.4) (cid:96)(y, f (x)) (cid:21) (cid:96)(y, f (x)) rearrange (8.5) deﬁnition (8.6) what we have achieved here is rewriting the test loss, which is an expectation over dnew, as an expectation over dold instead.3 this\\nis useful because we have access to labeled examples from dold\\nbut not dnew. the implicit suggested algorithm by this analysis\\nto to train a classiﬁer using our learning algorithm a, but where\\neach training example (xn, yn) is weighted according to the ratio\\ndnew(xn, yn)/dold(xn, yn). intuitively, this makes sense: the classiﬁer\\nis being told to pay more attention to training examples that have\\nhigh probability under the new distribution, and less attention to\\ntraining that have low probability under the new distribution.\\nthe problem with this approach is that we do not have access to\\ndnew or dold, so we cannot compute this ratio and therefore cannot\\nrun this algorithm. one approach to this problem is to try to explic- 3 in this example, we assumed a discrete\\ndistribution; if the distributions are con-\\ntinuous, the sums are simply replaced\\nwith integrals. bias and fairness 107 itly estimate these distributions, a task known as density estimation.\\nthis is an incredibly difﬁcult problem; far harder than the original\\nadaptation problem. a solution to this problem is to try to estimate the ratio directly,\\nrather than separately estimating the two probability distributions4.\\nthe key idea is to think of the adaptation as follows. all examples\\nare drawn according to some ﬁxed base distribution dbase. some\\nof these are selected to go into the new distribution, and some of\\nthem are selected to go into the old distribution. the mechanism for\\ndeciding which ones are kept and which are thrown out is governed\\nby a selection variable, which we call s. the choice of selection-or-\\nnot, s, is based only on the input example x and not on it’s label.\\nparticular, we deﬁne: in 4 bickel et al. 2007 ? what could go wrong if s got to\\nlook at the label, too? dold(x, y) ∝ dbase(x, y)p(s = 1 x)\\n(8.7)\\ndnew(x, y) ∝ dbase(x, y)p(s = 0 x)\\n(8.8)\\nthat is, the probability of drawing some pair (x, y) in the old distri-\\nbution is proportional to the probability of ﬁrst drawing that example\\naccording to the base distribution, and then the probability of se-\\nlecting that particular example into the old distribution. if we can\\nsuccessfully estimate p(s = 1 x), then the ratio that we sought, then\\nwe can compute the importance ratio as: dnew(x, y)\\ndold(x, y) = = = z = z = z 1 1 1 1 znew dbase(x, y)p(s = 0 x)\\nzold dbase(x, y)p(s = 1 x)\\nznew p(s = 0 x)\\nzold p(s = 1 x)\\np(s = 0 x)\\np(s = 1 x)\\n1 − p(s = 1 x)\\n(cid:20)\\np(s = 1 x)\\np(s = 1 x) − 1 (cid:21) 1 deﬁnition (8.9) cancel base (8.10) consolidate (8.11) binary selection (8.12) rearrange (8.13) this means that if we can estimate the selection probability p(s =\\n1 x), we’re done. we can therefore use 1/p(s = 1 xn) − 1 as an\\nexample weight on example (xn, yn) when feeding these examples\\ninto our learning algorithm a.\\nthe remaining question is how to estimate p(s = 1 xn). recall\\nthat s = 1 denotes the case that x is selected into the old distribution\\nand s = 0 denotes the case that x is selected into the new distribution.\\nthis means that predicting s is exactly a binary classiﬁcation problem,\\nwhere the “positive” class is the set of n examples from the old\\ndistribution and the “negative” class is the set of m examples from\\nthe new distribution. ? as a check: make sure that these\\nweights are always non-negative.\\nfurthermore, why is it okay to\\nignore the z factor? 108 a course in machine learning algorithm 23 selectionadaptation((cid:104)(xn, yn)(cid:105)n\\n1: ddist ← (cid:104)(xn, +1)(cid:105)n\\nn=1 (cid:83) (cid:104)(zm,−1)(cid:105)m\\nm=1\\n(cid:69)n\\nˆp ← train logistic regression on ddist 3: dweighted ←(cid:68) 2: 1 ˆp(xn) − 1) n=1 (xn, yn,\\n4: return a(dweighted) n=1, (cid:104)zm(cid:105)m m=1, a) // assemble data for distinguishing\\n// between old and new distributions // assemble weight classiﬁcation\\n// data using selector\\n// train classiﬁer this analysis gives rise to algorithm 8.2, which consists of essen-\\ntially two steps. the ﬁrst is to train a logistic regression classiﬁer5 to\\ndistinguish between old and new distributions. the second is to use\\nthat classiﬁer to produce weights on the labeled examples from the\\nold distribution and then train whatever learning algorithm you wish\\non that. in terms of the questions posed at the beginning of this chapter,\\nthis approach to adaptation measures nearness of the two distribu-\\ntions by the degree to which the selection probability is constant. in\\nparticular, if the selection probability is independent of x, then the\\ntwo distributions are identical. if the selection probabilities vary sigi-\\nﬁcantly as x changes, then the two distributions are considered very\\ndifferent. more generally, if it is easy to train a classiﬁer to distin-\\nguish between the old and new distributions, then they are very\\ndifferent. in the case of speech recognition failing as a function of gender, a\\ncore issue is that speech from men was massively over-represented\\nin the training data but not the test data. when the selection logistic\\nregression is trained, it is likely to say “old” on speech from men and\\n“new” on other speakers, thereby downweighting the signiﬁcance of\\nmale speakers and upweighting the signiﬁcance of speakers of other\\ngenders on the ﬁnal learned model. this would (hopefully) address\\nmany of the issues confounding that system. ',\n",
       " ' unsupervised adaptation is very challenging because we never get to\\nsee try labels in the new distribution. in many ways, unsupervised\\nadaptation attempts to guard against bad things happening. that is,\\nif an old distribution training example looks very unlike the new\\ndistribution, it (and consequently it’s features) are downweighted so\\nmuch as to be ignored. in supervised adaptation, we can hope for\\nmore: we can hope to actually do better on the new distribution than\\nthe old because we have labeled data there. the typical setup is similar to the unsupervised case. there are 5 the use of logistic regression is arbi-\\ntrary: it need only be a classiﬁcation\\nalgorithm that can produce probabili-\\nties. ? make up percentages for fraction\\nof speakers who are male in the\\nold and new distributions; estimate\\n(you’ll have to make some assump-\\ntions) what the importance weights\\nwould look like in this case. bias and fairness 109 6 daumé iii 2007 (cid:69)n\\nalgorithm 24 easyadapt((cid:104)(x(old)\\nn 1: d ←(cid:68) , 0(cid:105), y(old) , x(old) ) n n , y(old) n (cid:83)(cid:68) )(cid:105)n\\nn=1, (cid:104)(x(new)\\n((cid:104)x(new)\\nm , 0, x(new) n=1 ((cid:104)x(old)\\nn\\n2: return a(d) (cid:69)m\\nm )(cid:105)m\\nm=1, a)\\nm , y(new)\\nm (cid:105), y(new)\\n// union\\nm )\\n// of transformed data\\n// train classiﬁer m=1 two distributions, dold and dnew, and our goal is to learn a classi-\\nﬁer that does well in expectation on dnew. however, now we have\\nlabeled data from both distributions: n labeled examples from dold\\n(cid:105)n\\nand m labeled examples from dnew. call them (cid:104)x(old)\\nn=1 from\\nn\\ndold and (cid:104)x(new)\\nm=1 from dnew. again, suppose that both x(old)\\nn\\nand x(new) m (cid:105)m\\nm , y(new)\\nboth live in rd. , y(old) n m one way of answer the question of “how do we share informa- tion between the two distributions” is to say: when the distributions\\nagree on the value of a feature, let them share it, but when they dis-\\nagree, allow them to learn separately. for instance, in a sentiment\\nanalysis task, dold might be reviews of electronics and dnew might\\nbe reviews of hotel rooms. in both cases, if the review contains the\\nword “awesome” then it’s probably a positive review, regardless of\\nwhich distribution we’re in. we would want to share this information\\nacross distributions. on the other hand, “small” might be positive\\nin electronics and negative in hotels, and we would like the learning\\nalgorithm to be able to learn separate information for that feature. a very straightforward way to accomplish this is the feature aug-\\nmentation approach6. this is a simple preprocessing step after which\\none can apply any learning algorithm. the idea is to create three ver-\\nsions of every feature: one that’s shared (for words like “awesome”),\\none that’s old-distribution-speciﬁc and one that’s new-distribution-\\nspeciﬁc. the mapping is: new-only (cid:123)(cid:122) (cid:125) d-many , 0, 0, . . . , 0 (cid:124) (cid:69)\\n(cid:69) shared\\nx(old)\\nn old-only\\n, x(old) n x(new)\\nm , 0, 0, . . . , 0 , x(new) m (cid:124) (cid:123)(cid:122) d-many (cid:125) (cid:55)→(cid:68)\\n(cid:55)→(cid:68) x(old)\\nn x(new)\\nm (8.14) (8.15) once you’ve applied this transformation, you can take the union\\nof the (transformed) old and new labeled examples and feed the en-\\ntire set into your favorite classiﬁcation algorithm. that classiﬁcation\\nalgorithm can then choose to share strength between the two distri-\\nbutions by using the “shared” features, if possible; or, if not, it can\\nlearn distribution-speciﬁc properties on the old-only or new-only\\nparts. this is summarized in algorithm 8.3. note that this algorithm can be combined with the instance weight- ? why is it crucial that the separator\\nbe trained on the untransformed\\ndata? 110 a course in machine learning ing (unsupervised) learning algorithm. in this case, the logistic re-\\ngression separator should be trained on the untransformed data, and\\nthen weights can be used exactly as in algorithm 8.2. this is particu-\\nlarly useful if you have access to way more old distribution data than\\nnew distribution data, and you don’t want the old distribution data\\nto “wash out” the new distribution data. although this approach is general, it is most effective when the two distributions are “not too close but not too far”: • if the distributions are too far, and there’s little information to share, you’re probably better off throwing out the old distribution\\ndata and training just on the (untransformed) new distribution\\ndata. • if the distributions are too close, then you might as well just take\\nthe union of the (untransformed) old and new distribution data,\\nand training on that. in general, the interplay between how far the distributions are and\\nhow much new distribution data you have is complex, and you\\nshould always try “old only” and “new only” and “simple union”\\nas baselines. ',\n",
       " ' almost any data set in existence is biased in some way, in the sense\\nthat it captures an imperfect view of the world. the degree to which\\nthis bias is obvious can vary greatly: • there might be obvious bias in the labeling. for instance, in crimi-\\nnology, learning to predict sentence lengths by predicting the sen-\\ntences assigned by judges will simply learn to reproduce whatever\\nbias already exists in judicial sentencing. • there might be sample selection bias, as discussed early. in the\\nsame criminology example, the only people for which we have\\ntraining data are those that have already been arrested, charged\\nwith and convicted of a crime; these processes are inherantly bi-\\nased, and so any model learned on this data may exhibit similar\\nbiases. • the task itself might be biased because the designers had blindspots. an intelligent billboard that predicts the gender of the person\\nwalking toward it so as to show “better” advertisements may be\\ntrained as a binary classiﬁer between male/female and thereby\\nexcludes anyone who does not fall in the gender binary. a similar\\nexample holds for a classiﬁer that predicts political afﬁliation in bias and fairness 111 7 for instance, many languages have\\nverbal marking that agrees with the\\ngender of the subject. in such cases,\\n“the doctor treats . . . ” puts masculine\\nmarkers on the translation of “treats”\\nbut “the nurse treats . . . ” uses feminine\\nmarkers. 8 acm code of ethics the us as democrat/republican, when in fact there are far more\\npossibilities. • there might be bias in the features or model structure. machine translation systems often exhibit incorrect gender stereotypes7\\nbecause the relevant context, which would tell them the correct\\ngender, is not encoded in the feature representation. alone, or\\ncoupled with biased data, this leads to errors. • the loss function may favor certain types of errors over others.\\nyou’ve already seen such an example: using zero/one loss on\\na highly imbalanced class problem will often lead to a learned\\nmodel that ignores the minority class entirely. • a deployed system creates feedback loops when it begins to con- sume it’s own output as new input. for instance, once a spam\\nﬁlter is in place, spammers will adjust their strategies, leading to\\ndistribution shift in the inputs. or a car guidance system for pre-\\ndicting which roads are likely to be unoccupied may ﬁnd those\\nroads now occupied by other cars that it directed there. these are all difﬁcult questions, none of which has easy answers.\\nnonetheless, it’s important to be aware of how our systems may\\nfail, especially as they take over more and more of our lives. most\\ncomputing, engineering and mathematical societies (like the acm,\\nieee, bcs, etc.) have codes of ethics, all of which include a statement\\nabout avoiding harm; the following is taken from the acm code8: to minimize the possibility of indirectly harming others, computing\\nprofessionals must minimize malfunctions by following generally\\naccepted standards for system design and testing. furthermore, it is\\noften necessary to assess the social consequences of systems to project\\nthe likelihood of any serious harm to others. in addition to ethical questions, there are often related legal ques-\\ntions. for example, us law prohibits discrimination by race and gen-\\nder (among other “protected attributes”) in processes like hiring and\\nhousing. the current legal mechanism for measuring discimination is\\ndisparate impact and the 80% rule. informally, the 80% rule says that\\nyour rate of hiring women (for instance) must be at least 80% of your\\nrate of hiring men. formally, the rule states: pr(y = +1 g (cid:54)= male) ≥ 0.8× pr(y = +1 g = male) (8.16) of course, gender/male can be replaced with any other protected\\nattribute. one non-solution to the disparate impact problem is to simply throw out protected attributes from your dataset. importantly, yet un-\\nfortunately, this is not sufﬁcient. many other features often correlate 112 a course in machine learning strongly with gender, race, and other demographic information, and\\njust because you’ve removed explicit encodings of these factors does\\nnot imply that a model trained as such would satisfy the 80% rule. a\\nnatural question to ask is: can we build machine learning algorithms\\nthat have high accuracy but simultaneously avoid disparate impact? disparate impact is an imperfect measure of (un)fairness, and there are alternatives, each with it’s own pros and cons9. all of these\\nrely on predeﬁned categories of protected attributes, and a natural\\nquestion is where these come from if not governmental regulation.\\nregardless of the measure you choose, the most important thing to\\nkeep in mind is that just because something comes from data, or is\\nalgorithmically implemented, does not mean it’s fair. ',\n",
       " '? 9 friedler et al. 2016, hardt et al. 2016 to help better understand how badly things can go when the distri-\\nbution over inputs changes, it’s worth thinking about how to analyze\\nthis situation formally. suppose we have two distributions dold and\\ndnew, and let’s assume that the only thing that’s different about these\\nis the distribution they put on the inputs x and not the outputs y.\\n(we will return later to the usefulness of this assumption.) that is:\\ndold(x, y) = dold(x)d(y x) and dnew(x, y) = dnew(x)d(y x),\\nwhere d(y x) is shared between them.\\ndold and achieves some test error of \\x01(old). that is: let’s say that you’ve learned a classiﬁer f that does really well on (8.17) (cid:2)1[ f (x) (cid:54)= y](cid:3) \\x01(old) = e x∼dold\\ny∼d(· x) the question is: how badly can f do on the new distribution? we can calculate this directly. (cid:2)1[ f (x) (cid:54)= y](cid:3) \\x01(new)\\n= e x∼dnew\\ny∼d(· x) (cid:90)\\n(cid:90) = x (cid:90)\\n(cid:16)dnew(x) − dold(x) + dold(x) dnew(x)d(y x)1[ f (x) (cid:54)= y]dydx (cid:17)× y y x = (cid:90)\\n= \\x01(old)+(cid:90)\\n(cid:90) x y d(y x)1[ f (x) (cid:54)= y]dydx (cid:16)dnew(x) − dold(x) (cid:17)× d(y x)1[ f (x) (cid:54)= y]dydx (8.18) def. of e (8.19) add zero (8.20) def. \\x01old (8.21) (cid:90) ≤ \\x01(old) + = \\x01(old) + 2 (cid:12)(cid:12)(cid:12)dnew(x) − dold(x)\\n(cid:12)(cid:12)(cid:12)dnew − dold(cid:12)(cid:12)(cid:12)var x (cid:17) (cid:12)(cid:12)(cid:12)(cid:16) 1 dx worst case (cid:90)\\ndef. ·var y (8.22)\\n(8.23) bias and fairness 113 here, ·var is the total variation distance (or variational distance)\\nbetween two probability distributions, deﬁned as: p − qvar = sup e p(e) − q(e) = 1\\n2 p(x) − q(x) dx (8.24) x (cid:90) is a standard measure of dissimilarity between probability distribu-\\ntions. in particular, the variational distance is the largest difference\\nin probability that p and q assign to the same event (in this case, the\\nevent is an example x). the bound tells us that we might not be able to hope for very good error on the new distribution, even if we have very good error\\non the old distribution, when dold and dnew are very different (as-\\nsign very different probabilities to the same event). of course, this is\\nan upper bound, and possibly a very loose one.\\nthe second observation is that we barely used the assumption that\\ndnew and dold share the same label distribution d(y x) in the above\\nanalysis. (in fact, as an exercise, repeat the analysis without this as-\\nsumption. it will still go through.) in general, this assumption buys\\nus very little. in an extreme case, dnew and dold can essentially “en-\\ncode” which distribution a given x came from in one of the features.\\nonce the origin distribution is completely encoded in a feature, then\\nd(y x) could look at the encoding, and completely ﬂip the label\\nbased on which distribution it’s from. how could dnew and dold\\nencode the distribution? one way would be to set the 29th decimal\\ndigit of feature 1 to an odd value in the old distribution and an even\\nvalue in the new distribution. this tiny change will be essentially im-\\nperceptible if one looks at the data, but would give d(y x) enough\\npower to make our lives miserable. if we want a way out of this dilemma, we need more technology.\\nthe core idea is that if we’re learning a function f from some hypoth-\\nesis class f, and this hypothesis class isn’t rich enough to peek at the\\n29th decimal digit of feature 1, then perhaps things are not as bad\\nas they could be. this motivates the idea of looking at a measure of\\ndistance between probability distributions that depends on the hypoth-\\nesis class. a popular measure is the da-distance or the discrepancy.\\nthe discrepancy measure distances between probability distributions\\nbased on how much two function f and f (cid:48) in the hypothesis class can\\ndisagree on their labels. let: (cid:104) (cid:105) \\x01p( f , f (cid:48)) = ex∼p 1[ f (x) (cid:54)= f (cid:48)(x)] (8.25) 114 a course in machine learning you can think of \\x01p( f , f (cid:48)) as the error of f (cid:48) when the ground truth is\\ngiven by f , where the error is taken with repsect to examples drawn\\nfrom p. given a hypothesis class f, the discrepancy between p and\\nq is deﬁned as: (cid:12)(cid:12)\\x01p( f , f (cid:48)) − \\x01q( f , f (cid:48))(cid:12)(cid:12) da(p, q) = max\\nf , f (cid:48)∈f (8.26) the discrepancy very much has the ﬂavor of a classiﬁer: if you think\\nof f as providing the ground truth labeling, then the discrepancy is\\nlarge whenever there exists a function f (cid:48) that agrees strongly with f\\non distribution p but disagrees strongly with f on q. this feels natu-\\nral: if all functions behave similarly on p and q, then the discrepancy\\nwill be small, and we also expect to be able to generalize across these\\ntwo distributions easily.\\none very attractive property of the discrepancy is that you can\\nestimate it from ﬁnite unlabeled samples from dold and dnew. al-\\nthough not obvious at ﬁrst, the discrepancy is very closely related\\nto a quantity we saw earlier in unsupervised adaptation: a classiﬁer\\nthat distinguishes between dold and dnew. in fact, the discrepancy is\\nprecisely twice the accuracy of the best classiﬁer from h at separating\\ndold from dnew. how does this work in practice? exactly as in the section on un-\\nsupervised adaptation, we train a classiﬁer to distinguish between\\ndold and dnew. it needn’t be a probabilistic classiﬁer; any binary\\nclassiﬁer is ﬁne. this classiﬁer, the “domain separator,” will have\\nsome (heldout) accuracy, call it acc. the discrepancy is then exactly\\nda = 2(acc − 0.5). intuitively the accuracy of the domain separator is a natural mea- one can, in fact, prove a generalization bound—generalizing from sure of how different the two distributions are. if the two distribu-\\ntions are identical, you shouldn’t expect to get very good accuracy at\\nseparating them. in particular, you expect the accuracy to be around\\n0.5, which puts the discrepancy at zero. on the other hand, if the two\\ndistributions are really far apart, separation is easy, and you expect\\nan accuracy of about 1, yielding a discrepancy also of about 1.\\nﬁnite samples from dold to expected loss on dnew—based on the\\ndiscrepancy10.\\ntheorem 9 (unsupervised adaptation bound). given a ﬁxed rep-\\nresentation and a ﬁxed hypothesis space f, let f ∈ f and let \\x01(best) =\\nmin f ∗∈f 1\\n(cid:124)\\n2\\n\\x01(new)( f )\\nerror on dnew\\nin this bound, \\x01(best) denotes the error rate of the best possible\\nclassiﬁer from f, where the error rate is measured as the average (cid:2)\\x01(old)( f ∗) + \\x01(new)( f ∗)(cid:3), then, for all f ∈ f:\\n(cid:123)(cid:122)\\n(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\\n≤ \\x01(old)( f )\\nerror on dold (cid:125)\\n(cid:124)\\n+ da(dold,dnew) distance (cid:123)(cid:122) (cid:124)(cid:123)(cid:122)(cid:125) \\x01(best) + minimal avg error (8.27) 10 ben-david et al. 2007 bias and fairness 115 error rate on dnew and dold; this term ensures that at least some\\ngood classiﬁer exists that does well on both distributions. the main practical use of this result is that it suggests a way to\\nlook for representations that are good for adaptation: on the one\\nhand, we should try to get our training error (on dold) as low as\\npossible; on the other hand, we want to make sure that it is hard to\\ndistinguish between dold and dnew in this representation. 8.6 further reading todo further reading learning objectives:\\n• deﬁne the generative story for a naive bayes classiﬁer. • derive relative frequency as the so-\\nlution to a constrained optimization\\nproblem. • compare and contrast generative, conditional and discriminative\\nlearning. • explain when generative models are likely to fail. • derive logistic loss with an (cid:96)2 regularizer from a probabilistic\\nperspective. dependencies: the world is noisy and messy. you need to deal with the noise\\nand uncertainty. – daphne koller many of the models and algorithms you have learned about\\nthus far are relatively disconnected. there is an alternative view of\\nmachine learning that unites and generalizes much of what you have\\nalready learned. this is the probabilistic modeling framework, in\\nwhich you will explicitly think of learning as a problem of statistical\\ninference. in this chapter, you will learn about two ﬂavors of probabilistic models: generative and conditional. you will see that many of the ap-\\nproaches (both supervised and unsupervised) we have seen already\\ncan be cast as probabilistic models. through this new view, you will\\nbe able to develop learning algorithms that have inductive biases\\ncloser to what you, as a designer, believe. moreover, the two chap-\\nters that follow will make heavy use of the probabilistic modeling\\napproach to open doors to other learning problems. ',\n",
       " ' recall from chapter 2 that if we had access to the underlying prob-\\nability distribution d, then we could form a bayes optimal classiﬁer\\nas: f (bo)( ˆx) = arg max ˆy∈y d( ˆx, ˆy) (9.1) unfortunately, no one gave you this distribution, but the optimality\\nof this approach suggests that good way to build a classiﬁer is to\\ntry to estimate d. in other words, you try to learn a distribution ˆd,\\nwhich you hope to very similar to d, and then use this distribution\\nfor classiﬁcation. just as in the preceding chapters, you can try to\\nform your estimate of d based on a ﬁnite training set. the most direct way that you can attempt to construct such a probability distribution is to select a family of parametric distribu-\\ntions. for instance, a gaussian (or normal) distribution is parametric:\\nit’s parameters are its mean and covariance. the job of learning is probabilistic modeling 117 math review rules of probability\\na probability distribution p speciﬁes the likelihood of an event e, where p(e) ∈ [0, 1]. it’s often con-\\nvenient to think of events as “conﬁgurations of the world”, so p(e) says “how likely is it that the\\nworld is in conﬁguration e.” often world conﬁgurations are built up of smaller pieces, for instance you\\nmight say “e = the conﬁguration in which it is rainy, windy and cold.” formally, we might write this as\\n“e = {weather = rainy, wind = windy, temperature = cold}”, where we’ve used a convention that\\nrandom variables (like temperature) are capitalized and their instantiations (like cold) are lower case.\\nconsidering this event, we want to evaluate p(weather = rainy, wind = windy, temperature = cold),\\nor more generally p(a = a, b = b, c = c) for some random variables a, b and c, and some instantia-\\ntions of those random variables a, b and c respectively. there are a few standard rules of probability that we will use regularly: sum-to-one: if you sum over all possible conﬁgurations of the world, p sums to one: ∑e p(e = e) = 1.\\nmarginalization: you can sum out one random variable to remove it from the world: ∑a p(a = a, b =\\nb) = p(b = b).\\nchain rule: if a world conﬁguration consists of two or more random variables, you can evaluate the\\nlikelihood of the world one step at a time: p(a = a, b = b) = p(a = a)p(b = b a = a). events are\\nunordered, so you can also get p(a = a, b = b) = p(b = b)p(a = a b = b).\\nbayes rule: combining the two chain rule equalities and dividing, we can relate a conditional proba-\\nbility in one direction with that in the other direction: p(a = a b = b) = p(a = a)p(b = b a =\\na)/p(b = b). figure 9.1: then to infer which parameters are “best” as far as the observed train-\\ning data is concerned, as well as whatever inductive bias you bring.\\na key assumption that you will need to make is that the training data\\nyou have access to is drawn independently from d. in particular, as\\nyou draw examples (x1, y1) ∼ d then (x2, y2) ∼ d and so on, the\\nnth draw (xn, yn) is drawn from d and does not otherwise depend\\non the previous n − 1 samples. this assumption is usually false, but\\nis also usually sufﬁciently close to being true to be useful. together\\nwith the assumption that all the training data is drawn from the same\\ndistribution d leads to the i.i.d. assumption or independently and\\nidentically distributed assumption. this is a key assumption in al-\\nmost all of machine learning. ',\n",
       " ' suppose you need to model a coin that is possibly biased (you can\\nthink of this as modeling the label in a binary classiﬁcation problem),\\nand that you observe data hhth (where h means a ﬂip came up heads 118 a course in machine learning and t means it came up tails). you can assume that all the ﬂips came\\nfrom the same coin, and that each ﬂip was independent (hence, the\\ndata was i.i.d.). further, you may choose to believe that the coin has\\na ﬁxed probability β of coming up heads (and hence 1 − β of coming\\nup tails). thus, the parameter of your model is simply the scalar β. the most basic computation you might perform is maximum like- lihood estimation: namely, select the paramter β the maximizes the\\nprobability of the data under that parameter. in order to do so, you\\nneed to compute the probability of the data: pβ(d) = pβ(hhth) = pβ(h)pβ(h)pβ(t)pβ(h)\\n= ββ(1 − β)β\\n= β3(1 − β)\\n= β3 − β4 deﬁnition of d\\ndata is independent (9.2)\\n(9.3)\\n(9.4)\\n(9.5)\\n(9.6) ? describe a case in which at least\\none of the assumptions we are\\nmaking about the coin ﬂip is false. thus, if you want the parameter β that maximizes the probability of\\nthe data, you can take the derivative of β3 − β4 with respect to β, set\\nit equal to zero and solve for β: (9.7) (9.8)\\n(9.9)\\n(9.10) (cid:104) β3 − β4(cid:105) ∂\\n∂β = 3β2 − 4β3\\n4β3 = 3β2 ⇐⇒4β = 3\\n3\\n⇐⇒β =\\n4 thus, the maximum likelihood β is 0.75, which is probably what\\nyou would have selected by intuition. you can solve this problem\\nmore generally as follows. if you have h-many heads and t-many\\ntails, the probability of your data sequence is βh(1 − β)t. you can\\ntry to take the derivative of this with respect to β and follow the\\nsame recipe, but all of the products make things difﬁcult. a more\\nfriendly solution is to work with the log likelihood or log proba-\\nbility instead. the log likelihood of this data sequence is h log β +\\nt log(1 − β). differentiating with respect to β, you get h/β − t/(1 −\\nβ). to solve, you obtain h/β = t/(1 − β) so h(1 − β) = tβ.\\nthus h − hβ = tβ and so h = (h + t)β, ﬁnally yeilding that\\nβ = h/(h + t) or, simply, the fraction of observed data that came up\\nheads. in this case, the maximum likelihood estimate is nothing but\\nthe relative frequency of observing heads! now, suppose that instead of ﬂipping a coin, you’re rolling a k- sided die (for instance, to pick the label for a multiclass classiﬁcation\\nproblem). you might model this by saying that there are parameters\\nθ1, θ2, . . . , θk specifying, respectively, the probabilities that any given ? how do you know that the solution\\nof β = h/(h + t) is actually a\\nmaximum? probabilistic modeling 119 side comes up on a role. since these are themselves probabilities,\\neach θk should be at least zero, and the sum of the θks should be one.\\ngiven a data set that consists of x1 rolls of 1, x2 rolls of 2 and so on,\\nthe probability of this data is ∏k θ\\n∑k xk log θk. if you pick some particular parameter, say θ3, the deriva-\\ntive of this with respect to θ3 is x3/θ3, which you want to equate to\\nzero. this leads to. . . θ3 → ∞. xk\\nk , yielding a log probability of this is obviously “wrong.” from the mathematical formulation,\\nxk\\nk it’s correct: in fact, setting all of the θks to ∞ does maximize ∏k θ\\nfor\\nany (non-negative) xks. the problem is that you need to constrain the\\nθs to sum to one. in particular, you have a constraint that ∑k θk = 1\\nthat you forgot to enforce. a convenient way to enforce such con-\\nstraints is through the technique of lagrange multipliers. to make\\nthis problem consistent with standard minimization problems, it is\\nconvenient to minimize negative log probabilities, instead of maxi-\\nmizing log probabilities. thus, the constrainted optimization problem\\nis: − ∑\\nk xk log θk\\nθk − 1 = 0 min θ subj. to ∑\\nk (9.11) the lagrange multiplier approach involves adding a new variable λ\\nto the problem (called the lagrange variable) corresponding to the\\nconstraint, and to use that to move the constraint into the objective.\\nthe result, in this case, is: (cid:32) (cid:33) max λ min θ xk log θk − λ − ∑\\nk θk − 1 ∑\\nk (9.12) turning a constrained optimization problem into it’s corresponding\\nlagrangian is straightforward. the mystical aspect is why it works.\\nin this case, the idea is as follows. think of λ as an adversary: λ is\\ntrying to maximize this function (you’re trying to minimize it). if\\nyou pick some parameters θ that actually satisfy the constraint, then\\nthe green term in eq (9.12) goes to zero, and therefore λ does not\\nmatter: the adversary cannot do anything. on the other hand, if the\\nconstraint is even slightly unsatisﬁed, then λ can tend toward +∞\\nor −∞ to blow up the objective. so, in order to have a non-inﬁnite\\nobjective value, the optimizer must ﬁnd values of θ that satisfy the\\nconstraint. if we solve the inner optimization of eq (9.12) by differentiating with respect to θ1, we get x1/θ1 = λ, yielding θ1 = x1/λ. in general,\\nthe solution is θk = xk/λ. remembering that the goal of λ is to\\nenforce the sums-to-one constraint, we can set λ = ∑k xk and verify 120 a course in machine learning that this is a solution. thus, our optimal θk = xk/ ∑k xk, which again\\ncompletely corresponds to intuition. ',\n",
       " ' now, consider the binary classiﬁcation problem. you are looking for\\na parameterized probability distribution that can describe the training\\ndata you have. to be concrete, your task might be to predict whether\\na movie review is positive or negative (label) based on what words\\n(features) appear in that review. thus, the probability for a single data\\npoint can be written as: pθ((y, x)) = pθ(y, x1, x2, . . . , xd) (9.13) the challenge in working with a probability distribution like eq (9.13)\\nis that it’s a distribution over a lot of variables. you can try to sim-\\nplify it by applying the chain rule of probabilities: pθ(x1, x2, . . . , xd, y) = pθ(y)pθ(x1 y)pθ(x2 y, x1)pθ(x3 y, x1, x2)\\n(9.14)\\n(9.15) · · · pθ(xd y, x1, x2, . . . , xd−1)\\npθ(xd y, x1, . . . , xd−1) = pθ(y) ∏ d at this point, this equality is exact for any probability distribution.\\nhowever, it might be difﬁcult to craft a probability distribution for\\nthe 10000th feature, given the previous 9999. even if you could, it\\nmight be difﬁcult to accurately estimate it. at this point, you can\\nmake assumptions. a classic assumption, called the naive bayes as-\\nsumption, is that the features are independent, conditioned on the label.\\nin the movie review example, this is saying that once you know that\\nit’s a positive review, the probability that the word “excellent” appears\\nis independent of whether “amazing” also appeared. (note that\\nthis does not imply that these words are independent when you\\ndon’t know the label—they most certainly are not.) formally this\\nassumption states that: assumption: p(xd y, xd(cid:48) ) = p(xd y) , ∀d (cid:54)= d(cid:48) (9.16) under this assumption, you can simplify eq (9.15) to: pθ((y, x)) = pθ(y) ∏ d pθ(xd y) naive bayes assumption (9.17) at this point, you can start parameterizing p. suppose, for now,\\nthat your labels are binary and your features are also binary. in this\\ncase, you could model the label as a biased coin, with probability of\\nheads (e.g., positive review) given by θ0. then, for each label, you probabilistic modeling 121 can imagine having one (biased) coin for each feature. so if there are\\nd-many features, you’ll have 1 + 2d total coins: one for the label\\n(call it θ0) and one for each label/feature combination (call these θ+1\\nand as θ−1). in the movie review example, we might expect θ0 ≈ 0.4\\n(forty percent of movie reviews are positive) and also that θ+1 might\\ngive high probability to words like “excellent” and “amazing” and\\n“good” and θ−1 might give high probability to words like “terrible”\\nand “boring” and “hate”. you can rewrite the probability of a single\\nexample as follows, eventually leading to the log probability of the\\nentire data set: pθ((y, x)) = pθ(y) ∏ pθ(xd y) (cid:16) = [y=+1]\\n0 θ d (1 − θ0)[y=−1](cid:17) ∏ naive bayes assumption [xd=1] (y),d (1 − θ(y),d)[xd=0] θ (9.18) (9.19) model assumptions d solving for θ0 is identical to solving for the biased coin case from\\nbefore: it is just the relative frequency of positive labels in your data\\n(because θ0 doesn’t depend on x at all). for the other parameters,\\nyou can repeat the same exercise as before for each of the 2d coins\\nindependently. this yields: ˆθ0 = ˆθ(+1),d = ˆθ(−1),d = ∑\\nn [yn = +1] 1\\nn\\n∑n[yn = +1 ∧ xn,d = 1]\\n∑n[yn = −1 ∧ xn,d = 1] ∑n[yn = +1]\\n∑n[yn = −1] (9.20) (9.21) (9.22) in the case that the features are not binary, you need to choose a dif-\\nferent model for p(xd y). the model we chose here is the bernouilli\\ndistribution, which is effectively a distribution over independent\\ncoin ﬂips. for other types of data, other distributions become more\\nappropriate. the die example from before corresponds to a discrete\\ndistribution. if the data is continuous, you might choose to use a\\ngaussian distribution (aka normal distribution). the choice of dis-\\ntribution is a form of inductive bias by which you can inject your\\nknowledge of the problem into the learning algorithm. ',\n",
       " ' consider the predictions made by the naive bayes model with bernoulli\\nfeatures in eq (9.18). you can better understand this model by con-\\nsidering its decision boundary. in the case of probabilistic models, 122 a course in machine learning math review common probability distributions\\nthere are a few common probability distributions that we use in this book. the ﬁrst is the bernouilli\\ndistribution, which models binary outcomes (like coin ﬂips). a bernouilli distribution, ber(θ) is pa-\\nrameterized by a single scalar value θ ∈ [0, 1] that represents the probability of heads. the likelihood\\nfunction is ber(x θ) = θx(1 − θ)1−x. the generalization of the bernouilli to more than two possible\\noutcomes (like rolls of a die) is the discrete distribution, disc(th). if the die has k sides, then θ ∈ rk\\nwith all entries non-negative and ∑k θk = 1. θk is the probabability that the die comes up on side k.\\nthe likelihood function is disc(x θ) = ∏k θ\\n. the binomial distribution is just like the bernouilli\\nbin(k n,θ)=n\\nkθk(1−θ)n−k ), where n\\ndistribution but for multiple ﬂips of the rather than a single ﬂip; it’s likelihood is (\\nis the number of ﬂips and k is the number of heads. the multinomial distribution extends the discrete\\ndistribution also to multiple rolls; it’s likelihood is mult(x n, θ) =\\nxk\\nk , where n is the total\\nnumber of rolls and xk is the number of times the die came up on side k (so ∑k xk = n). the preceding\\ndistributions are all discrete. 1[x=k]\\nk ∏k θ ∏k xk! n! there are two common continuous distributions we need. the ﬁrst is the uniform distribution,\\nb−a 1[x ∈\\nuni(a, b) which is uniform over the closed range [a, b]. it’s density function is uni(x a, b) = 1\\n[a, b]]. finally, the gaussian distribution is parameterized by a mean µ and variance σ2 and has density\\nnor(x µ, σ2) = (2πσ2)− 1 (cid:104)− 1\\n2σ2 (x − µ)2(cid:105) 2 exp . the decision boundary is the set of inputs for which the likelihood of\\ny = +1 is precisely 50%. or, in other words, the set of inputs x for\\nwhich p(y = +1 x)/p(y = −1 x) = 1. in order to do this, the\\nﬁrst thing to notice is that p(y x) = p(y, x)/p(x). in the ratio, the\\np(x) terms cancel, leaving p(y = +1, x)/p(y = −1, x). instead of\\ncomputing this ratio, it is easier to compute the log-likelihood ratio\\n(or llr), log p(y = +1, x) − log p(y = −1, x), computed below: (cid:34) θ0 ∏\\n(cid:34) d llr = log [xd=1] (+1),d (1 − θ(+1),d)[xd=0] θ − log (1 − θ0) ∏ [xd=1] (−1),d (1 − θ(−1),d)[xd=0] θ (cid:35) (cid:35) (cid:16) d (cid:16) = log θ0 − log(1 − θ0) + ∑ [xd = 1] log θ(+1),d − log θ(−1),d + ∑\\nd [xd = 0] d log(1 − θ(+1),d) − log(1 − θ(−1),d) (9.23) (cid:17) (cid:17) = ∑\\nd xd log θ(+1),d\\nθ(−1),d + ∑\\nd (1 − xd) log 1 − θ(+1),d\\n1 − θ(−1),d (9.24) + log θ0\\n1 − θ0 (9.25) figure 9.2: model assumptions take logs and rearrange simplify log terms (cid:34) xd log = ∑\\nd θ(+1),d\\nθ(−1),d − log 1 − θ(+1),d\\n1 − θ(−1),d (cid:35) + ∑\\nd log 1 − θ(+1),d\\n1 − θ(−1),d probabilistic modeling 123 + log θ0\\n1 − θ0 group x-terms = x · w + b wd = log θ(+1),d(1 − θ(−1),d)\\nθ(−1),d(1 − θ(+1),d) , b = ∑\\nd log 1 − θ(+1),d\\n1 − θ(−1),d (9.26)\\n(9.27) + log θ0\\n1 − θ0\\n(9.28) the result of the algebra is that the naive bayes model has precisely\\nthe form of a linear model! thus, like perceptron and many of the\\nother models you’ve previous studied, the decision boundary is\\nlinear. ',\n",
       " ' a useful way to develop probabilistic models is to tell a generative\\nstory. this is a ﬁctional story that explains how you believe your\\ntraining data came into existence. to make things interesting, con-\\nsider a multiclass classiﬁcation problem, with continuous features\\nmodeled by independent gaussians. since the label can take values\\n1 . . . k, you can use a discrete distribution (die roll) to model it (as\\nopposed to the bernoilli distribution from before):\\n1. for each example n = 1 . . . n:\\n(a) choose a label yn ∼ disc(θ)\\n(b) for each feature d = 1 . . . d: i. choose feature value xn,d ∼ nor(µyn,d, σ2 yn,d) this generative story can be directly translated into a likelihood\\nfunction by replacing the “for each”s with products: (cid:122) ∏\\nn p(d) = for each example (cid:34) (cid:125)(cid:124) exp 1(cid:113)\\n(cid:124) 2πσ2 yn,d θyn(cid:124)(cid:123)(cid:122)(cid:125) choose label ∏\\nd (cid:124) (xn,d − µyn,d)2 yn,d − 1\\n2σ2 (cid:123)(cid:122)\\n(cid:123)(cid:122) choose feature value for each feature (cid:123)\\n(cid:35)\\n(cid:125)\\n(cid:125) (cid:34) you can take logs to arrive at the log-likelihood: log p(d) = ∑\\nn log θyn + ∑ d − 1\\n2 log(σ2 yn,d) − 1\\n2σ2 yn,d (9.29) (cid:35) + const (xn,d − µyn,d)2 (9.30) 124 a course in machine learning to optimize for θ, you need to add a “sums to one” constraint as\\nbefore. this leads to the previous solution where the θks are propor-\\ntional to the number of examples with label k. in the case of the µs\\nyou can take a derivative with respect to, say µk,i and obtain: ∂ log p(d) ∂µk,i = ∂ ∂µk,i − ∑\\nn ∑\\nd 1\\n2σ2 yn,d (xn,d − µyn,d)2 = ∂ ∂µk,i − ∑\\nn:yn=k 1\\n2σ2\\nk,d (xn,i − µk,i)2 = ∑\\nn:yn=k 1\\nσ2\\nk,d (xn,i − µk,i) setting this equal to zero and solving yields: µk,i = ∑n:yn=k xn,i\\n∑n:yn=k 1 ignore irrelevant terms (9.31) ignore irrelevant terms (9.32) take derivative (9.33) (9.34) namely, the sample mean of the ith feature of the data points that fall\\nin class k. a similar analysis for σ2 k,i yields: (cid:35) ∂ log p(d) ∂σ2\\nk,i = ∂\\n∂σ2\\nk,i − ∑\\ny:yn=k 1\\n2 log(σ2 k,i) + 1\\n2σ2\\nk,i (xn,i − µk,i)2 (cid:34) (cid:34) (cid:104) (cid:35) (cid:105) = − ∑\\ny:yn=k 1\\n2σ2\\nk,i − 1\\n2(σ2\\nk,i)2 (xn,i − µk,i)2 = 1\\n2σ4\\nk,i ∑\\ny:yn=k (xn,i − µk, i)2 − σ2 k,i ignore irrelevant terms take derivative simplify (9.35) (9.36) (9.37) (9.38) you can now set this equal to zero and solve, yielding: σ2\\nk,i = ∑n:yn=k(xn,i − µk,i)2 ∑n:yn=k 1 which is just the sample variance of feature i for class k. ',\n",
       " ' in the foregoing examples, the task was formulated as attempting to\\nmodel the joint distribution of (x, y) pairs. this may seem wasteful:\\nat prediction time, all you care about is p(y x), so why not model it\\ndirectly? ? what would the estimate be if you\\ndecided that, for a given class k, all\\nfeatures had equal variance? what\\nif you assumed feature i had equal\\nvariance for each class? under what\\ncircumstances might it be a good\\nidea to make such assumptions? probabilistic modeling 125 starting with the case of regression is actually somewhat simpler\\nthan starting with classiﬁcation in this case. suppose you “believe”\\nthat the relationship between the real value y and the vector x should\\nbe linear. that is, you expect that y = w · x + b should hold for some\\nparameters (w, b). of course, the data that you get does not exactly\\nobey this: that’s ﬁne, you can think of deviations from y = w · x +\\nb as noise. to form a probabilistic model, you must assume some\\ndistribution over noise; a convenient choice is zero-mean gaussian\\nnoise. this leads to the following generative story:\\n1. for each example n = 1 . . . n:\\n(a) compute tn = w · xn + b\\n(b) choose noise en ∼ nor(0, σ2)\\n(c) return yn = tn + en in this story, the variable tn stands for “target.” it is the noiseless\\nvariable that you do not get to observe. similarly en is the error\\n(noise) on example n. the value that you actually get to observe is\\nyn = tn + en. see figure 9.3.\\na basic property of the gaussian distribution is additivity. namely,\\nthat if a ∼ nor(µ, σ2) and b = a + c, then b ∼ nor(µ + c, σ2). given\\nthis, from the generative story above, you can derive a shorter gener-\\native story:\\n1. for each example n = 1 . . . n: (a) choose yn ∼ nor(w · xn + b, σ2) figure 9.3: pictorial view of targets\\nversus labels reading off the log likelihood of a dataset from this generative story,\\nyou obtain: (cid:20) (cid:21) log p(d) = ∑\\nn − 1\\n2 log(σ2) − 1 2σ2 (w · xn + b − yn)2 = − 1 2σ2 ∑ n (w · xn + b − yn)2 + const model assumptions (9.39) remove constants (9.40) this is precisely the linear regression model you encountered in\\nsection 7.6! to minimizing the negative log probability, you need only\\nsolve for the regression coefﬁcients w, b as before. in the case of binary classiﬁcation, using a gaussian noise model does not make sense. switching to a bernoulli model, which de-\\nscribes binary outcomes, makes more sense. the only remaining\\ndifﬁculty is that the parameter of a bernoulli is a value between zero\\nand one (the probability of “heads”) so your model must produce 126 a course in machine learning such values. a classic approach is to produce a real-valued target, as\\nbefore, and then transform this target into a value between zero and\\none, so that −∞ maps to 0 and +∞ maps to 1. a function that does\\nthis is the logistic function1, deﬁned below and plotted in figure 9.4: logistic function: σ(z) = 1 1 + exp[−z] = exp z 1 + exp z (9.41) the logistic function has several nice properties that you can verify\\nfor yourself: σ(−z) = 1 − σ(z) and ∂σ/∂z = zσ2(z). using the logistic function, you can write down a generative story 1 also called the sigmoid function\\nbecause of it’s “s”-shape. figure 9.4: sketch of logistic function for binary classiﬁcation: 1. for each example n = 1 . . . n: (a) compute tn = σ (w · xn + b)\\n(b) compute zn ∼ ber(tn)\\n(c) return yn = 2zn − 1 (to make it ±1) the log-likelihood for this model is:\\nlog p(d) = ∑\\nn (cid:104)\\n[yn = +1] log σ (w · xn + b)\\n+ [yn = −1] log σ (−w · xn + b) (cid:105) log σ (yn (w · xn + b)) = ∑\\nn = − ∑\\nn = − ∑\\nn log [1 + exp (−yn (w · xn + b))] (cid:96)(log)(yn, w · xn + b) model and properties of σ (9.42) join terms (9.43)\\ndeﬁnition of σ\\n(9.44) deﬁnition of (cid:96)(log) (9.45) as you can see, the log-likelihood is precisely the negative of (a scaled version of) the logistic loss from chapter 7. this model is the\\nlogistic regression model, and this is where logisitic loss originally\\nderived from. todo: conditional versus joint ',\n",
       " ' in the foregoing discussion, parameters of the model were selected\\naccording to the maximum likelihood criteria: ﬁnd the parameters\\nθ that maximize pθ(d). the trouble with this approach is easy to probabilistic modeling 127 see even in a simple coin ﬂipping example. if you ﬂip a coin twice\\nand it comes up heads both times, the maximum likelihood estimate\\nfor the bias of the coin is 100%: it will always come up heads. this is\\ntrue even if you had only ﬂipped it once! if course if you had ﬂipped\\nit one million times and it had come up heads every time, then you\\nmight ﬁnd this to be a reasonable solution. this is clearly undesirable behavior, especially since data is expen-\\nsive in a machine learning setting. one solution (there are others!) is\\nto seek parameters that balance a tradeoff between the likelihood of\\nthe data and some prior belief you have about what values of those\\nparameters are likely. taking the case of the logistic regression, you\\nmight a priori believe that small values of w are more likely than\\nlarge values, and choose to represent this as a gaussian prior on each\\ncomponent of w. the maximum a posteriori principle is a method for incoporat- ing both data and prior beliefs to obtain a more balanced parameter\\nestimate. in abstract terms, consider a probabilistic model over data\\nd that is parameterized by parameters θ. if you think of the pa-\\nrameters as just another random variable, then you can write this\\nmodel as p(d θ), and maximum likelihood amounts to choosing θ\\nto maximize p(d θ). however, you might instead with to maximize\\nthe probability of the parameters, given the data. namely, maximize\\np(θ d). this term is known as the posterior distribution on θ, and\\ncan be computed by bayes’ rule: prior\\np(θ) (cid:122)\\n(cid:122)(cid:125)(cid:124)(cid:123)\\n(cid:125)(cid:124)\\n(cid:123)\\np(d θ)\\n(cid:124)(cid:123)(cid:122)(cid:125)\\np(d) (cid:124)\\n(cid:123)(cid:122)\\n(cid:125)\\np(θ d) posterior = likelihood evidence (cid:90) , where p(d) = dθp(θ)p(d θ) (9.46) this reads: the posterior is equal to the prior times the likelihood di-\\nvided by the evidence.2 the evidence is a scary-looking term (it has\\nan integral!) but note that from the perspective of seeking parameters\\nθ than maximize the posterior, the evidence is just a constant (it does\\nnot depend on θ) and therefore can be ignored. returning to the logistic regression example with gaussian priors on the weights, the log posterior looks like: 2 the evidence is sometimes called the\\nmarginal likelihood. log p(θ d) = − ∑\\nn (cid:96)(log)(yn, w · xn + b) − ∑ d 1\\n2σ2 w2 d + const = − ∑\\nn (cid:96)(log)(yn, w · xn + b) − 1 2σ2 w2 and therefore reduces to a regularized logistic function, with a model deﬁnition (9.47)\\n(9.48) 128 a course in machine learning squared 2-norm regularizer on the weights. (a 1-norm regularizer\\ncan be obtained by using a laplace prior on w rather than a gaussian\\nprior on w.) 9.8 further reading todo todo – the first learning models you learned about (decision trees\\nand nearest neighbor models) created complex, non-linear decision\\nboundaries. we moved from there to the perceptron, perhaps the\\nmost classic linear model. at this point, we will move back to non-\\nlinear learning models, but using all that we have learned about\\nlinear learning thus far. this chapter presents an extension of perceptron learning to non-\\nlinear decision boundaries, taking the biological inspiration of neu-\\nrons even further. in the perceptron, we thought of the input data\\npoint (e.g., an image) as being directly connected to an output (e.g.,\\nlabel). this is often called a single-layer network because there is one\\nlayer of weights. now, instead of directly connecting the inputs to\\nthe outputs, we will insert a layer of “hidden” nodes, moving from\\na single-layer network to a multi-layer network. but introducing\\na non-linearity at inner layers, this will give us non-linear decision\\nboundaires. in fact, such networks are able to express almost any\\nfunction we want, not just linear functions. the trade-off for this ﬂex-\\nibility is increased complexity in parameter tuning and model design. ',\n",
       " ' one of the major weaknesses of linear models, like perceptron and\\nthe regularized linear models from the previous chapter, is that they\\nare linear! namely, they are unable to learn arbitrary decision bound-\\naries. in contrast, decision trees and knn could learn arbitrarily\\ncomplicated decision boundaries. one approach to doing this is to chain together a collection of perceptrons to build more complex neural networks. an example of\\na two-layer network is shown in figure 10.1. here, you can see ﬁve\\ninputs (features) that are fed into two hidden units. these hidden\\nunits are then fed in to a single output unit. each edge in this ﬁgure\\ncorresponds to a different weight. (even though it looks like there are\\nthree layers, this is called a two-layer network because we don’t count learning objectives:\\n• explain the biological inspiration for multi-layer neural networks. • construct a two-layer network that can solve the xor problem. • implement the back-propogation\\nalgorithm for training multi-layer\\nnetworks. • explain the trade-off between depth and breadth in network structure.\\n• contrast neural networks with ra-\\ndial basis functions with k-nearest\\nneighbor learning. dependencies: figure 10.1: picture of a two-layer\\nnetwork with 5 inputs and two hidden\\nunits 130 a course in machine learning the inputs as a real layer. that is, it’s two layers of trained weights.)\\nprediction with a neural network is a straightforward generaliza-\\ntion of prediction with a perceptron. first you compute activations\\nof the nodes in the hidden unit based on the inputs and the input\\nweights. then you compute activations of the output unit given the\\nhidden unit activations and the second layer of weights. the only major difference between this computation and the per- ceptron computation is that the hidden units compute a non-linear\\nfunction of their inputs. this is usually called the activation function\\nor link function. more formally, if wi,d is the weights on the edge\\nconnecting input d to hidden unit i, then the activation of hidden unit\\ni is computed as:\\nhi = f (wi · x) (10.1) where f is the link function and wi refers to the vector of weights\\nfeeding in to node i.\\none example link function is the sign function. that is, if the\\nincoming signal is negative, the activation is −1. otherwise the\\nactivation is +1. this is a potentially useful activiation function,\\nbut you might already have guessed the problem with it: it is non-\\ndifferentiable. explain bias!!!\\na more popular link function is the hyperbolic tangent function,\\ntanh. a comparison between the sign function and the tanh function\\nis in figure 10.2. as you can see, it is a reasonable approximation\\nto the sign function, but is convenient in that it is differentiable.1\\nbecause it looks like an “s” and because the greek character for “s”\\nis “sigma,” such functions are usually called sigmoid functions. assuming for now that we are using tanh as the link function, the overall prediction made by a two-layer network can be computed\\nusing algorithm 10.1. this function takes a matrix of weights w\\ncorresponding to the ﬁrst layer weights and a vector of weights v cor-\\nresponding to the second layer. you can write this entire computation\\nout in one line as: vi tanh(wi · ˆx) ˆy = ∑\\ni\\n= v · tanh(wˆx) (10.2) (10.3) where the second line is short hand assuming that tanh can take a\\nvector as input and product a vector as output. figure 10.2: picture of sign versus tanh\\n1 it’s derivative is just 1 − tanh2(x). ? is it necessary to use a link function\\nat all? what would happen if you\\njust used the identify function as a\\nlink? neural networks 131 x0 x1 x2\\ny\\n+1 +1 +1 +1\\n+1 +1\\n-1\\n-1\\n-1 +1 +1\\n-1\\n-1 +1\\n-1 +1\\ntable 10.1: small xor data set. ? verify that these output weights\\nwill actually give you xor. ? this shows how to create an “or”\\nfunction. how can you create an\\n“and” function? algorithm 25 twolayernetworkpredict(w, v, ˆx)\\n1: for i = 1 to number of hidden units do\\n2:\\n3: end for\\n4: return v · h hi ← tanh(wi · ˆx) // compute activation of hidden unit i // compute output unit the claim is that two-layer neural networks are more expressive than single layer networks (i.e., perceptrons). to see this, you can\\nconstruct a very small two-layer network for solving the xor prob-\\nlem. for simplicity, suppose that the data set consists of four data\\npoints, given in table 10.1. the classiﬁcation rule is that y = +1 if an\\nonly if x1 = x2, where the features are just ±1. to achieve the “or” behavior, you can start by setting the bias to you can solve this problem using a two layer network with two\\nhidden units. the key idea is to make the ﬁrst hidden unit compute\\nan “or” function: x1 ∨ x2. the second hidden unit can compute an\\n“and” function: x1 ∧ x2. the the output can combine these into a\\nsingle prediction that mimics xor. once you have the ﬁrst hidden\\nunit activate for “or” and the second for “and,” you need only set the\\noutput weights as −2 and +1, respectively.\\n−0.5 and the weights for the two “real” features as both being 1. you\\ncan check for yourself that this will do the “right thing” if the link\\nfunction were the sign function. of course it’s not, it’s tanh. to get\\ntanh to mimic sign, you need to make the dot product either really\\nreally large or really really small. you can accomplish this by set-\\nting the bias to −500, 000 and both of the two weights to 1, 000, 000.\\nnow, the activation of this unit will be just slightly above −1 for\\nx = (cid:104)−1,−1(cid:105) and just slightly below +1 for the other three examples. at this point you’ve seen that one-layer networks (aka percep-\\ntrons) can represent any linear function and only linear functions.\\nyou’ve also seen that two-layer networks can represent non-linear\\nfunctions like xor. a natural question is: do you get additional\\nrepresentational power by moving beyond two layers? the answer\\nis partially provided in the following theorem, due originally to\\ngeorge cybenko for one particular type of link function, and ex-\\ntended later by kurt hornik to arbitrary link functions. theorem 10 (two-layer networks are universal function approx-\\nimators). let f be a continuous function on a bounded subset of d-\\ndimensional space. then there exists a two-layer neural network ˆf with a\\nﬁnite number of hidden units that approximate f arbitrarily well. namely, for all x in the domain of f,(cid:12)(cid:12)f(x) − ˆf(x)(cid:12)(cid:12) < \\x01. or, in colloquial terms “two-layer networks can approximate any 132 a course in machine learning function.” this is a remarkable theorem. practically, it says that if you give me a function f and some error tolerance parameter \\x01, i can construct\\na two layer network that computes f. in a sense, it says that going\\nfrom one layer to two layers completely changes the representational\\ncapacity of your model. when working with two-layer networks, the key question is: how many hidden units should i have? if your data is d dimensional\\nand you have k hidden units, then the total number of parameters\\nis (d + 2)k. (the ﬁrst +1 is from the bias, the second is from the\\nsecond layer of weights.) following on from the heuristic that you\\nshould have one to two examples for each parameter you are trying\\nto estimate, this suggests a method for choosing the number of hid-\\nden units as roughly (cid:98) n\\nd(cid:99). in other words, if you have tons and tons\\nof examples, you can safely have lots of hidden units. if you only\\nhave a few examples, you should probably restrict the number of\\nhidden units in your network. the number of units is both a form of inductive bias and a form\\nof regularization. in both view, the number of hidden units controls\\nhow complex your function will be. lots of hidden units ⇒ very\\ncomplicated function. as the number increases, training performance\\ncontinues to get better. but at some point, test performance gets\\nworse because the network has overﬁt the data. ',\n",
       " ' the back-propagation algorithm is a classic approach to training\\nneural networks. although it was not originally seen this way, based\\non what you know from the last chapter, you can summarize back-\\npropagation as: back-propagation = gradient descent + chain rule (10.4) more speciﬁcally, the set up is exactly the same as before. you are\\ngoing to optimize the weights in the network to minimize some ob-\\njective function. the only difference is that the predictor is no longer\\nlinear (i.e., ˆy = w · x + b) but now non-linear (i.e., v · tanh(wˆx)).\\nthe only question is how to do gradient descent on this more compli-\\ncated objective. for now, we will ignore the idea of regularization. this is for two\\nreasons. the ﬁrst is that you already know how to deal with regular-\\nization, so everything you’ve learned before applies. the second is\\nthat historically, neural networks have not been regularized. instead,\\npeople have used early stopping as a method for controlling overﬁt-\\nting. presently, it’s not obvious which is a better solution: both are valid options. neural networks 133 to be completely explicit, we will focus on optimizing squared\\nerror. again, this is mostly for historic reasons. you could easily\\nreplace squared error with your loss function of choice. our overall\\nobjective is: (cid:32) (cid:33)2 min\\nw,v ∑\\nn 1\\n2 yn − ∑ i vi f (wi · xn) (10.5) here, f is some link function like tanh. the easy case is to differentiate this with respect to v: the weights for the output unit. without even doing any math, you should be\\nable to guess what this looks like. the way to think about it is that\\nfrom vs perspective, it is just a linear model, attempting to minimize\\nsquared error. the only “funny” thing is that its inputs are the activa-\\ntions h rather than the examples x. so the gradient with respect to v\\nis just as for the linear case. to make things notationally more convenient, let en denote the error on the nth example (i.e., the blue term above), and let hn denote\\nthe vector of hidden unit activations on that example. then: ∇v = − ∑ n enhn (10.6) this is exactly like the linear case. one way of interpreting this is:\\nhow would the output weights have to change to make the prediction\\nbetter? this is an easy question to answer because they can easily\\nmeasure how their changes affect the output. the more complicated aspect to deal with is the weights corre- sponding to the ﬁrst layer. the reason this is difﬁcult is because the\\nweights in the ﬁrst layer aren’t necessarily trying to produce speciﬁc\\nvalues, say 0 or 5 or −2.1. they are simply trying to produce acti-\\nvations that get fed to the output layer. so the change they want to\\nmake depends crucially on how the output layer interprets them. thankfully, the chain rule of calculus saves us. ignoring the sum over data points, we can compute: (cid:33)2 l(w) = vi f (wi · x) (cid:32) = 1\\ny − ∑\\n2\\ni\\n∂l\\n(cid:32)\\n∂ fi\\n= − ∂ fi\\n∂wi\\ny − ∑\\ni = f (cid:48)(wi · x)x ∂l\\n∂wi\\n∂l\\n∂ fi\\n∂ fi\\n∂wi (cid:33) vi = −evi vi f (wi · x) (10.7) (10.8) (10.9) (10.10) 134 a course in machine learning algorithm 26 twolayernetworktrain(d, η, k, maxiter)\\n1: w ← d×k matrix of small random values\\n2: v ← k-vector of small random values\\n3: for iter = 1 . . . maxiter do\\n4: g ← d×k matrix of zeros\\ng ← k-vector of zeros\\nfor all (x,y) ∈ d do\\nfor i = 1 to k do\\nai ← wi · ˆx\\nhi ← tanh(ai) 5: 6: 9: // initialize input layer weights\\n// initialize output layer weights // initialize input layer gradient\\n// initialize output layer gradient 7: 8: 10: 11: 12: 13: 14: 15: 16: end for 17: 18: w ← w − ηg end for\\nv ← v − ηg 19:\\n20: end for\\n21: return w, v end for\\nˆy ← v · h\\ne ← y − ˆy\\ng ← g − eh\\nfor i = 1 to k do gi ← gi − evi(1 − tanh2(ai))x // compute activation of hidden unit i // compute output unit\\n// compute error\\n// update gradient for output layer // update gradient for input layer // update input layer weights\\n// update output layer weights putting this together, we get that the gradient with respect to wi is: ∇wi = −evi f (cid:48)(wi · x)x (10.11) intuitively you can make sense of this. if the overall error of the\\npredictor (e) is small, you want to make small steps. if vi is small\\nfor hidden unit i, then this means that the output is not particularly\\nsensitive to the activation of the ith hidden unit. thus, its gradient\\nshould be small. if vi ﬂips sign, the gradient at wi should also ﬂip\\nsigns. the name back-propagation comes from the fact that you\\npropagate gradients backward through the network, starting at the\\nend. the complete instantiation of gradient descent for a two layer network with k hidden units is sketched in algorithm 10.2. note that\\nthis really is exactly a gradient descent algorithm; the only different is\\nthat the computation of the gradients of the input layer is moderately\\ncomplicated. as a bit of practical advice, implementing the back-propagation algorithm can be a bit tricky. sign errors often abound. a useful trick\\nis ﬁrst to keep w ﬁxed and work on just training v. then keep v\\nﬁxed and work on training w. then put them together. ? what would happen to this algo-\\nrithm if you wanted to optimize\\nexponential loss instead of squared\\nerror? what if you wanted to add in\\nweight regularization? ? if you like matrix calculus, derive\\nthe same algorithm starting from\\neq (10.3). ',\n",
       " ' neural networks 135 based on what you know about linear models, you might be tempted\\nto initialize all the weights in a neural network to zero. you might\\nalso have noticed that in algorithm 10.2, this is not what’s done:\\nthey’re initialized to small random values. the question is why? the answer is because an initialization of w = 0 and v = 0 will lead to “uninteresting” solutions. in other words, if you initialize the\\nmodel in this way, it will eventually get stuck in a bad local optimum.\\nto see this, ﬁrst realize that on any example x, the activation hi of the\\nhidden units will all be zero since w = 0. this means that on the ﬁrst\\niteration, the gradient on the output weights (v) will be zero, so they\\nwill stay put. furthermore, the gradient w1,d for the dth feature on\\nthe ith unit will be exactly the same as the gradient w2,d for the same\\nfeature on the second unit. this means that the weight matrix, after\\na gradient step, will change in exactly the same way for every hidden\\nunit. thinking through this example for iterations 2 . . . , the values of\\nthe hidden units will always be exactly the same, which means that\\nthe weights feeding in to any of the hidden units will be exactly the\\nsame. eventually the model will converge, but it will converge to a\\nsolution that does not take advantage of having access to the hidden\\nunits. this shows that neural networks are sensitive to their initialization.\\nin particular, the function that they optimize is non-convex, meaning\\nthat it might have plentiful local optima. (one of which is the trivial\\nlocal optimum described in the preceding paragraph.) in a sense,\\nneural networks must have local optima. suppose you have a two\\nlayer network with two hidden units that’s been optimized. you have\\nweights w1 from inputs to the ﬁrst hidden unit, weights w2 from in-\\nputs to the second hidden unit and weights (v1, v2) from the hidden\\nunits to the output. if i give you back another network with w1 and\\nw2 swapped, and v1 and v2 swapped, the network computes exactly\\nthe same thing, but with a markedly different weight structure. this\\nphenomena is known as symmetric modes (“mode” referring to an\\noptima) meaning that there are symmetries in the weight space. it\\nwould be one thing if there were lots of modes and they were all\\nsymmetric: then ﬁnding one of them would be as good as ﬁnding\\nany other. unfortunately there are additional local optima that are\\nnot global optima. random initialization of the weights of a network is a way to address both of these problems. by initializing a network with small\\nrandom weights (say, uniform between −0.1 and 0.1), the network is\\nunlikely to fall into the trivial, symmetric local optimum. moreover,\\nby training a collection of networks, each with a different random figure 10.3: convergence of randomly\\ninitialized networks 136 a course in machine learning initialization, you can often obtain better solutions that with just\\none initialization. in other words, you can train ten networks with\\ndifferent random seeds, and then pick the one that does best on held-\\nout data. figure 10.3 shows prototypical test-set performance for ten\\nnetworks with different random initialization, plus an eleventh plot\\nfor the trivial symmetric network initialized with zeros. one of the typical complaints about neural networks is that they are ﬁnicky. in particular, they have a rather large number of knobs to\\ntune:\\n1. the number of layers\\n2. the number of hidden units per layer\\n3. the gradient descent learning rate η\\n4. the initialization\\n5. the stopping iteration or weight regularization the last of these is minor (early stopping is an easy regularization\\nmethod that does not require much effort to tune), but the others\\nare somewhat signiﬁcant. even for two layer networks, having to\\nchoose the number of hidden units, and then get the learning rate\\nand initialization “right” can take a bit of work. clearly it can be\\nautomated, but nonetheless it takes time. another difﬁculty of neural networks is that their weights can\\nbe difﬁcult to interpret. you’ve seen that, for linear networks, you\\ncan often interpret high weights as indicative of positive examples\\nand low weights as indicative of negative examples. in multilayer\\nnetworks, it becomes very difﬁcult to try to understand what the\\ndifferent hidden units are doing. ',\n",
       " ' the deﬁnition of neural networks and the back-propagation algo-\\nrithm can be generalized beyond two layers to any arbitrary directed\\nacyclic graph. in practice, it is most common to use a layered net-\\nwork like that shown in figure 10.4 unless one has a very strong\\nreason (aka inductive bias) to do something different. however, the\\nview as a directed graph sheds a different sort of insight on the back-\\npropagation algorithm. suppose that your network structure is stored in some directed\\nacyclic graph, like that in figure 10.5. we index nodes in this graph\\nas u, v. the activation before applying non-linearity at a node is au\\nand after non-linearity is hu. the graph has a single sink, which is\\nthe output node y with activation ay (no non-linearity is performed figure 10.4: multi-layer network figure 10.5: dag network neural networks 137 hu ← corresponding feature of x algorithm 27 forwardpropagation(x)\\n1: for all input nodes u do\\n2:\\n3: end for\\n4: for all nodes v in the network whose parent’s are computed do\\n5: av ← ∑u∈par(v) w(u,v)hu\\nhv ← tanh(av) 6:\\n7: end for\\n8: return ay algorithm 28 backpropagation(x, y)\\n1: run forwardpropagation(x) to compute activations\\n2: ey ← y − ay\\n3: for all nodes v in the network whose error ev is computed do\\n4: for all u ∈ par(v) do // compute overall network error 5: 6: gu,v ← −evhu\\n// compute gradient of this edge\\neu ← eu + evwu,v(1 − tanh2(au)) // compute the “error” of the parent node end for 7:\\n8: end for\\n9: return all gradients ge on the output unit). the graph has d-many inputs (i.e., nodes with\\nno parent), whose activations hu are given by an input example. an\\nedge (u, v) is from a parent to a child (i.e., from an input to a hidden\\nunit, or from a hidden unit to the sink). each edge has a weight wu,v.\\nwe say that par(u) is the set of parents of u. there are two relevant algorithms: forward-propagation and back- propagation. forward-propagation tells you how to compute the\\nactivation of the sink y given the inputs. back-propagation computes\\nderivatives of the edge weights for a given input. the key aspect of the forward-propagation algorithm is to iter-\\natively compute activations, going deeper and deeper in the dag.\\nonce the activations of all the parents of a node u have been com-\\nputed, you can compute the activation of node u. this is spelled out\\nin algorithm 10.4. this is also explained pictorially in figure 10.6.\\nback-propagation (see algorithm 10.4) does the opposite: it com- putes gradients top-down in the network. the key idea is to compute\\nan error for each node in the network. the error at the output unit is\\nthe “true error.” for any input unit, the error is the amount of gradi-\\nent that we see coming from our children (i.e., higher in the network).\\nthese errors are computed backwards in the network (hence the\\nname back-propagation) along with the gradients themselves. this is\\nalso explained pictorially in figure 10.7. given the back-propagation algorithm, you can directly run gradi- ent descent, using it as a subroutine for computing the gradients. figure 10.6: picture of forward prop figure 10.7: picture of back prop 138 a course in machine learning ',\n",
       " ' at this point, you’ve seen how to train two-layer networks and how\\nto train arbitrary networks. you’ve also seen a theorem that says\\nthat two-layer networks are universal function approximators. this\\nbegs the question: if two-layer networks are so great, why do we care\\nabout deeper networks? to understand the answer, we can borrow some ideas from cs\\ntheory, namely the idea of circuit complexity. the goal is to show\\nthat there are functions for which it might be a “good idea” to use a\\ndeep network. in other words, there are functions that will require a\\nhuge number of hidden units if you force the network to be shallow,\\nbut can be done in a small number of units if you allow it to be deep.\\nthe example that we’ll use is the parity function which, ironically\\nenough, is just a generalization of the xor problem. the function is\\ndeﬁned over binary inputs as: xd mod 2 (10.12) parity(x) = ∑\\nd (cid:40) = 1 if the number of 1s in x is odd\\n0 if the number of 1s in x is even (10.13)\\nit is easy to deﬁne a circuit of depth o(log2 d) with o(d)-many\\ngates for computing the parity function. each gate is an xor, ar-\\nranged in a complete binary tree, as shown in figure 10.8. (if you\\nwant to disallow xor as a gate, you can ﬁx this by allowing the\\ndepth to be doubled and replacing each xor with an and, or and\\nnot combination, like you did at the beginning of this chapter.) this shows that if you are allowed to be deep, you can construct a\\ncircuit with that computes parity using a number of hidden units that\\nis linear in the dimensionality. so can you do the same with shallow\\ncircuits? the answer is no. it’s a famous result of circuit complexity\\nthat parity requires exponentially many gates to compute in constant\\ndepth. the formal theorem is below:\\ntheorem 11 (parity function complexity). any circuit of depth k <\\nlog2 d that computes the parity function of d input bits must contain oed\\ngates. this is a very famous result because it shows that constant-depth\\ncircuits are less powerful that deep circuits. although a neural net-\\nwork isn’t exactly the same as a circuit, the is generally believed that\\nthe same result holds for neural networks. at the very least, this\\ngives a strong indication that depth might be an important considera-\\ntion in neural networks. one way of thinking about the issue of breadth versus depth has to do with the number of parameters that need to be estimated. by figure 10.8: nnet:paritydeep: deep\\nfunction for computing parity ? what is it about neural networks\\nthat makes it so that the theorem\\nabout circuits does not apply di-\\nrectly? neural networks 139 ? while these small derivatives might\\nmake training difﬁcult, they might\\nbe good for other reasons: what\\nreasons? the heuristic that you need roughly one or two examples for every\\nparameter, a deep model could potentially require exponentially\\nfewer examples to train than a shallow model! this now ﬂips the question: if deep is potentially so much better, why doesn’t everyone use deep networks? there are at least two\\nanswers. first, it makes the architecture selection problem more\\nsigniﬁcant. namely, when you use a two-layer network, the only\\nhyperparameter to choose is how many hidden units should go in\\nthe middle layer. when you choose a deep network, you need to\\nchoose how many layers, and what is the width of all those layers.\\nthis can be somewhat daunting. a second issue has to do with training deep models with back-\\npropagation. in general, as back-propagation works its way down\\nthrough the model, the sizes of the gradients shrink. you can work\\nthis out mathematically, but the intuition is simpler. if you are the\\nbeginning of a very deep network, changing one single weight is\\nunlikely to have a signiﬁcant effect on the output, since it has to\\ngo through so many other units before getting there. this directly\\nimplies that the derivatives are small. this, in turn, means that back-\\npropagation essentially never moves far from its initialization when\\nrun on very deep networks. finding good ways to train deep networks is an active research\\narea. there are two general strategies. the ﬁrst is to attempt to ini-\\ntialize the weights better, often by a layer-wise initialization strategy.\\nthis can be often done using unlabeled data. after this initializa-\\ntion, back-propagation can be run to tweak the weights for whatever\\nclassiﬁcation problem you care about. a second approach is to use a\\nmore complex optimization procedure, rather than gradient descent.\\nyou will learn about some such procedures later in this book. ',\n",
       " ' at this point, we’ve seen that: (a) neural networks can mimic linear\\nfunctions and (b) they can learn more complex functions. a rea-\\nsonable question is whether they can mimic a knn classiﬁer, and\\nwhether they can do it efﬁciently (i.e., with not-too-many hidden\\nunits). a natural way to train a neural network to mimic a knn classiﬁer is to replace the sigmoid link function with a radial basis function\\n(rbf). in a sigmoid network (i.e., a network with sigmoid links),\\nthe hidden units were computed as hi = tanh(wi, x·). in an rbf\\nnetwork, the hidden units are computed as: (10.14) (cid:104)−γi wi − x2(cid:105) hi = exp 140 a course in machine learning in other words, the hidden units behave like little gaussian “bumps” centered around locations speciﬁed by the vectors wi. a one-dimensional\\nexample is shown in figure 10.9. the parameter γi speciﬁes the width\\nof the gaussian bump. if γi is large, then only data points that are\\nreally close to wi have non-zero activations. to distinguish sigmoid\\nnetworks from rbf networks, the hidden units are typically drawn\\nwith sigmoids or with gaussian bumps, as in figure 10.10. training rbf networks involves ﬁnding good values for the gas-\\nsian widths, γi, the centers of the gaussian bumps, wi and the con-\\nnections between the gaussian bumps and the output unit, v. this\\ncan all be done using back-propagation. the gradient terms for v re-\\nmain unchanged from before, the the derivates for the other variables\\ndiffer (see exercise ??). one of the big questions with rbf networks is: where should the gaussian bumps be centered? one can, of course, apply back-\\npropagation to attempt to ﬁnd the centers. another option is to spec-\\nify them ahead of time. for instance, one potential approach is to\\nhave one rbf unit per data point, centered on that data point. if you\\ncarefully choose the γs and vs, you can obtain something that looks\\nnearly identical to distance-weighted knn by doing so. this has the\\nadded advantage that you can go futher, and use back-propagation\\nto learn good gaussian widths (γ) and “voting” factors (v) for the\\nnearest neighbor algorithm. 10.7 further reading todo further reading figure 10.9: nnet:rbfpicture: a one-d\\npicture of rbf bumps figure 10.10: nnet:unitsymbols: picture\\nof nnet with sigmoid/rbf units ? consider an rbf network with\\none hidden unit per training point,\\ncentered at that point. what bad\\nthing might happen if you use back-\\npropagation to estimate the γs and\\nv on this data if you’re not careful?\\nhow could you be careful? learning objectives:\\n• explain how kernels generalize both feature combinations and basis\\nfunctions. • contrast dot products with kernel products. • implement kernelized perceptron.\\n• derive a kernelized version of regularized least squares regression. • implement a kernelized version of the perceptron. • derive the dual formulation of the support vector machine. dependencies: many who have had an opportunity of knowing any more about\\nmathematics confuse it with arithmetic, and consider it an arid\\nscience. in reality, however, it is a science which requires a great\\namount of imagination. – soﬁa kovalevskaya linear models are great because they are easy to understand\\nand easy to optimize. they suffer because they can only learn very\\nsimple decision boundaries. neural networks can learn more com-\\nplex decision boundaries, but lose the nice convexity properties of\\nmany linear models. one way of getting a linear model to behave non-linearly is to transform the input. for instance, by adding feature pairs as addi-\\ntional inputs. learning a linear model on such a representation is\\nconvex, but is computationally prohibitive in all but very low dimen-\\nsional spaces. you might ask: instead of explicitly expanding the fea-\\nture space, is it possible to stay with our original data representation\\nand do all the feature blow up implicitly? surprisingly, the answer is\\noften “yes” and the family of techniques that makes this possible are\\nknown as kernel approaches. ',\n",
       " ' in section 5.4, you learned one method for increasing the expressive\\npower of linear models: explode the feature space. for instance,\\na “quadratic” feature explosion might map a feature vector x =\\n(cid:104)x1, x2, x3, . . . , xd(cid:105) to an expanded version denoted φ(x): φ(x) = (cid:104)1, 2x1, 2x2, 2x3, . . . , 2xd,\\nx2\\n1, x1x2, x1x3, . . . , x1xd,\\nx2x1, x2\\n2, x2x3, . . . , x2xd,\\nx3x1, x3x2, x2\\n3, . . . , x2xd,\\n. . . ,\\nd(cid:105)\\nxdx1, xdx2, xdx3, . . . , x2 (11.1) (note that there are repetitions here, but hopefully most learning\\nalgorithms can deal well with redundant features; in particular, the\\n2x1 terms are due to collapsing some repetitions.) 142 a course in machine learning you could then train a classiﬁer on this expanded feature space.\\nthere are two primary concerns in doing so. the ﬁrst is computa-\\ntional: if your learning algorithm scales linearly in the number of fea-\\ntures, then you’ve just squared the amount of computation you need\\nto perform; you’ve also squared the amount of memory you’ll need.\\nthe second is statistical: if you go by the heuristic that you should\\nhave about two examples for every feature, then you will now need\\nquadratically many training examples in order to avoid overﬁtting.\\nthis chapter is all about dealing with the computational issue. it will turn out in chapter 12 that you can also deal with the statistical\\nissue: for now, you can just hope that regularization will be sufﬁcient\\nto attenuate overﬁtting. the key insight in kernel-based learning is that you can rewrite\\nmany linear models in a way that doesn’t require you to ever ex-\\nplicitly compute φ(x). to start with, you can think of this purely\\nas a computational “trick” that enables you to use the power of a\\nquadratic feature mapping without actually having to compute and\\nstore the mapped vectors. later, you will see that it’s actually quite a\\nbit deeper. most algorithms we discuss involve a product of the form\\nw · φ(x), after performing the feature mapping. the goal is to rewrite\\nthese algorithms so that they only ever depend on dot products be-\\ntween two examples, say x and z; namely, they depend on φ(x) · φ(z).\\nto understand why this is helpful, consider the quadratic expansion\\nfrom above, and the dot-product between two vectors. you get: φ(x) · φ(z) = 1 + x1z1 + x2z2 + · · · + xdzd + x2\\n1 + · · · + x1xdz1zd+\\n1z2\\n· · · + xdx1zdz1 + xdx2zdz2 + · · · + x2\\n(11.2)\\ndz2\\nd\\n= 1 + 2 ∑\\nxdzd + ∑\\n∑\\n(11.3)\\ne\\nd\\nd\\n= 1 + 2x · z + (x · z)2\\n= (1 + x · z)2 (11.4)\\n(11.5) xdxezdze thus, you can compute φ(x) · φ(z) in exactly the same amount of\\ntime as you can compute x · z (plus the time it takes to perform an\\naddition and a multiply, about 0.02 nanoseconds on a circa 2011\\nprocessor). the rest of the practical challenge is to rewrite your algorithms so\\nthat they only depend on dot products between examples and not on\\nany explicit weight vectors. ',\n",
       " ' consider the original perceptron algorithm from chapter 4, re- peated in algorithm 11.2 using linear algebra notation and using fea-\\nture expansion notation φ(x). in this algorithm, there are two places kernel methods 143 // initialize weights and bias algorithm 29 perceptrontrain(d, maxiter)\\n1: w ← 0, b ← 0\\n2: for iter = 1 . . . maxiter do\\n3: // compute activation for this example // update weights\\n// update bias for all (x,y) ∈ d do\\na ← w · φ(x) + b\\nif ya ≤ 0 then\\nw ← w + y φ(x)\\nb ← b + y 4: 5: 6: 7: 8: end if\\nend for 9:\\n10: end for\\n11: return w, b math review spans\\nif u = {ui}i\\nlinear combinations of uis; namely: span(u ) = {∑i aiui\\nlinearly independent, then the dimension of span(u ) is i; in particular, if there are d-many linearly\\nindependent vectors then they span rd. i=1 is a set of vectors in rd, then the span of u is the set of vectors that can be written as\\n: a1 ∈ r, . . . , ai ∈ r}. if all of the uis are figure 11.1: where φ(x) is used explicitly. the ﬁrst is in computing the activation\\n(line 4) and the second is in updating the weights (line 6). the goal is\\nto remove the explicit dependence of this algorithm on φ and on the\\nweight vector. to do so, you can observe that at any point in the algorithm, the\\nweight vector w can be written as a linear combination of expanded\\ntraining data. in particular, at any point, w = ∑n αnφ(xn) for some\\nparameters α. initially, w = 0 so choosing α = 0 yields this. if the\\nﬁrst update occurs on the nth training example, then the resolution\\nweight vector is simply ynφ(xn), which is equivalent to setting αn =\\nyn. if the second update occurs on the mth training example, then all\\nyou need to do is update αm ← αm + ym. this is true, even if you\\nmake multiple passes over the data. this observation leads to the\\nfollowing representer theorem, which states that the weight vector of\\nthe perceptron lies in the span of the training data. theorem 12 (perceptron representer theorem). during a run of\\nthe perceptron algorithm, the weight vector w is always in the span of the\\n(assumed non-empty) training data, φ(x1), . . . , φ(xn).\\nproof of theorem 12. by induction. base case: the span of any non-\\nempty set contains the zero vector, which is the initial weight vec-\\ntor. inductive case: suppose that the theorem is true before the kth\\nupdate, and suppose that the kth update happens on example n.\\nby the inductive hypothesis, you can write w = ∑i αiφ(xi) before 144 a course in machine learning algorithm 30 kernelizedperceptrontrain(d, maxiter)\\n1: α ← 0, b ← 0\\n2: for iter = 1 . . . maxiter do\\nfor all (xn,yn) ∈ d do\\n3: // initialize coefﬁcients and bias a ← ∑m αmφ(xm) · φ(xn) + b\\nif yna ≤ 0 then\\nαn ← αn + yn\\nb ← b + y // compute activation for this example // update coefﬁcients\\n// update bias 4: 5: 6: 7: 8: end if\\nend for 9:\\n10: end for\\n11: return α, b the update. the new weight vector is [∑i αiφ(xi)] + ynφ(xn) =\\n∑i(αi + yn[i = n])φ(xi), which is still in the span of the training\\ndata. now that you know that you can always write w = ∑n αnφ(xn) for some αis, you can additionall compute the activations (line 4) as: (cid:32) (cid:33) w · φ(x) + b = ∑\\nn αnφ(xn) · φ(x) + b\\n(cid:105) + b (cid:104) = ∑\\nn αn φ(xn) · φ(x) deﬁnition of w (11.6) dot products are linear (11.7) this now depends only on dot-products between data points, and\\nnever explicitly requires a weight vector. you can now rewrite the\\nentire perceptron algorithm so that it never refers explicitly to the\\nweights and only ever depends on pairwise dot products between\\nexamples. this is shown in algorithm 11.2. the advantage to this “kernelized” algorithm is that you can per- form feature expansions like the quadratic feature expansion from\\nthe introduction for “free.” for example, for exactly the same cost as\\nthe quadratic features, you can use a cubic feature map, computed\\n¨φ(x)φ(z) = (1 + x · z)3, which corresponds to three-way inter-\\nas\\nactions between variables. (and, in general, you can do so for any\\npolynomial degree p at the same computational complexity.) ',\n",
       " ' for a complete change of pace, consider the k-means algorithm from\\nsection 3. this algorithm is for clustering where there is no notion of\\n“training labels.” instead, you want to partition the data into coher-\\nent clusters. for data in rd, it involves randomly initializing k-many kernel methods 145 cluster means µ(1), . . . , µ(k). the algorithm then alternates between the\\nfollowing two steps until convergence, with x replaced by φ(x) since\\nthat is the eventual goal: 1. for each example n, set cluster label zn = arg mink\\n2. for each cluster k, update µ(k) = 1\\nnk ∑n:zn=k φ(xn), where nk is the number of n with zn = k. (cid:12)(cid:12)(cid:12)(cid:12)φ(xn) − µ(k)(cid:12)(cid:12)(cid:12)(cid:12)2. the question is whether you can perform these steps without ex-\\nplicitly computing φ(xn). the representer theorem is more straight-\\nforward here than in the perceptron. the mean of a set of data is,\\nalmost by deﬁnition, in the span of that data (choose the ais all to be\\nequal to 1/n). thus, so long as you initialize the means in the span\\nof the data, you are guaranteed always to have the means in the span\\nof the data. given this, you know that you can write each mean as an\\n(k)\\nexpansion of the data; say that µ(k) = ∑n α\\nn φ(xn) for some parame-\\nters α n (there are n×k-many such parameters). given this expansion, in order to execute step (1), you need to (k) compute norms. this can be done as follows: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)φ(xn) − µ(k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)φ(xn) − ∑\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) φ(xn)2 + (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∑ m m α zn = arg min k = arg min k = arg min k (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 (k)\\nm φ(xm) (cid:34) + φ(xn) · (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 (11.9) (cid:35) (k)\\nm φ(xm) α ∑\\nm (k)\\nm φ(xm) α (11.8) deﬁnition of zn deﬁnition of µ(k) expand quadratic term linearity and constant k α (k) (k)\\nm α ∑\\nm ∑\\nm(cid:48) = arg min m(cid:48) φ(xm) · φ(xm(cid:48) ) + ∑ (11.10)\\nm φ(xm) · φ(xn) + const\\n(11.11)\\nthis computation can replace the assignments in step (1) of k-means.\\nthe mean updates are more direct in step (2): (k) m α (cid:40) 1 nk\\n0 if zn = k\\notherwise (11.12) µ(k) = 1\\nnk ∑\\nn:zn=k φ(xn) ⇐⇒ α (k)\\nn = ',\n",
       " ' a kernel is just a form of generalized dot product. you can also\\nthink of it as simply shorthand for φ(x) · φ(z), which is commonly\\nwritten kφ(x, z). or, when φ is clear from context, simply k(x, z). 146 a course in machine learning this is often refered to as the kernel product between x and z (under\\nthe mapping φ). in this view, what you’ve seen in the preceding two sections is that you can rewrite both the perceptron algorithm and the k-means\\nalgorithm so that they only ever depend on kernel products between data\\npoints, and never on the actual datapoints themselves. this is a very pow-\\nerful notion, as it has enabled the development of a large number of\\nnon-linear algorithms essentially “for free” (by applying the so-called\\nkernel trick, that you’ve just seen twice). this raises an interesting question. if you have rewritten these\\nalgorithms so that they only depend on the data through a function\\nk : x×x → r, can you stick any function k in these algorithms,\\nor are there some k that are “forbidden?” in one sense, you “could”\\nuse any k, but the real question is: for what types of functions k do\\nthese algorithms retain the properties that we expect them to have\\n(like convergence, optimality, etc.)?\\none way to answer this question is to say that k(·,·) is a valid\\nkernel if it corresponds to the inner product between two vectors.\\nthat is, k is valid if there exists a function φ such that k(x, z) =\\nφ(x) · φ(z). this is a direct deﬁnition and it should be clear that if k\\nsatisﬁes this, then the algorithms go through as expected (because\\nthis is how we derived them). you’ve already seen the general class of polynomial kernels, which have the form: k(poly)\\nd (x, z) = 1 + x · z (cid:16) (cid:17)d (11.13) where d is a hyperparameter of the kernel. these kernels correspond\\nto polynomial feature expansions.\\nthere is an alternative characterization of a valid kernel function\\nthat is more mathematical. it states that k : x×x → r is a kernel if\\nk is positive semi-definite (or, in shorthand, psd). this property is\\nalso sometimes called mercer’s condition. in this context, this means the for all functions f that are square integrable (i.e.,(cid:82) f (x)2dx < ∞),\\n(cid:90)(cid:90) other than the zero function, the following property holds: f (x)k(x, z) f (z)dxdz > 0 (11.14) this likely seems like it came out of nowhere. unfortunately, the\\nconnection is well beyond the scope of this book, but is covered well\\nis external sources. for now, simply take it as a given that this is an\\nequivalent requirement. (for those so inclined, the appendix of this\\nbook gives a proof, but it requires a bit of knowledge of function\\nspaces to understand.) the question is: why is this alternative characterization useful? it\\nis useful because it gives you an alternative way to construct kernel functions. for instance, using it you can easily prove the following,\\nwhich would be difﬁcult from the deﬁnition of kernels as inner prod-\\nucts after feature mappings. theorem 13 (kernel addition). if k1 and k2 are kernels, the k deﬁned\\nby k(x, z) = k1(x, z) + k2(x, z) is also a kernel.\\nproof of theorem 13. you need to verify the positive semi-deﬁnite\\nproperty on k. you can do this as follows: (cid:90)(cid:90) f (x)k(x, z) f (z)dxdz = = f (x) [k1(x, z) + k2(x, z)] f (z)dxdz\\n(11.15) f (x)k1(x, z) f (z)dxdz kernel methods 147 deﬁnition of k (cid:90)(cid:90)\\n(cid:90)(cid:90)\\n(cid:90)(cid:90) + f (x)k2(x, z) f (z)dxdz distributive rule (11.16) (11.17) k1 and k2 are psd > 0 + 0 more generally, any positive linear combination of kernels is still a\\nkernel. speciﬁcally, if k1, . . . , km are all kernels, and α1, . . . , αm ≥ 0,\\nthen k(x, z) = ∑m αmkm(x, z) is also a kernel. you can also use this property to show that the following gaus- sian kernel (also called the rbf kernel) is also psd: (cid:104)−γ x − z2(cid:105) k(rbf)\\nγ (x, z) = exp (11.18) here γ is a hyperparameter that controls the width of this gaussian-\\nlike bumps. to gain an intuition for what the rbf kernel is doing,\\nconsider what prediction looks like in the perceptron: f (x) = ∑\\nn\\n= ∑\\nn αnk(xn, x) + b (cid:104)−γ xn − z2(cid:105) αn exp (11.19) (11.20) in this computation, each training example is getting to “vote” on the\\nlabel of the test point x. the amount of “vote” that the nth training\\nexample gets is proportional to the negative exponential of the dis-\\ntance between the test point and itself. this is very much like an rbf\\nneural network, in which there is a gaussian “bump” at each training\\nexample, with variance 1/(2γ), and where the αns act as the weights\\nconnecting these rbf bumps to the output. showing that this kernel is positive deﬁnite is a bit of an exercise in analysis (particularly, integration by parts), but otherwise not\\ndifﬁcult. again, the proof is provided in the appendix. 148 a course in machine learning so far, you have seen two bsaic classes of kernels: polynomial\\nkernels (k(x, z) = (1 + x · z)d), which includes the linear kernel\\n(k(x, z) = x · z) and rbf kernels (k(x, z) = exp[−γ x − z2]). the\\nformer have a direct connection to feature expansion; the latter to\\nrbf networks. you also know how to combine kernels to get new\\nkernels by addition. in fact, you can do more than that: the product\\nof two kernels is also a kernel. as far as a “library of kernels” goes, there are many. polynomial\\nand rbf are by far the most popular. a commonly used, but techni-\\ncally invalid kernel, is the hyperbolic-tangent kernel, which mimics\\nthe behavior of a two-layer neural network. it is deﬁned as: k(tanh) = tanh(1 + x · z) warning: not psd (11.21) a ﬁnal example, which is not very common, but is nonetheless\\ninteresting, is the all-subsets kernel. suppose that your d features\\nare all binary: all take values 0 or 1. let a ⊆ {1, 2, . . . d} be a subset of features, and let fa(x) = (cid:86) d∈a xd be the conjunction of all the\\nfeatures in a. let φ(x) be a feature vector over all such as, so that\\nthere are 2d features in the vector φ. you can compute the kernel\\nassociated with this feature mapping as: (cid:16) (cid:17) k(subs)(x, z) = ∏\\nd 1 + xdzd (11.22) verifying the relationship between this kernel and the all-subsets\\nfeature mapping is left as an exercise (but closely resembles the ex-\\npansion for the quadratic kernel). ',\n",
       " ' kernelization predated support vector machines, but svms are def-\\ninitely the model that popularized the idea. recall the deﬁnition of\\nthe soft-margin svm from chapter 7.7 and in particular the opti-\\nmization problem (7.38), which attempts to balance a large margin\\n(small w2) with a small loss (small ξns, where ξn is the slack on\\nthe nth training example). this problem is repeated below: ξn (11.23) min\\nw,b,ξ 1\\n2 w2 + c ∑\\nn subj. to yn (w · xn + b) ≥ 1 − ξn ξn ≥ 0 (∀n)\\n(∀n) previously, you optimized this by explicitly computing the slack\\nvariables ξn, given a solution to the decision boundary, w and b.\\nhowever, you are now an expert with using lagrange multipliers kernel methods 149 to optimize constrained problems! the overall goal is going to be to\\nrewrite the svm optimization problem in a way that it no longer ex-\\nplicitly depends on the weights w and only depends on the examples\\nxn through kernel products. there are 2n constraints in this optimization, one for each slack constraint and one for the requirement that the slacks are non-\\nnegative. unlike the last time, these constraints are now inequalities,\\nwhich require a slightly different solution. first, you rewrite all the\\ninequalities so that they read as something ≥ 0 and then add cor-\\nresponding lagrange multipliers. the main difference is that the\\nlagrange multipliers are now constrained to be non-negative, and\\ntheir sign in the augmented objective function matters.\\nthe second set of constraints is already in the proper form; the\\nﬁrst set can be rewritten as yn (w · xn + b) − 1 + ξn ≥ 0. you’re now\\nready to construct the lagrangian, using multipliers αn for the ﬁrst\\nset of constraints and βn for the second set. l(w, b, ξ, α, β) = 1\\n2 ξn − ∑ βnξn w2 + c ∑\\nn\\n− ∑\\nn n αn [yn (w · xn + b) − 1 + ξn] the new optimization problem is: min\\nw,b,ξ max\\nα≥0 max\\nβ≥0 l(w, b, ξ, α, β) (11.24) (11.25) (11.26) the intuition is exactly the same as before. if you are able to ﬁnd a\\nsolution that satisﬁes the constraints (e.g., the purple term is prop-\\nerly non-negative), then the βns cannot do anything to “hurt” the\\nsolution. on the other hand, if the purple term is negative, then the\\ncorresponding βn can go to +∞, breaking the solution. you can solve this problem by taking gradients. this is a bit te- dious, but and important step to realize how everything ﬁts together.\\nsince your goal is to remove the dependence on w, the ﬁrst step is to\\ntake a gradient with respect to w, set it equal to zero, and solve for w\\nin terms of the other variables. ∇wl = w − ∑ n αnynxn = 0 ⇐⇒ w = ∑ n αnynxn (11.27) at this point, you should immediately recognize a similarity to the\\nkernelized perceptron: the optimal weight vector takes exactly the\\nsame form in both algorithms.\\nyou can now take this new expression for w and plug it back in to\\nthe expression for l, thus removing w from consideration. to avoid\\nsubscript overloading, you should replace the n in the expression for 150 a course in machine learning w with, say, m. this yields: l(b, ξ, α, β) = 1\\n2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∑ m\\n− ∑\\nn (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)\\n(cid:32)(cid:34) ∑\\nm αmymxm (cid:34) + c ∑\\nn ξn − ∑\\n(cid:35) n βnξn (cid:33) αn yn αmymxm · xn + b − 1 + ξn (11.28) (cid:35) (11.29) (11.31)\\n(11.32) (11.33) at this point, it’s convenient to rewrite these terms; be sure you un-\\nderstand where the following comes from: l(b, ξ, α, β) = 1\\n2 ∑\\n∑\\nn\\nm\\n− ∑\\nn αnαmynymxn · xm + ∑\\nαnαmynymxn · xm − ∑\\n∑\\nm n n (c − βn)ξn (11.30) αn (ynb − 1 + ξn) = − 1\\nαnαmynymxn · xm + ∑\\n∑\\n∑\\n2\\nn\\nm\\n−b ∑\\nαnyn − ∑\\nn αn(ξn − 1) n n (c − βn)ξn things are starting to look good: you’ve successfully removed the de-\\npendence on w, and everything is now written in terms of dot prod-\\nucts between input vectors! this might still be a difﬁcult problem to\\nsolve, so you need to continue and attempt to remove the remaining\\nvariables b and ξ. the derivative with respect to b is:\\n∂l\\n∂b = − ∑\\nn αnyn = 0 (11.34) this doesn’t allow you to substitute b with something (as you did\\nwith w), but it does mean that the fourth term (b ∑n αnyn) goes to\\nzero at the optimum. the last of the original variables is ξn; the derivatives in this case look like: ∂l\\n∂ξn = c − βn − αn ⇐⇒ c − βn = αn (11.35) again, this doesn’t allow you to substitute, but it does mean that you\\ncan rewrite the second term, which as ∑n(c − βn)ξn as ∑n αnξn. this\\nthen cancels with (most of) the ﬁnal term. however, you need to be\\ncareful to remember something. when we optimize, both αn and βn\\nare constrained to be non-negative. what this means is that since we\\nare dropping β from the optimization, we need to ensure that αn ≤ c,\\notherwise the corresponding β will need to be negative, which is not kernel methods 151 allowed. you ﬁnally wind up with the following, where xn · xm has\\nbeen replaced by k(xn, xm): l(α) = ∑\\nn αn − 1\\n2 ∑\\nn ∑\\nm αnαmynymk(xn, xm) (11.36) if you are comfortable with matrix notation, this has a very compact\\nform. let 1 denote the n-dimensional vector of all 1s, let y denote\\nthe vector of labels and let g be the n×n matrix, where gn,m =\\nynymk(xn, xm), then this has the following form: (cid:62)1 − 1\\n2 α (cid:62)gα l(α) = α (11.37)\\nthe resulting optimization problem is to maximize l(α) as a function\\nof α, subject to the constraint that the αns are all non-negative and\\nless than c (because of the constraint added when removing the β\\nvariables). thus, your problem is: min 1\\n− l(α) =\\n2\\nsubj. to 0 ≤ αn ≤ c α ∑\\nn ∑\\nm αnαmynymk(xn, xm) − ∑ n αn (11.38)\\n(∀n) one way to solve this problem is gradient descent on α. the only\\ncomplication is making sure that the αs satisfy the constraints. in\\nthis case, you can use a projected gradient algorithm: after each\\ngradient update, you adjust your parameters to satisfy the constraints\\nby projecting them into the feasible region. in this case, the projection\\nis trivial: if, after a gradient step, any αn < 0, simply set it to 0; if any\\nαn > c, set it to c. ',\n",
       " ' the prior discussion involved quite a bit of math to derive a repre-\\nsentation of the support vector machine in terms of the lagrange\\nvariables. this mapping is actually sufﬁciently standard that every-\\nthing in it has a name. the original problem variables (w, b, ξ) are\\ncalled the primal variables; the lagrange variables are called the\\ndual variables. the optimization problem that results after removing\\nall of the primal variables is called the dual problem. a succinct way of saying what you’ve done is: you found that after converting the svm into its dual, it is possible to kernelize. to understand svms, a ﬁrst step is to peek into the dual formula-\\ntion, eq (11.38). the objective has two terms: the ﬁrst depends on the\\ndata, and the second depends only on the dual variables. the ﬁrst\\nthing to notice is that, because of the second term, the αs “want” to 152 a course in machine learning get as large as possible. the constraint ensures that they cannot ex-\\nceed c, which means that the general tendency is for the αs to grow\\nas close to c as possible. to further understand the dual optimization problem, it is useful to think of the kernel as being a measure of similarity between two\\ndata points. this analogy is most clear in the case of rbf kernels,\\nbut even in the case of linear kernels, if your examples all have unit\\nnorm, then their dot product is still a measure of similarity. since you\\ncan write the prediction function as f (ˆx) = sign(∑n αnynk(xn, ˆx)), it\\nis natural to think of αn as the “importance” of training example n,\\nwhere αn = 0 means that it is not used at all at test time. consider two data points that have the same label; namely, yn = ym. this means that ynym = +1 and the objective function has a term\\nthat looks like αnαmk(xn, xm). since the goal is to make this term\\nsmall, then one of two things has to happen: either k has to be small,\\nor αnαm has to be small. if k is already small, then this doesn’t affect\\nthe setting of the corresponding αs. but if k is large, then this strongly\\nencourages at least one of αn or αm to go to zero. so if you have two\\ndata points that are very similar and have the same label, at least one\\nof the corresponding αs will be small. this makes intuitive sense: if\\nyou have two data points that are basically the same (both in the x\\nand y sense) then you only need to “keep” one of them around.\\nsuppose that you have two data points with different labels:\\nynym = −1. again, if k(xn, xm) is small, nothing happens. but if\\nit is large, then the corresponding αs are encouraged to be as large as\\npossible. in other words, if you have two similar examples with dif-\\nferent labels, you are strongly encouraged to keep the corresponding\\nαs as large as c. an alternative way of understanding the svm dual problem is\\ngeometrically. remember that the whole point of introducing the\\nvariable αn was to ensure that the nth training example was correctly\\nclassiﬁed, modulo slack. more formally, the goal of αn is to ensure\\nthat yn(w · xn + b) − 1 + ξn ≥ 0. suppose that this constraint it\\nnot satisﬁed. there is an important result in optimization theory,\\ncalled the karush-kuhn-tucker conditions (or kkt conditions, for\\nshort) that states that at the optimum, the product of the lagrange\\nmultiplier for a constraint, and the value of that constraint, will equal\\nzero. in this case, this says that at the optimum, you have: yn (w · xn + b) − 1 + ξn αn = 0 (11.39) (cid:104) (cid:105) in order for this to be true, it means that (at least) one of the follow-\\ning must be true: αn = 0 or yn (w · xn + b) − 1 + ξn = 0 (11.40) kernel methods 153 a reasonable question to ask is: under what circumstances will αn\\nbe non-zero? from the kkt conditions, you can discern that αn can\\nbe non-zero only when the constraint holds exactly; namely, that\\nyn (w · xn + b) − 1 + ξn = 0. when does that constraint hold ex-\\nactly? it holds exactly only for those points precisely on the margin of\\nthe hyperplane. in other words, the only training examples for which αn (cid:54)= 0 are those that lie precisely 1 unit away from the maximum margin\\ndecision boundary! (or those that are “moved” there by the corre-\\nsponding slack.) these points are called the support vectors because\\nthey “support” the decision boundary. in general, the number of sup-\\nport vectors is far smaller than the number of training examples, and\\ntherefore you naturally end up with a solution that only uses a subset\\nof the training data. from the ﬁrst discussion, you know that the points that wind up\\nbeing support vectors are exactly those that are “confusable” in the\\nsense that you have to examples that are nearby, but have different la-\\nbels. this is a completely in line with the previous discussion. if you\\nhave a decision boundary, it will pass between these “confusable”\\npoints, and therefore they will end up being part of the set of support\\nvectors. 11.7 further reading todo further reading learning objectives:\\n• explain why inductive bias is necessary. • deﬁne the pac model and explain why both the “p” and “a” are\\nnecessary. • explain the relationship between complexity measures and regulariz-\\ners. • identify the role of complexity in generalization. • formalize the relationship between margins and complexity. dependencies: the universe is under no obligation to make sense to you.\\nneil degrasse tyson – by now, you are an expert at building learning algorithms. you\\nprobably understand how they work, intuitively. and you under-\\nstand why they should generalize. however, there are several basic\\nquestions you might want to know the answer to. is learning always\\npossible? how many training examples will i need to do a good job\\nlearning? is my test performance going to be much worse than my\\ntraining performance? the key idea that underlies all these answer is\\nthat simple functions generalize well. the amazing thing is that you can actually prove strong results that address the above questions. in this chapter, you will learn\\nsome of the most important results in learning theory that attempt\\nto answer these questions. the goal of this chapter is not theory for\\ntheory’s sake, but rather as a way to better understand why learning\\nmodels work, and how to use this theory to build better algorithms.\\nas a concrete example, we will see how 2-norm regularization prov-\\nably leads to better generalization performance, thus justifying our\\ncommon practice! ',\n",
       " ' in contrast to the quote at the start of this chapter, a practitioner\\nfriend once said “i would happily give up a few percent perfor-\\nmance for an algorithm that i can understand.” both perspectives\\nare completely valid, and are actually not contradictory. the second\\nstatement is presupposing that theory helps you understand, which\\nhopefully you’ll ﬁnd to be the case in this chapter. theory can serve two roles. it can justify and help understand why common practice works. this is the “theory after” view. it can\\nalso serve to suggest new algorithms and approaches that turn out to\\nwork well in practice. this is the “theory before” view. often, it turns\\nout to be a mix. practitioners discover something that works surpris-\\ningly well. theorists ﬁgure out why it works and prove something\\nabout it. and in the process, they make it better or ﬁnd new algo- learning theory 155 1 in 2008, corinna cortes and vladimir\\nvapnik won it for support vector\\nmachines. rithms that more directly exploit whatever property it is that made\\nthe theory go through. theory can also help you understand what’s possible and what’s\\nnot possible. one of the ﬁrst things we’ll see is that, in general, ma-\\nchine learning can not work. of course it does work, so this means\\nthat we need to think harder about what it means for learning algo-\\nrithms to work. by understanding what’s not possible, you can focus\\nour energy on things that are. probably the biggest practical success story for theoretical machine learning is the theory of boosting, which you won’t actually see in\\nthis chapter. (you’ll have to wait for chapter 13.) boosting is a very\\nsimple style of algorithm that came out of theoretical machine learn-\\ning, and has proven to be incredibly successful in practice. so much\\nso that it is one of the de facto algorithms to run when someone gives\\nyou a new data set. in fact, in 2004, yoav freund and rob schapire\\nwon the acm’s paris kanellakis award for their boosting algorithm\\nadaboost. this award is given for theoretical accomplishments that\\nhave had a signiﬁcant and demonstrable effect on the practice of\\ncomputing.1 ',\n",
       " ' one nice thing about theory is that it forces you to be precise about\\nwhat you are trying to do. you’ve already seen a formal deﬁnition\\nof binary classiﬁcation in chapter 6. but let’s take a step back and\\nre-analyze what it means to learn to do binary classiﬁcation.\\nfrom an algorithmic perspective, a natural question is whether\\nthere is an “ultimate” learning algorithm, aawesome, that solves the\\nbinary classiﬁcation problem above. in other words, have you been\\nwasting your time learning about knn and perceptron and decision\\ntrees, when aawesome is out there. what would such an ultimate learning algorithm do? you would like it to take in a data set d and produce a function f . no matter\\nwhat d looks like, this function f should get perfect classiﬁcation on\\nall future examples drawn from the same distribution that produced\\nd. a little bit of introspection should demonstrate that this is impos-\\nsible. for instance, there might be label noise in our distribution. as\\na very simple example, let x = {−1, +1} (i.e., a one-dimensional,\\nbinary distribution. deﬁne the data distribution as: d((cid:104)+1(cid:105), +1) = 0.4\\nd((cid:104)+1(cid:105),−1) = 0.1 d((cid:104)−1(cid:105),−1) = 0.4\\nd((cid:104)−1(cid:105), +1) = 0.1 (12.1)\\n(12.2) in other words, 80% of data points in this distrubtion have x = y 156 a course in machine learning and 20% don’t. no matter what function your learning algorithm\\nproduces, there’s no way that it can do better than 20% error on this\\ndata.\\ngiven this, it seems hopeless to have an algorithm aawesome that\\nalways achieves an error rate of zero. the best that we can hope is\\nthat the error rate is not “too large.” unfortunately, simply weakening our requirement on the error\\nrate is not enough to make learning possible. the second source of\\ndifﬁculty comes from the fact that the only access we have to the\\ndata distribution is through sampling. in particular, when trying to\\nlearn about a distribution like that in 12.1, you only get to see data\\npoints drawn from that distribution. you know that “eventually” you\\nwill see enough data points that your sample is representative of the\\ndistribution, but it might not happen immediately. for instance, even\\nthough a fair coin will come up heads only with probability 1/2, it’s\\ncompletely plausible that in a sequence of four coin ﬂips you never\\nsee a tails, or perhaps only see one tails.\\naawesome will always work. in particular, if we happen to get a lousy\\nsample of data from d, we need to allow aawesome to do something\\ncompletely unreasonable.\\nthus, we cannot hope that aawesome will do perfectly, every time.\\nwe cannot even hope that it will do pretty well, all of the time. nor\\ncan we hope that it will do perfectly, most of the time. the best best\\nwe can reasonably hope of aawesome is that it it will do pretty well,\\nmost of the time. so the second thing that we have to give up is the hope that ',\n",
       " ' probably approximately correct (pac) learning is a formalism\\nof inductive learning based on the realization that the best we can\\nhope of an algorithm is that it does a good job (i.e., is approximately\\ncorrect), most of the time (i.e., it is probably appoximately correct).2 consider a hypothetical learning algorithm. you run it on ten dif-\\nferent binary classiﬁcation data sets. for each one, it comes back with\\nfunctions f1, f2, . . . , f10. for some reason, whenever you run f4 on a\\ntest point, it crashes your computer. for the other learned functions,\\ntheir performance on test data is always at most 5% error. if this\\nsitutation is guaranteed to happen, then this hypothetical learning\\nalgorithm is a pac learning algorithm. it satisﬁes “probably” because\\nit only failed in one out of ten cases, and it’s “approximate” because\\nit achieved low, but non-zero, error on the remainder of the cases. this leads to the formal deﬁnition of an (\\x01, δ) pac-learning algo- rithm. in this deﬁnition, \\x01 plays the role of measuring accuracy (in ? it’s clear that if your algorithm pro-\\nduces a deterministic function that\\nit cannot do better than 20% error.\\nwhat if it produces a stochastic (aka\\nrandomized) function? 2 leslie valiant invented the notion\\nof pac learning in 1984. in 2011,\\nhe received the turing award, the\\nhighest honor in computing for his\\nwork in learning theory, computational\\ncomplexity and parallel systems. learning theory 157 the previous example, \\x01 = 0.05) and δ plays the role of measuring\\nfailure (in the previous, δ = 0.1).\\ndeﬁnitions 1. an algorithm a is an (\\x01, δ)-pac learning algorithm if, for\\nall distributions d: given samples from d, the probability that it returns a\\n“bad function” is at most δ; where a “bad” function is one with test error\\nrate more than \\x01 on d. there are two notions of efﬁciency that matter in pac learning. the\\nﬁrst is the usual notion of computational complexity. you would prefer\\nan algorithm that runs quickly to one that takes forever. the second\\nis the notion of sample complexity: the number of examples required\\nfor your algorithm to achieve its goals. note that the goal of both\\nof these measure of complexity is to bound how much of a scarse\\nresource your algorithm uses. in the computational case, the resource\\nis cpu cycles. in the sample case, the resource is labeled examples.\\ndeﬁnition: an algorithm a is an efﬁcient (\\x01, δ)-pac learning al-\\ngorithm if it is an (\\x01, δ)-pac learning algorithm whose runtime is\\npolynomial in 1 \\x01 and 1\\nδ . in other words, suppose that you want your algorithm to achieve\\n4% error rate rather than 5%. the runtime required to do so should\\nno go up by an exponential factor. ',\n",
       " ' to get a better sense of pac learning, we will start with a completely\\nirrelevant and uninteresting example. the purpose of this example is\\nonly to help understand how pac learning works.\\nthe setting is learning conjunctions. your data points are binary\\nvectors, for instance x = (cid:104)0, 1, 1, 0, 1(cid:105). someone guarantees for you\\nthat there is some boolean conjunction that deﬁnes the true labeling\\nof this data. for instance, x1 ∧ ¬x2 ∧ x5 (“or” is not allowed). in\\nformal terms, we often call the true underlying classiﬁcation function\\nthe concept. so this is saying that the concept you are trying to learn\\nis a conjunction. in this case, the boolean function would assign a\\nnegative label to the example above. since you know that the concept you are trying to learn is a con-\\njunction, it makes sense that you would represent your function as\\na conjunction as well. for historical reasons, the function that you\\nlearn is often called a hypothesis and is often denoted h. however,\\nin keeping with the other notation in this book, we will continue to\\ndenote it f .\\nformally, the set up is as follows. there is some distribution dx\\nover binary data points (vectors) x = (cid:104)x1, x2, . . . , xd(cid:105). there is a ﬁxed 158 a course in machine learning concept conjunction c that we are trying to learn. there is no noise,\\nso for any example x, its true label is simply y = c(x). what is a reasonable algorithm in this case? suppose that you observe the example in table 12.1. from the ﬁrst example, we know\\nthat the true formula cannot include the term x1. if it did, this exam-\\nple would have to be negative, which it is not. by the same reason-\\ning, it cannot include x2. by analogous reasoning, it also can neither\\ninclude the term ¬x3 nor the term ¬x4. this suggests the algorithm in algorithm 12.4, colloquially the y\\n+1\\n+1\\n-1 x1\\n0\\n0\\n1 x2\\n0\\n1\\n1 x3\\n1\\n1\\n0 x4\\n1\\n1\\n1 table 12.1: data set for learning con-\\njunctions. ? verify that algorithm 12.4 main-\\ntains an invariant that it always errs\\non the side of classifying examples\\nnegative and never errs the other\\nway. “throw out bad terms” algorithm. in this algorithm, you begin with\\na function that includes all possible 2d terms. note that this function\\nwill initially classify everything as negative. you then process each\\nexample in sequence. on a negative example, you do nothing. on\\na positive example, you throw out terms from f that contradict the\\ngiven positive example. if you run this algorithm on the data in table 12.1, the sequence of f s that you cycle through are: f 0(x) = x1 ∧ ¬x1 ∧ x2 ∧ ¬x2 ∧ x3 ∧ ¬x3 ∧ x4 ∧ ¬x4\\nf 1(x) = ¬x1 ∧ ¬x2 ∧ x3 ∧ x4\\nf 2(x) = ¬x1 ∧ x3 ∧ x4\\nf 3(x) = ¬x1 ∧ x3 ∧ x4 (12.3)\\n(12.4)\\n(12.5)\\n(12.6) the ﬁrst thing to notice about this algorithm is that after processing\\nan example, it is guaranteed to classify that example correctly. this\\nobservation requires that there is no noise in the data. the second thing to notice is that it’s very computationally ef-\\nﬁcient. given a data set of n examples in d dimensions, it takes\\no(nd) time to process the data. this is linear in the size of the data\\nset. however, in order to be an efﬁcient (\\x01, δ)-pac learning algorithm,\\nyou need to be able to get a bound on the sample complexity of this\\nalgorithm. sure, you know that its run time is linear in the number\\nof example n. but how many examples n do you need to see in order\\nto guarantee that it achieves an error rate of at most \\x01 (in all but δ-\\nmany cases)? perhaps n has to be gigantic (like 22d/\\x01) to (probably)\\nguarantee a small error. the goal is to prove that the number of samples n required to\\n(probably) achieve a small error is not-too-big. the general proof\\ntechnique for this has essentially the same ﬂavor as almost every pac\\nlearning proof around. first, you deﬁne a “bad thing.” in this case,\\na “bad thing” is that there is some term (say ¬x8) that should have\\nbeen thrown out, but wasn’t. then you say: well, bad things happen.\\nthen you notice that if this bad thing happened, you must not have learning theory 159 // initialize function algorithm 31 binaryconjunctiontrain(d) f ← x1 ∧ ¬x1 ∧ x2 ∧ ¬x2 ∧ · · · ∧ xd ∧ ¬xd\\n1:\\n2: for all positive examples (x,+1) in d do\\n3: for d = 1 . . . d do\\nif xd = 0 then else f ← f without term “xd”\\nf ← f without term “¬xd” 4: 5: 6: 7: 8: end if\\nend for 9:\\n10: end for\\n11: return f seen any positive training examples with x8 = 0. so example with\\nx8 = 0 must have low probability (otherwise you would have seen\\nthem). so bad things must not be that common.\\ntheorem 14. with probability at least (1 − δ): algorithm 12.4 requires at\\nmost n = . . . examples to achieve an error rate ≤ \\x01. proof of theorem 14. let c be the concept you are trying to learn and\\nlet d be the distribution that generates the data. a learned function f can make a mistake if it contains any term t\\nthat is not in c. there are initially 2d many terms in f , and any (or\\nall!) of them might not be in c. we want to ensure that the probability\\nthat f makes an error is at most \\x01. it is sufﬁcient to ensure that for a term t (e.g., ¬x5), we say that t “negates” an example x if t(x) = 0. call a term t “bad” if (a) it does not appear in c and (b) has\\nprobability at least \\x01/2d of appearing (with respect to the unknown\\ndistribution d over data points). first, we show that if we have no bad terms left in f , then f has an error rate at most \\x01. we know that f contains at most 2d terms, since is begins with 2d terms and throws them out. the algorithm begins with 2d terms (one for each variable and one for each negated variable). note that f will only make one type\\nof error: it can call positive examples negative, but can never call a\\nnegative example positive. let c be the true concept (true boolean\\nformula) and call a term “bad” if it does not appear in c. a speciﬁc\\nbad term (e.g., ¬x5) will cause f to err only on positive examples\\nthat contain a corresponding bad value (e.g., x5 = 1). todo... ﬁnish\\nthis what we’ve shown in this theorem is that: if the true underly- ing concept is a boolean conjunction, and there is no noise, then the\\n“throw out bad terms” algorithm needs n ≤ . . . examples in order 160 a course in machine learning to learn a boolean conjunction that is (1 − δ)-likely to achieve an er-\\nror of at most \\x01. that is to say, that the sample complexity of “throw\\nout bad terms” is . . . . moreover, since the algorithm’s runtime is\\nlinear in n, it is an efﬁcient pac learning algorithm. ',\n",
       " ' the previous example of boolean conjunctions is mostly just a warm-\\nup exercise to understand pac-style proofs in a concrete setting.\\nin this section, you get to generalize the above argument to a much\\nlarger range of learning problems. we will still assume that there is\\nno noise, because it makes the analysis much simpler. (don’t worry:\\nnoise will be added eventually.) william of occam (c. 1288 – c. 1348) was an english friar and philosopher is is most famous for what later became known as oc-\\ncam’s razor and popularized by bertrand russell. the principle ba-\\nsically states that you should only assume as much as you need. or,\\nmore verbosely, “if one can explain a phenomenon without assuming\\nthis or that hypothetical entity, then there is no ground for assuming\\nit i.e. that one should always opt for an explanation in terms of the\\nfewest possible number of causes, factors, or variables.” what occam\\nactually wrote is the quote that began this chapter. in a machine learning context, a reasonable paraphrase is “simple\\nsolutions generalize well.” in other words, you have 10, 000 features\\nyou could be looking at. if you’re able to explain your predictions\\nusing just 5 of them, or using all 10, 000 of them, then you should just\\nuse the 5. the occam’s razor theorem states that this is a good idea, theo-\\nretically. it essentially states that if you are learning some unknown\\nconcept, and if you are able to ﬁt your training data perfectly, but you\\ndon’t need to resort to a huge class of possible functions to do so,\\nthen your learned function will generalize well. it’s an amazing theo-\\nrem, due partly to the simplicity of its proof. in some ways, the proof\\nis actually easier than the proof of the boolean conjunctions, though it\\nfollows the same basic argument. in order to state the theorem explicitly, you need to be able to think about a hypothesis class. this is the set of possible hypotheses\\nthat your algorithm searches through to ﬁnd the “best” one. in the\\ncase of the boolean conjunctions example, the hypothesis class, h,\\nis the set of all boolean formulae over d-many variables. in the case\\nof a perceptron, your hypothesis class is the set of all possible linear\\nclassiﬁers. the hypothesis class for boolean conjunctions is ﬁnite; the\\nhypothesis class for linear classiﬁers is inﬁnite. for occam’s razor, we\\ncan only work with ﬁnite hypothesis classes. learning theory 161 figure 12.1: thy:dt: picture of full\\ndecision tree theorem 15 (occam’s bound). suppose a is an algorithm that learns\\na function f from some ﬁnite hypothesis class h. suppose the learned\\nfunction always gets zero error on the training data. then, the sample com-\\nplexity of f is at most logh. todo comments proof of theorem 15. todo this theorem applies directly to the “throw out bad terms” algo-\\nrithm, since (a) the hypothesis class is ﬁnite and (b) the learned func-\\ntion always achieves zero error on the training data. to apply oc-\\ncam’s bound, you need only compute the size of the hypothesis class\\nh of boolean conjunctions. you can compute this by noticing that\\nthere are a total of 2d possible terms in any formula in h. moreover,\\neach term may or may not be in a formula. so there are 22d = 4d\\npossible formulae; thus, h = 4d. applying occam’s bound, we see\\nthat the sample complexity of this algorithm is n ≤ . . . . of course, occam’s bound is general enough to capture other\\nlearning algorithms as well. in particular, it can capture decision\\ntrees! in the no-noise setting, a decision tree will always ﬁt the train-\\ning data perfectly. the only remaining difﬁculty is to compute the\\nsize of the hypothesis class of a decision tree learner. for simplicity’s sake, suppose that our decision tree algorithm\\nalways learns complete trees: i.e., every branch from root to leaf\\nis length d. so the number of split points in the tree (i.e., places\\nwhere a feature is queried) is 2d−1. (see figure 12.1.) each split\\npoint needs to be assigned a feature: there d-many choices here.\\nthis gives d2d−1 trees. the last thing is that there are 2d leaves\\nof the tree, each of which can take two possible values, depending\\non whether this leaf is classiﬁed as +1 or −1: this is 2×2d = 2d+1\\npossibilities. putting this all togeter gives a total number of trees\\nh = d2d−12d+1 = d22d = d4d. applying occam’s bound, we see\\nthat todo examples is enough to learn a decision tree! ',\n",
       " ' occam’s bound is a fantastic result for learning over ﬁnite hypothesis\\nspaces. unfortunately, it is completely useless when h = ∞. this is\\nbecause the proof works by using each of the n training examples to\\n“throw out” bad hypotheses until only a small number are left. but if\\nh = ∞, and you’re throwing out a ﬁnite number at each step, there\\nwill always be an inﬁnite number remaining. this means that, if you want to establish sample complexity results\\nfor inﬁnite hypothesis spaces, you need some new way of measuring 162 a course in machine learning their “size” or “complexity.” a prototypical way of doing this is to\\nmeasure the complexity of a hypothesis class as the number of different\\nthings it can do. as a silly example, consider boolean conjunctions again. your input is a vector of binary features. however, instead of representing\\nyour hypothesis as a boolean conjunction, you choose to represent\\nit as a conjunction of inequalities. that is, instead of writing x1 ∧\\n¬x2 ∧ x5, you write [x1 > 0.2] ∧ [x2 < 0.77] ∧ [x5 < π/4]. in this\\nrepresentation, for each feature, you need to choose an inequality\\n(< or >) and a threshold. since the thresholds can be arbitrary real\\nvalues, there are now inﬁnitely many possibilities: h = 2d×∞ = ∞.\\nhowever, you can immediately recognize that on binary features,\\nthere really is no difference between [x2 < 0.77] and [x2 < 0.12] and\\nany other number of inﬁnitely many possibilities. in other words,\\neven though there are inﬁnitely many hypotheses, there are only ﬁnitely\\nmany behaviors. the vapnik-chernovenkis dimension (or vc dimension) is a classic measure of complexity of inﬁnite hypothesis classes based on\\nthis intuition3. the vc dimension is a very classiﬁcation-oriented no-\\ntion of complexity. the idea is to look at a ﬁnite set of unlabeled ex-\\namples, such as those in figure 12.2. the question is: no matter how\\nthese points were labeled, would we be able to ﬁnd a hypothesis that\\ncorrectly classiﬁes them. the idea is that as you add more points,\\nbeing able to represent an arbitrary labeling becomes harder and\\nharder. for instance, regardless of how the three points are labeled,\\nyou can ﬁnd a linear classiﬁer that agrees with that classiﬁcation.\\nhowever, for the four points, there exists a labeling for which you\\ncannot ﬁnd a perfect classiﬁer. the vc dimension is the maximum\\nnumber of points for which you can always ﬁnd such a classiﬁer.\\nyou can think of vc dimension as a game between you and an adversary. to play this game, you choose k unlabeled points however\\nyou want. then your adversary looks at those k points and assigns\\nbinary labels to them them however they want. you must then ﬁnd\\na hypothesis (classiﬁer) that agrees with their labeling. you win if\\nyou can ﬁnd such a hypothesis; they win if you cannot. the vc\\ndimension of your hypothesis class is the maximum number of points\\nk so that you can always win this game. this leads to the following\\nformal deﬁnition, where you can interpret there exists as your move\\nand for all as adversary’s move. deﬁnitions 2. for data drawn from some space x , the vc dimension of\\na hypothesis space h over x is the maximal k such that: there exists a set\\nx ⊆ x of size x = k, such that for all binary labelings of x, there exists\\na function f ∈ h that matches this labeling. figure 12.2: thy:vcex: ﬁgure with three\\nand four examples\\n3 yes, this is the same vapnik who\\nis credited with the creation of the\\nsupport vector machine. ? what is that labeling? what is it’s\\nname? learning theory 163 in general, it is much easier to show that the vc dimension is at\\nleast some value; it is much harder to show that it is at most some\\nvalue. for example, following on the example from figure 12.2, the\\nimage of three points (plus a little argumentation) is enough to show\\nthat the vc dimension of linear classiﬁers in two dimension is at least\\nthree. to show that the vc dimension is exactly three it sufﬁces to show\\nthat you cannot ﬁnd a set of four points such that you win this game\\nagainst the adversary. this is much more difﬁcult. in the proof that\\nthe vc dimension is at least three, you simply need to provide an\\nexample of three points, and then work through the small number of\\npossible labelings of that data. to show that it is at most three, you\\nneed to argue that no matter what set of four point you pick, you\\ncannot win the game. 12.7 further reading todo learning objectives:\\n• implement bagging and explain how it reduces variance in a predictor.\\n• explain the difference between a weak learner and a strong learner. • derive the adaboost algorithm.\\n• understand the relationship between\\nboosting decision stumps and linear\\nclassiﬁcation. dependencies: this is the central illusion in life: that randomness is a risk, that it\\nis a bad thing. . . – nassim nicholas taleb groups of people can often make better decisions than\\nindividuals, especially when group members each come in with\\ntheir own biases. the same is true in machine learning. ensemble\\nmethods are learning models that achieve performance by combining\\nthe opinions of multiple learners. in doing so, you can often get away\\nwith using much simpler learners and still achieve great performance.\\nmoreover, ensembles are inherantly parallel, which can make them\\nmuch more efﬁcient at training and test time, if you have access to\\nmultiple processors. in this chapter, you will learn about various ways of combining\\nbase learners into ensembles. one of the shocking results we will\\nsee is that you can take a learning model that only ever does slightly\\nbetter than chance, and turn it into an arbitrarily good learning\\nmodel, though a technique known as boosting. you will also learn\\nhow ensembles can decrease the variance of predictors as well as\\nperform regularization. ',\n",
       " ' all of the learning algorithms you have seen so far are deterministic.\\nif you train a decision tree multiple times on the same data set, you\\nwill always get the same tree back. in order to get an effect out of\\nvoting multiple classiﬁers, they need to differ. there are two primary\\nways to get variability. you can either change the learning algorithm\\nor change the data set. building an emsemble by training different classiﬁers is the most straightforward approach. as in single-model learning, you are given\\na data set (say, for classiﬁcation). instead of learning a single classiﬁer\\n(e.g., a decision tree) on this data set, you learn multiple different\\nclassiﬁers. for instance, you might train a decision tree, a perceptron,\\na knn, and multiple neural networks with different architectures.\\ncall these classiﬁers f1, . . . , fm. at test time, you can make a predic-\\ntion by voting. on a test example ˆx, you compute ˆy1 = f1( ˆx), . . . , ensemble methods 165 ? which of the classiﬁers you’ve\\nlearned about so far have high\\nvariance? figure 13.1: picture of sampling with\\nreplacement\\n1 to sample with replacement, imagine\\nputting all items from d in a hat. to\\ndraw a single sample, pick an element\\nat random from that hat, write it down,\\nand then put it back. ˆym = fm( ˆx). if there are more +1s in the list (cid:104)y1, . . . , ym then you\\npredict +1; otherwise you predict −1. the main advantage of ensembles of different classiﬁers is that it\\nis unlikely that all classiﬁers will make the same mistake. in fact, as\\nlong as every error is made by a minority of the classiﬁers, you will\\nachieve optimal classiﬁcation! unfortunately, the inductive biases of\\ndifferent learning algorithms are highly correlated. this means that\\ndifferent algorithms are prone to similar types of errors. in particular,\\nensembles tend to reduce the variance of classiﬁers. so if you have\\na classiﬁcation algorithm that tends to be very sensitive to small\\nchanges in the training data, ensembles are likely to be useful. note that the voting scheme naturally extends to multiclass clas-\\nsiﬁcation. however, it does not make sense in the contexts of regres-\\nsion, ranking or collective classiﬁcation. this is because you will\\nrarely see the same exact output predicted twice by two different\\nregression models (or ranking models or collective classiﬁcation mod-\\nels). for regression, a simple solution is to take the mean or median\\nprediction from the different models. for ranking and collective clas-\\nsiﬁcation, different approaches are required. instead of training different types of classiﬁers on the same data set, you can train a single type of classiﬁer (e.g., decision tree) on\\nmultiple data sets. the question is: where do these multiple data sets\\ncome from, since you’re only given one at training time? one option is to fragment your original data set. for instance, you\\ncould break it into 10 pieces and build decision trees on each of these\\npieces individually. unfortunately, this means that each decision tree\\nis trained on only a very small part of the entire data set and is likely\\nto perform poorly. a better solution is to use bootstrap resampling. this is a tech-\\nnique from the statistics literature based on the following observa-\\ntion. the data set we are given, d, is a sample drawn i.i.d. from an\\nunknown distribution d. if we draw a new data set ˜d by random\\nsampling from d with replacement1, then ˜d is also a sample from d.\\nfigure 13.1 shows the process of bootstrap resampling of ten objects.\\napplying this idea to ensemble methods yields a technique known as bagging. you start with a single data set d that contains n train-\\ning examples. from this single data set, you create m-many “boot-\\nstrapped training sets” ˜d1, . . . ˜dm. each of these bootstrapped sets\\nalso contains n training examples, drawn randomly from d with\\nreplacement. you can then train a decision tree (or other model)\\nseperately on each of these data sets to obtain classiﬁers f1, . . . , fm.\\nas before, you can use these classiﬁers to vote on new test points. note that the bootstrapped data sets will be similar. however, they\\nwill not be too similar. for example, if n is large then the number of figure 13.2: graph depicting overﬁtting\\nusing regularization versus bagging 166 a course in machine learning examples that are not present in any particular bootstrapped sample\\nis relatively large. the probability that the ﬁrst training example is\\nnot selected once is (1 − 1/n). the probability that it is not selected\\nat all is (1 − 1/n)n. as n → ∞, this tends to 1/e ≈ 0.3679. (already\\nfor n = 1000 this is correct to four decimal points.) so only about\\n63% of the original training examples will be represented in any\\ngiven bootstrapped set. since bagging tends to reduce variance, it provides an alternative\\napproach to regularization. that is, even if each of the learned clas-\\nsiﬁers f1, . . . , fm are individually overﬁt, they are likely to be overﬁt\\nto different things. through voting, you are able to overcome a sig-\\nniﬁcant portion of this overﬁtting. figure 13.2 shows this effect by\\ncomparing regularization via hyperparameters to regularization via\\nbagging. ',\n",
       " ' boosting is the process of taking a crummy learning algorithm (tech-\\nnically called a weak learner) and turning it into a great learning\\nalgorithm (technically, a strong learner). of all the ideas that origi-\\nnated in the theoretical machine learning community, boosting has\\nhad—perhaps—the greatest practical impact. the idea of boosting\\nis reminiscent of what you (like me!) might have thought when you\\nﬁrst learned about ﬁle compression. if i compress a ﬁle, and then\\nre-compress it, and then re-compress it, eventually i’ll end up with a\\nﬁnal that’s only one byte in size!\\nto be more formal, let’s deﬁne a strong learning algorithm l as\\nfollows. when given a desired error rate \\x01, a failure probability δ\\nand access to “enough” labeled examples from some distribution d,\\nthen, with high probability (at least 1 − δ), l learns a classiﬁer f that\\nhas error at most \\x01. this is precisely the deﬁnition of pac learning\\nthat you learned about in chapter 12. building a strong learning\\nalgorithm might be difﬁcult. we can as if, instead, it is possible to\\nbuild a weak learning algorithm w that only has to achieve an error\\nrate of 49%, rather than some arbitrary user-deﬁned parameter \\x01.\\n(49% is arbitrary: anything strictly less than 50% would be ﬁne.)\\nwork for taking a weak learning algorithm w and turning it into a\\nstrong learning algorithm. the particular boosting algorithm dis-\\ncussed here is adaboost, short for “adaptive boosting algorithm.”\\nadaboost is famous because it was one of the ﬁrst practical boosting\\nalgorithms: it runs in polynomial time and does not require you to\\ndeﬁne a large number of hyperparameters. it gets its name from the\\nlatter beneﬁt: it automatically adapts to the data that you give it. boosting is more of a “framework” than an algorithm. it’s a frame- ensemble methods 167 ? what happens if the weak learn-\\ning assumption is violated and ˆ\\x01 is\\nequal to 50%? what if it is worse\\nthan 50%? what does this mean, in\\npractice? n , 1 algorithm 32 adaboost(w, d, k)\\n1: d(0) ← (cid:104) 1\\n2: for k = 1 . . . k do\\n3: n(cid:105)\\nn , . . . , 1\\nf (k) ← w (d, d(k-1))\\nˆyn ← f (k)(xn), ∀n\\n(cid:17)\\n(cid:16) 1− ˆ\\x01(k)\\nˆ\\x01(k) ← ∑n d(k-1)\\n[yn (cid:54)= ˆyn]\\nα(k) ← 1\\nexp[−α(k)yn ˆyn], ∀n\\nn ← 1\\nd(k) n\\n2 log\\nz d(k-1)\\nn 4: 5: 6: ˆ\\x01(k) 7:\\n8: end for 9: return f (ˆx) = sgn(cid:2)∑k α(k) f (k)(ˆx)(cid:3) // initialize uniform importance to each example // train kth classiﬁer on weighted data\\n// make predictions on training data\\n// compute weighted training error\\n// compute “adaptive” parameter // re-weight examples and normalize // return (weighted) voted classiﬁer the intuition behind adaboost is like studying for an exam by using a past exam. you take the past exam and grade yourself. the\\nquestions that you got right, you pay less attention to. those that you\\ngot wrong, you study more. then you take the exam again and repeat\\nthis process. you continually down-weight the importance of questions\\nyou routinely answer correctly and up-weight the importance of ques-\\ntions you routinely answer incorrectly. after going over the exam\\nmultiple times, you hope to have mastered everything. the precise adaboost training algorithm is shown in algorithm 13.2. the basic functioning of the algorithm is to maintain a weight dis-\\ntribution d, over data points. a weak learner, f (k) is trained on this\\nweighted data. (note that we implicitly assume that our weak learner\\ncan accept weighted training data, a relatively mild assumption that\\nis nearly always true.) the (weighted) error rate of f (k) is used to de-\\ntermine the adaptive parameter α, which controls how “important” f (k)\\nis. as long as the weak learner does, indeed, achieve < 50% error,\\nthen α will be greater than zero. as the error drops to zero, α grows\\nwithout bound. after the adaptive parameter is computed, the weight distibution\\nis updated for the next iteration. as desired, examples that are cor-\\nrectly classiﬁed (for which yn ˆyn = +1) have their weight decreased\\nmultiplicatively. examples that are incorrectly classiﬁed (yn ˆyn = −1)\\nhave their weight increased multiplicatively. the z term is a nom-\\nralization constant to ensure that the sum of d is one (i.e., d can be\\ninterpreted as a distribution). the ﬁnal classiﬁer returned by ad-\\naboost is a weighted vote of the individual classiﬁers, with weights\\ngiven by the adaptive parameters. to better understand why α is deﬁned as it is, suppose that our weak learner simply returns a constant function that returns the\\n(weighted) majority class. so if the total weight of positive exam-\\nples exceeds that of negative examples, f (x) = +1 for all x; otherwise\\nf (x) = −1 for all x. to make the problem moderately interesting,\\nsuppose that in the original training set, there are 80 positive ex- 168 a course in machine learning 2 log 4] = 1 2 log 4] = 2. we can compute z = 80×1 amples and 20 negative examples. in this case, f (1)(x) = +1. it’s\\nweighted error rate will be ˆ\\x01(1) = 0.2 because it gets every negative\\nexample wrong. computing, we get α(1) = 1\\n2 log 4. before normaliza-\\ntion, we get the new weight for each positive (correct) example to be\\n1 exp[− 1\\n2. the weight for each negative (incorrect) example\\n2 + 20×2 = 80.\\nbecomes 1 exp[ 1\\ntherefore, after normalization, the weight distribution on any single\\npositive example is 1\\n160 and the weight on any negative example is 1\\n40.\\nhowever, since there are 80 positive examples and 20 negative exam-\\nples, the cumulative weight on all positive examples is 80× 1\\n160 = 1\\n2;\\nthe cumulative weight on all negative examples is 20× 1\\n40 = 1\\n2. thus,\\nafter a single boosting iteration, the data has become precisely evenly\\nweighted. this guarantees that in the next iteration, our weak learner\\nmust do something more interesting than majority voting if it is to\\nachieve an error rate less than 50%, as required. one of the major attractions of boosting is that it is perhaps easy\\nto design computationally efﬁcient weak learners. a very popular\\ntype of weak learner is a shallow decision tree: a decision tree with a\\nsmall depth limit. figure 13.3 shows test error rates for decision trees\\nof different maximum depths (the different curves) run for differing\\nnumbers of boosting iterations (the x-axis). as you can see, if you\\nare willing to boost for many iterations, very shallow trees are quite\\neffective. in fact, a very popular weak learner is a decision decision stump:\\na decision tree that can only ask one question. this may seem like a\\nsilly model (and, in fact, it is on it’s own), but when combined with\\nboosting, it becomes very effective. to understand why, suppose for\\na moment that our data consists only of binary features, so that any\\nquestion that a decision tree might ask is of the form “is feature 5\\non?” by concentrating on decision stumps, all weak functions must\\nhave the form f (x) = s(2xd − 1), where s ∈ {±1} and d indexes some\\nfeature. now, consider the ﬁnal form of a function learned by adaboost. ? this example uses concrete num-\\nbers, but the same result holds no\\nmatter what the data distribution\\nlooks like nor how many examples\\nthere are. write out the general case\\nto see that you will still arrive at an\\neven weighting after one iteration. we can expand it as follow, where we let fk denote the single feature\\nselected by the kth decision stump and let sk denote its sign: (cid:35) (cid:35) (cid:35) f (x) = sgn = sgn αk f (k)(x) αksk(2x fk − 1) 2αkskx fk − ∑ αksk k = sgn\\n= sgn [w · x + b] (cid:34)\\n(cid:34)\\n(cid:34) ∑\\nk\\n∑\\nk\\n∑\\nk figure 13.3: perf comparison of depth\\nvs # boost\\n? why do the functions have this\\nform? (13.1) (13.2) (13.3) (13.4) ensemble methods 169 algorithm 33 randomforesttrain(d, depth, k)\\n1: for k = 1 . . . k do\\n2: t(k) ← complete binary tree of depth depth with random feature splits\\nf (k) ← the function computed by t(k), with leaves ﬁlled in by d 3:\\n4: end for 5: return f (ˆx) = sgn(cid:2)∑k f (k)(ˆx)(cid:3) // return voted classiﬁer where wd = ∑\\nk: fk=d 2αksk and b = − ∑\\nk αksk (13.5) thus, when working with decision stumps, adaboost actually pro-\\nvides an algorithm for learning linear classiﬁers! in fact, this con-\\nnection has recently been strengthened: you can show that adaboost\\nprovides an algorithm for optimizing exponential loss. (however,\\nthis connection is beyond the scope of this book.) as a further example, consider the case of boosting a linear classi-\\nﬁer. in this case, if we let the kth weak classiﬁer be parameterized by\\nw(k) and b(k), the overall predictor will have the form: (13.6) (cid:34) (cid:16) w(k) · x + b(k)(cid:17)(cid:35) f (x) = sgn ∑\\nk αksgn you can notice that this is nothing but a two-layer neural network,\\nwith k-many hidden units! of course it’s not a classiﬁcally trained\\nneural network (once you learn w(k) you never go back and update\\nit), but the structure is identical. ',\n",
       " ' one of the most computationally expensive aspects of ensembles of\\ndecision trees is training the decision trees. this is very fast for de-\\ncision stumps, but for deeper trees it can be prohibitively expensive.\\nthe expensive part is choosing the tree structure. once the tree struc-\\nture is chosen, it is very cheap to ﬁll in the leaves (i.e., the predictions\\nof the trees) using the training data. an efﬁcient and surprisingly effective alternative is to use trees\\nwith ﬁxed structures and random features. collections of trees are\\ncalled forests, and so classiﬁers built like this are called random\\nforests. the random forest training algorithm, shown in algo-\\nrithm 13.3 is quite short. it takes three arguments: the data, a desired\\ndepth of the decision trees, and a number k of total decision trees to\\nbuild. the algorithm generates each of the k trees independently, which makes it very easy to parallelize. for each trees, it constructs a full\\nbinary tree of depth depth. the features used at the branches of this 170 a course in machine learning tree are selected randomly, typically with replacement, meaning that\\nthe same feature can appear multiple times, even in one branch. the\\nleaves of this tree, where predictions are made, are ﬁlled in based on\\nthe training data. this last step is the only point at which the training\\ndata is used. the resulting classiﬁer is then just a voting of the k-\\nmany random trees. the most amazing thing about this approach is that it actually works remarkably well. it tends to work best when all of the features\\nare at least marginally relevant, since the number of features selected\\nfor any given tree is small. an intuitive reason that it works well\\nis the following. some of the trees will query on useless features.\\nthese trees will essentially make random predictions. but some\\nof the trees will happen to query on good features and will make\\ngood predictions (because the leaves are estimated based on the\\ntraining data). if you have enough trees, the random ones will wash\\nout as noise, and only the good trees will have an effect on the ﬁnal\\nclassiﬁcation. 13.4 further reading todo further reading learning objectives:\\n• understand and be able to imple-\\nment stochastic gradient descent\\nalgorithms. • compare and contrast small ver-\\nsus large batch sizes in stochastic\\noptimization. • derive subgradients for sparse regularizers. • implement feature hashing. dependencies: one essential object is to choose that arrangement which shall\\ntend to reduce to a minimum the time necessary for completing\\nthe calculation. – ada lovelace so far, our focus has been on models of learning and basic al-\\ngorithms for those models. we have not placed much emphasis on\\nhow to learn quickly. the basic techniques you learned about so far\\nare enough to get learning algorithms running on tens or hundreds\\nof thousands of examples. but if you want to build an algorithm for\\nweb page ranking, you will need to deal with millions or billions\\nof examples, in hundreds of thousands of dimensions. the basic\\napproaches you have seen so far are insufﬁcient to achieve such a\\nmassive scale. in this chapter, you will learn some techniques for scaling learning algorithms. this are useful even when you do not have billions of\\ntraining examples, because it’s always nice to have a program that\\nruns quickly. you will see techniques for speeding up both model\\ntraining and model prediction. the focus in this chapter is on linear\\nmodels (for simplicity), but most of what you will learn applies more\\ngenerally. ',\n",
       " '? everyone always wants fast algorithms. in the context of machine\\nlearning, this can mean many things. you might want fast training\\nalgorithms, or perhaps training algorithms that scale to very large\\ndata sets (for instance, ones that will not ﬁt in main memory). you\\nmight want training algorithms that can be easily parallelized. or,\\nyou might not care about training efﬁciency, since it is an ofﬂine\\nprocess, and only care about how quickly your learned functions can\\nmake classiﬁcation decisions. it is important to separate out these desires. if you care about efﬁciency at training time, then what you are really asking for are\\nmore efﬁcient learning algorithms. on the other hand, if you care\\nabout efﬁciency at test time, then you are asking for models that can\\nbe quickly evaluated. one issue that is not covered in this chapter is parallel learning. 172 a course in machine learning this is largely because it is currently not a well-understood area in\\nmachine learning. there are many aspects of parallelism that come\\ninto play, such as the speed of communication across the network,\\nwhether you have shared memory, etc. right now, this the general,\\npoor-man’s approach to parallelization, is to employ ensembles. ',\n",
       " ' during training of most learning algorithms, you consider the entire\\ndata set simultaneously. this is certainly true of gradient descent\\nalgorithms for regularized linear classiﬁers (recall algorithm 7.4), in\\nwhich you ﬁrst compute a gradient over the entire training data (for\\nsimplicity, consider the unbiased case): g = ∑\\nn ∇w(cid:96)(yn, w · xn) + λw (14.1) where (cid:96)(y, ˆy) is some loss function. then you update the weights by\\nw ← w − ηg. in this algorithm, in order to make a single update, you\\nhave to look at every training example. when there are billions of training examples, it is a bit silly to look at every one before doing anything. perhaps just on the basis of the\\nﬁrst few examples, you can already start learning something! stochastic optimization involves thinking of your training data\\nas a big distribution over examples. a draw from this distribution\\ncorresponds to picking some example (uniformly at random) from\\nyour data set. viewed this way, the optimization problem becomes a\\nstochastic optimization problem, because you are trying to optimize\\nsome function (say, a regularized linear classiﬁer) over a probability\\ndistribution. you can derive this intepretation directly as follows: w∗ = arg max w = arg max w = arg max w ∑\\nn ∑\\nn ∑\\nn = arg max w e (y,x)∼d (cid:21) r(w) (cid:96)(yn, w · xn) + r(w)\\n(cid:20)\\n(cid:20) 1 (cid:96)(yn, w · xn) + 1\\nn (cid:96)(yn, w · xn) +\\n(cid:20) (cid:96)(y, w · x) + n (cid:21)\\n(cid:21) 1\\nn2 r(w) 1\\nn r(w) deﬁnition (14.2) move r inside sum\\n(14.3) divide through by n\\n(14.4) write as expectation\\n(14.5)\\n(14.6) where d is the training data distribution given this framework, you have the following general form of an efficient learning 173 algorithm 34 stochasticgradientdescent(f, d, s, k, η1, . . . )\\n1: z(0) ← (cid:104)0, 0, . . . , 0(cid:105)\\n2: for k = 1 . . . k do\\n3: // initialize variable we are optimizing g(k) ← ∇zf (d(k))(cid:12)(cid:12)z(k-1) d(k) ← s-many random data points from d\\nz(k) ← z(k-1) − η(k)g(k) // compute gradient on sample\\n// take a step down the gradient 4: 5:\\n6: end for\\n7: return z(k) optimization problem:\\nζ [f (z, ζ)] min e z (14.7) in the example, ζ denotes the random choice of examples over the\\ndataset, z denotes the weight vector and f (w, ζ) denotes the loss on\\nthat example plus a fraction of the regularizer. stochastic optimization problems are formally harder than regu-\\nlar (deterministic) optimization problems because you do not even\\nget access to exact function values and gradients. the only access\\nyou have to the function f that you wish to optimize are noisy mea-\\nsurements, governed by the distribution over ζ. despite this lack of\\ninformation, you can still run a gradient-based algorithm, where you\\nsimply compute local gradients on a current sample of data. more precisely, you can draw a data point at random from your\\ndata set. this is analogous to drawing a single value ζ from its\\ndistribution. you can compute the gradient of f just at that point.\\nin this case of a 2-norm regularized linear model, this is simply\\ng = ∇w(cid:96)(y, w · x) + 1\\nn w, where (y, x) is the random point you\\nselected. given this estimate of the gradient (it’s an estimate because\\nit’s based on a single random draw), you can take a small gradient\\nstep w ← w − ηg. this is the stochastic gradient descent algorithm (sgd). in prac- tice, taking gradients with respect to a single data point might be\\ntoo myopic. in such cases, it is useful to use a small batch of data.\\nhere, you can draw 10 random examples from the training data\\nand compute a small gradient (estimate) based on those examples:\\ng = ∑10\\nn w, where you need to include 10\\ncounts of the regularizer. popular batch sizes are 1 (single points)\\nand 10. the generic sgd algorithm is depicted in algorithm 14.2,\\nwhich takes k-many steps over batches of s-many examples. m=1 ∇w(cid:96)(ym, w · xm) + 10 in stochastic gradient descent, it is imperative to choose good step\\nsizes. it is also very important that the steps get smaller over time at\\na reasonable slow rate. in particular, convergence can be guaranteed\\nfor learning rates of the form: η(k) = η0√\\nk , where η0 is a ﬁxed, initial\\nstep size, typically 0.01, 0.1 or 1 depending on how quickly you ex- 174 a course in machine learning pect the algorithm to converge. unfortunately, in comparisong to\\ngradient descent, stochastic gradient is quite sensitive to the selection\\nof a good learning rate. there is one more practical issues related to the use of sgd as a\\nlearning algorithm: do you really select a random point (or subset\\nof random points) at each step, or do you stream through the data\\nin order. the answer is akin to the answer of the same question for\\nthe perceptron algorithm (chapter 4). if you do not permute your\\ndata at all, very bad things can happen. if you do permute your data\\nonce and then do multiple passes over that same permutation, it\\nwill converge, but more slowly. in theory, you really should permute\\nevery iteration. if your data is small enough to ﬁt in memory, this\\nis not a big deal: you will only pay for cache misses. however, if\\nyour data is too large for memory and resides on a magnetic disk\\nthat has a slow seek time, randomly seeking to new data points for\\neach example is prohibitivly slow, and you will likely need to forgo\\npermuting the data. the speed hit in convergence speed will almost\\ncertainly be recovered by the speed gain in not having to seek on disk\\nroutinely. (note that the story is very different for solid state disks,\\non which random accesses really are quite efﬁcient.) ',\n",
       " ' for many learning algorithms, the test-time efﬁciency is governed\\nby how many features are used for prediction. this is one reason de-\\ncision trees tend to be among the fastest predictors: they only use a\\nsmall number of features. especially in cases where the actual com-\\nputation of these features is expensive, cutting down on the number\\nthat are used at test time can yield huge gains in efﬁciency. moreover,\\nthe amount of memory used to make predictions is also typically\\ngoverned by the number of features. (note: this is not true of kernel\\nmethods like support vector machines, in which the dominant cost is\\nthe number of support vectors.) furthermore, you may simply believe\\nthat your learning problem can be solved with a very small number\\nof features: this is a very reasonable form of inductive bias. this is the idea behind sparse models, and in particular, sparse\\nregularizers. one of the disadvantages of a 2-norm regularizer for\\nlinear models is that they tend to never produce weights that are\\nexactly zero. they get close to zero, but never hit it. to understand\\nwhy, as a weight wd approaches zero, its gradient also approaches\\nzero. thus, even if the weight should be zero, it will essentially never\\nget there because of the constantly shrinking gradient. this suggests that an alternative regularizer is required to yield a\\nsparse inductive bias. an ideal case would be the zero-norm regular- efficient learning 175 izer, which simply counts the number of non-zero values in a vector:\\nw0 = ∑d[wd (cid:54)= 0]. if you could minimize this regularizer, you\\nwould be explicitly minimizing the number of non-zero features. un-\\nfortunately, not only is the zero-norm non-convex, it’s also discrete.\\noptimizing it is np-hard.\\na reasonable middle-ground is the one-norm: w1 = ∑d wd.\\nit is indeed convex: in fact, it is the tighest (cid:96)p norm that is convex.\\nmoreover, its gradients do not go to zero as in the two-norm. just as\\nhinge-loss is the tightest convex upper bound on zero-one error, the\\none-norm is the tighest convex upper bound on the zero-norm. at this point, you should be content. you can take your subgradi-\\nent optimizer for arbitrary functions and plug in the one-norm as a\\nregularizer. the one-norm is surely non-differentiable at wd = 0, but\\nyou can simply choose any value in the range [−1, +1] as a subgradi-\\nent at that point. (you should choose zero.) unfortunately, this does not quite work the way you might expect. the issue is that the gradient might “overstep” zero and you will\\nnever end up with a solution that is particularly sparse. for example,\\nat the end of one gradient step, you might have w3 = 0.6. your\\ngradient might have g6 = 0.8 and your gradient step (assuming\\nη = 1) will update so that the new w3 = −0.2. in the subsequent\\niteration, you might have g6 = −0.3 and step to w3 = 0.1. this observation leads to the idea of trucated gradients. the idea is simple: if you have a gradient that would step you over wd = 0,\\nthen just set wd = 0. in the easy case when the learning rate is 1, this\\nmeans that if the sign of wd − gd is different than the sign of wd then\\nyou truncate the gradient step and simply set wd = 0. in other words,\\ngd should never be larger than wd once you incorporate learning\\nrates, you can express this as: \\uf8f1\\uf8f4\\uf8f2\\uf8f4\\uf8f3 gd gd\\n0 gd ← if wd > 0 and gd ≤ 1\\nif wd < 0 and gd ≥ 1\\notherwise η(k) wd\\nη(k) wd (14.8) this works quite well in the case of subgradient descent. it works\\nsomewhat less well in the case of stochastic subgradient descent. the\\nproblem that arises in the stochastic case is that wherever you choose\\nto stop optimizing, you will have just touched a single example (or\\nsmall batch of examples), which will increase the weights for a lot of\\nfeatures, before the regularizer “has time” to shrink them back down\\nto zero. you will still end up with somewhat sparse solutions, but not\\nas sparse as they could be. there are algorithms for dealing with this\\nsituation, but they all have a heuristic ﬂavor to them and are beyond\\nthe scope of this book. 176 a course in machine learning ',\n",
       " ' as much as speed is a bottleneck in prediction, so often is memory\\nusage. if you have a very large number of features, the amount of\\nmemory that it takes to store weights for all of them can become\\nprohibitive, especially if you wish to run your algorithm on small de-\\nvices. feature hashing is an incredibly simple technique for reducing\\nthe memory footprint of linear models, with very small sacriﬁces in\\naccuracy. the basic idea is to replace all of your features with hashed ver-\\nsions of those features, thus reducing your space from d-many fea-\\nture weights to p-many feature weights, where p is the range of\\nthe hash function. you can actually think of hashing as a (random-\\nized) feature mapping φ : rd → rp, for some p (cid:28) d. the idea\\nis as follows. first, you choose a hash function h whose domain is\\n[d] = {1, 2, . . . , d} and whose range is [p]. then, when you receive a\\nfeature vector x ∈ rd, you map it to a shorter feature vector ˆx ∈ rp.\\nalgorithmically, you can think of this mapping as follows: 1. initialize ˆx = (cid:104)0, 0, . . . , 0(cid:105) 2. for each d = 1 . . . d: (a) hash d to position p = h(d)\\n(b) update the pth position by adding xd: ˆxp ← ˆxp + xd 3. return ˆx mathematically, the mapping looks like: φ(x)p = ∑\\nd [h(d) = p]xd = ∑ d∈h−1(p) xd where h−1(p) = {d : h(d) = p}. (14.9) in the (unrealistic) case where p = d and h simply encodes a per- mutation, then this mapping does not change the learning problem\\nat all. all it does is rename all of the features. in practice, p (cid:28) d\\nand there will be collisions. in this context, a collision means that\\ntwo features, which are really different, end up looking the same to\\nthe learning algorithm. for instance, “is it sunny today?” and “did\\nmy favorite sports team win last night?” might get mapped to the\\nsame location after hashing. the hope is that the learning algorithm\\nis sufﬁciently robust to noise that it can handle this case well. consider the kernel deﬁned by this hash mapping. namely:\\nk(hash)(x, z) = φ(x) · φ(z) (cid:33) (cid:32) ∑\\nd,e = ∑\\np\\n= ∑\\np\\n= ∑\\nd e∈h−1(h(d)) = x · z + ∑\\nd xdze ∑\\ne(cid:54)=d, e∈h−1(h(d)) [h(d) = p]zd ∑\\nd [h(d) = p]xd ∑\\nd\\n[h(d) = p][h(e) = p]xdze\\n∑ (cid:33)(cid:32) xdze efficient learning 177 (14.10) (14.11) (14.12) (14.13) (14.14) this hash kernel has the form of a linear kernel plus a small number\\nof quadratic terms. the particular quadratic terms are exactly those\\ngiven by collisions of the hash function. there are two things to notice about this. the ﬁrst is that collisions might not actually be bad things! in a sense, they’re giving you a\\nlittle extra representational power. in particular, if the hash function\\nhappens to select out feature pairs that beneﬁt from being paired,\\nthen you now have a better representation. the second is that even if\\nthis doesn’t happen, the quadratic term in the kernel has only a small\\neffect on the overall prediction. in particular, if you assume that your\\nhash function is pairwise independent (a common assumption of\\nhash functions), then the expected value of this quadratic term is zero,\\nand its variance decreases at a rate of o(p−2). in other words, if you\\nchoose p ≈ 100, then the variance is on the order of 0.0001. 14.5 further reading todo further reading learning objectives:\\n• explain the difference between linear and non-linear dimensionality\\nreduction. • relate the view of pca as maximiz- ing variance with the view of it as\\nminimizing reconstruction error. • implement latent semantic analysis for text data. • motivate manifold learning from the perspective of reconstruction error.\\n• understand k-means clustering as distance minimization. • explain the importance of initial- ization in k-means and furthest-ﬁrst\\nheuristic. • implement agglomerative clustering.\\n• argue whether spectral cluster-\\ning is a clustering algorithm or a\\ndimensionality reduction algorithm. dependencies: [my father] advised me to sit every few months in my reading\\nchair for an entire evening, close my eyes and try to think of new\\nproblems to solve. i took his advice very seriously and have been\\nglad ever since that he did.\\n– luis walter alvarez if you have access to labeled training data, you know what\\nto do. this is the “supervised” setting, in which you have a teacher\\ntelling you the right answers. unfortunately, ﬁnding such a teacher\\nis often difﬁcult, expensive, or down right impossible. in those cases,\\nyou might still want to be able to analyze your data, even though you\\ndo not have labels. unsupervised learning is learning without a teacher. one basic thing that you might want to do with data is to visualize it. sadly, it\\nis difﬁcult to visualize things in more than two (or three) dimensions,\\nand most data is in hundreds of dimensions (or more). dimension-\\nality reduction is the problem of taking high dimensional data and\\nembedding it in a lower dimension space. another thing you might\\nwant to do is automatically derive a partitioning of the data into\\nclusters. you’ve already learned a basic approach for doing this: the\\nk-means algorithm (chapter 3). here you will analyze this algorithm\\nto see why it works. you will also learn more advanced clustering\\napproaches. ',\n",
       " ' the k-means clustering algorithm is re-presented in algorithm 15.1. there are two very basic questions about this algorithm: (1) does it\\nconverge (and if so, how quickly); (2) how sensitive it is to initializa-\\ntion? the answers to these questions, detailed below, are: (1) yes it\\nconverges, and it converges very quickly in practice (though slowly\\nin theory); (2) yes it is sensitive to initialization, but there are good\\nways to initialize it. consider the question of convergence. the following theorem states that the k-means algorithm converges, though it does not say\\nhow quickly it happens. the method of proving the convergence is\\nto specify a clustering quality objective function, and then to show\\nthat the k-means algorithm converges to a (local) optimum of that\\nobjective function. the particular objective function that k-means algorithm 35 k-means(d, k)\\n1: for k = 1 to k do\\nµk ← some random location\\n2:\\n3: end for\\n4: repeat\\n5: for n = 1 to n do zn ← argmink µk − xn end for\\nfor k = 1 to k do µk ← mean({ xn : zn = k }) 6: 7: 8: 9: end for 10:\\n11: until converged\\n12: return z unsupervised learning 179 // randomly initialize mean for kth cluster // assign example n to closest center // re-estimate mean of cluster k // return cluster assignments is optimizing is the sum of squared distances from any data point to its\\nassigned center. this is a natural generalization of the deﬁnition of a\\nmean: the mean of a set of points is the single point that minimizes\\nthe sum of squared distances from the mean to every point in the\\ndata. formally, the k-means objective is:\\n= ∑\\nk (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xn − µzn l(z, µ; d) = ∑\\nn xn − µk2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 ∑\\nn:zn=k (15.1) theorem 16 (k-means convergence theorem). for any dataset d and\\nany number of clusters k, the k-means algorithm converges in a ﬁnite num-\\nber of iterations, where convergence is measured by l ceasing the change.\\nproof of theorem 16. the proof works as follows. there are only two\\npoints in which the k-means algorithm changes the values of µ or z:\\nlines 6 and 9. we will show that both of these operations can never\\nincrease the value of l. assuming this is true, the rest of the argu-\\nment is as follows. after the ﬁrst pass through the data, there are\\nare only ﬁnitely many possible assignments to z and µ, because z is\\ndiscrete and because µ can only take on a ﬁnite number of values:\\nmeans of some subset of the data. furthermore, l is lower-bounded\\nby zero. together, this means that l cannot decrease more than a\\nﬁnite number of times. thus, it must stop decreasing at some point,\\nand at that point the algorithm has converged.\\nit remains to show that lines 6 and 9 decrease l. for line 6, when\\nlooking at example n, suppose that the previous value of zn is a and\\nthe new value is b. it must be the case that xn − µb ≤ xn − µb.\\nthus, changing from a to b can only decrease l. for line 9, consider\\nthe second form of l. line 9 computes µk as the mean of the data\\npoints for which zn = k, which is precisely the point that minimizes\\nsquared sitances. thus, this update to µk can only decrease l. there are several aspects of k-means that are unfortunate. first,\\nthe convergence is only to a local optimum of l. in practice, this 180 a course in machine learning means that you should usually run it 10 times with different initial-\\nizations and pick the one with minimal resulting l. second, one\\ncan show that there are input datasets and initializations on which\\nit might take an exponential amount of time to converge. fortu-\\nnately, these cases almost never happen in practice, and in fact it has\\nrecently been shown that (roughly) if you limit the ﬂoating point pre-\\ncision of your machine, k-means will converge in polynomial time\\n(though still only to a local optimum), using techniques of smoothed\\nanalysis. the biggest practical issue in k-means is initialization. if the clus-\\nter means are initialized poorly, you often get convergence to uninter-\\nesting solutions. a useful heuristic is the furthest-ﬁrst heuristic. this\\ngives a way to perform a semi-random initialization that attempts to\\npick initial means as far from each other as possible. the heuristic is\\nsketched below:\\n1. pick a random example m and set µ1 = xm.\\n2. for k = 2 . . . k: (a) find the example m that is as far as possible from all previ- ously selected means; namely: m = arg maxm mink(cid:48)<k xm − µk(cid:48)2\\nand set µk = xm in this heuristic, the only bit of randomness is the selection of the\\nﬁrst data point. after that, it is completely deterministic (except in\\nthe rare case that there are multiple equidistant points in step 2a). it\\nis extremely important that when selecting the 3rd mean, you select\\nthat point that maximizes the minimum distance to the closest other\\nmean. you want the point that’s as far away from all previous means\\nas possible. the furthest-ﬁrst heuristic is just that: a heuristic. it works very well in practice, though can be somewhat sensitive to outliers (which\\nwill often get selected as some of the initial means). however, this\\noutlier sensitivity is usually reduced after one iteration through the\\nk-means algorithm. despite being just a heuristic, it is quite useful in\\npractice. you can turn the heuristic into an algorithm by adding a bit more randomness. this is the idea of the k-means++ algorithm, which\\nis a simple randomized tweak on the furthest-ﬁrst heuristic. the\\nidea is that when you select the kth mean, instead of choosing the\\nabsolute furthest data point, you choose a data point at random, with\\nprobability proportional to its distance squared. this is made formal\\nin algorithm 15.1. if you use k-means++ as an initialization for k-means, then you\\nare able to achieve an approximation guarantee on the ﬁnal value unsupervised learning 181 algorithm 36 k-means++(d, k)\\n1: µ1 ← xm for m chosen uniformly at random // randomly initialize ﬁrst point\\n2: for k = 2 to k do\\n3: // compute distances\\n// normalize to probability distribution\\n// pick an example at random 4: 5: dn ← mink(cid:48)<k xn − µk(cid:48)2, ∀n\\np ← 1\\n∑n nd\\nm ← random sample from p\\nµk ← xm d 6:\\n7: end for\\n8: run k-means using µ as initial centers of the objective. this doesn’t tell you that you will reach the global\\noptimum, but it does tell you that you will get reasonably close. in\\nparticular, if ˆl is the value obtained by running k-means++, then this\\nwill not be “too far” from l(opt), the true global minimum.\\ntheorem 17 (k-means++ approximation guarantee). the expected\\nvalue of the objective returned by k-means++ is never more than o(log k)\\nfrom optimal and can be as close as o(1) from optimal. even in the former\\ncase, with 2k random restarts, one restart will be o(1) from optimal (with high probability). formally: e(cid:2) ˆl(cid:3) ≤ 8(log k + 2)l(opt). moreover, if the\\ndata is “well suited” for clustering, then e(cid:2) ˆl(cid:3) ≤ o(1)l(opt). the notion of “well suited” for clustering informally states that\\nthe advantage of going from k − 1 clusters to k clusters is “large.”\\nformally, it means that lk\\n(opt) is the\\noptimal value for clustering with k clusters, and \\x01 is the desired\\ndegree of approximation. the idea is that if this condition does not\\nhold, then you shouldn’t bother clustering the data. (opt) ≤ \\x012lk−1 (opt), where lk one of the biggest practical issues with k-means clustering is\\n“choosing k.” namely, if someone just hands you a dataset and\\nasks you to cluster it, how many clusters should you produce? this\\nis difﬁcult, because increasing k will always decrease lk\\n(opt) (until\\nk > n), and so simply using l as a notion of goodness is insufﬁ-\\ncient (analogous to overﬁtting in a supervised setting). a number\\nof “information criteria” have been proposed to try to address this\\nproblem. they all effectively boil down to “regularizing” k so that\\nthe model cannot grow to be too complicated. the two most popular\\nare the bayes information criteria (bic) and the akaike information\\ncriteria (aic), deﬁned below in the context of k-means: bic: aic: arg min k arg min k ˆlk + k log d\\nˆlk + 2kd (15.2)\\n(15.3) the informal intuition behind these criteria is that increasing k is\\ngoing to make lk go down. however, if it doesn’t go down “by\\nenough” then it’s not worth doing. in the case of bic, “by enough” 182 a course in machine learning means by an amount proportional to log d; in the case of aic, it’s\\nproportional to 2d. thus, aic provides a much stronger penalty for\\nmany clusters than does bic, especially in high dimensions. a more formal intuition for bic is the following. you ask yourself the question “if i wanted to send this data across a network, how\\nmany bits would i need to send?” clearly you could simply send\\nall of the n examples, each of which would take roughly log d bits\\nto send. this gives n log d to send all the data. alternatively, you\\ncould ﬁrst cluster the data and send the cluster centers. this will take\\nk log d bits. then, for each data point, you send its center as well as\\nits deviation from that center. it turns out this will cost exactly ˆlk\\nbits. therefore, the bic is precisely measuring how many bits it will\\ntake to send your data using k clusters. the k that minimizes this\\nnumber of bits is the optimal value. ',\n",
       " ' dimensionality reduction is the task of taking a dataset in high di-\\nmensions (say 10000) and reducing it to low dimensions (say 2) while\\nretaining the “important” characteristics of the data. since this is\\nan unsupervised setting, the notion of important characteristics is\\ndifﬁcult to deﬁne. consider the dataset in figure 15.1, which lives in high dimensions (two) and you want to reduce to low dimensions (one). in the case\\nof linear dimensionality reduction, the only thing you can do is to\\nproject the data onto a vector and use the projected distances as the\\nembeddings. figure 15.2 shows a projection of this data onto the\\nvector that points in the direction of maximal variance of the original\\ndataset. intuitively, this is a reasonable notion of importance, since\\nthis is the direction in which most information is encoded in the data. for the rest of this section, assume that the data is centered:\\nnamely, the mean of all the data is at the origin. (this will sim-\\nply make the math easier.) suppose the two dimensional data is\\nx1, . . . , xn and you’re looking for a vector u that points in the direc-\\ntion of maximal variance. you can compute this by projecting each\\npoint onto u and looking at the variance of the result. in order for\\nthe projection to make sense, you need to constrain u2 = 1. in\\nthis case, the projections are x1 · u, x2 · u, . . . , xn · u. call these values\\np1, . . . , pn.\\nthe goal is to compute the variance of the {pn}s and then choose\\nu to maximize this variance. to compute the variance, you ﬁrst need\\nto compute the mean. because the mean of the xns was zero, the figure 15.1: unsup:pcadata: data in\\ntwo dimensions figure 15.2: unsup:pcadata2: projection\\nof that data down to one dimension by\\npca math review eigenvalues and eigenvectors\\ntodo the usual... unsupervised learning 183 figure 15.3: mean of the ps is also zero. this can be seen as follows: (cid:32) (cid:33) ∑\\nn pn = ∑\\nn xn · u = ∑\\nn xn · u = 0 · u = 0 (15.4) the variance of the {pn} is then just ∑n p2\\nn. finding the optimal\\nu (from the perspective of variance maximization) reduces to the\\nfollowing optimization problem: max u (xn · u)2 ∑\\nn subj. to u2 = 1 (15.5) in this problem it becomes apparent why keeping u unit length is\\nimportant: if not, u would simply stretch to have inﬁnite length to\\nmaximize the objective.\\nit is now helpful to write the collection of datapoints xn as a n×\\nd matrix x. if you take this matrix x and multiply it by u, which\\nhas dimensions d×1, you end up with a n×1 vector whose values\\nare exactly the values p. the objective in eq (15.5) is then just the\\nsquared norm of p. this simpliﬁes eq (15.5) to: xu2 subj. to u2 − 1 = 0 max u (15.6) where the constraint has been rewritten to make it amenable to con-\\nstructing the lagrangian. doing so and taking gradients yields: (cid:17) l(u, λ) = xu2 − λ (cid:16)u2 − 1\\n(cid:17)\\n∇ul = 2x(cid:62)xu − 2λu\\n=⇒ λu =\\nu x(cid:62)x (cid:16) (15.7)\\n(15.8)\\n(15.9) you can solve this expression (λu = x(cid:62)xu) by computing the ﬁrst\\neigenvector and eigenvalue of the matrix x(cid:62)x. this gives you the solution to a projection into a one-dimensional space. to get a second dimension, you want to ﬁnd a new vector v on\\nwhich the data has maximal variance. however, to avoid redundancy,\\nyou want v to be orthogonal to u; namely u · v = 0. this gives: xv2 subj. to v2 = 1, and u · v = 0 (15.10) max v following the same procedure as before, you can construct a la- 184 a course in machine learning algorithm 37 pca(d, k)\\n1: µ ← mean(x) x − µ1(cid:62)(cid:17) (cid:62)(cid:16) 2: d ←(cid:16) x − µ1(cid:62)(cid:17) 3: {λk, uk} ← top k eigenvalues/eigenvectors of d\\n4: return (x − µ1) u // compute data mean for centering\\n// compute covariance, 1 is a vector of ones grangian and differentiate: l(v, λ1, λ2) = xv2 − λ1 (cid:17) − λ2u · v (cid:16)v2 − 1\\n(cid:17) ∇ul = 2x(cid:62)xv − 2λ1v − λ2u\\nv − λ2\\n=⇒ λ1v =\\nu\\n2 x(cid:62)x (cid:16) // project data using u (15.11)\\n(15.12)\\n(15.13) however, you know that u is the ﬁrst eigenvector of x(cid:62)x, so the\\nsolution to this problem for λ1 and v is given by the second eigen-\\nvalue/eigenvector pair of x(cid:62)x. repeating this analysis inductively tells you that if you want to project onto k mutually orthogonal dimensions, you simply need to\\ntake the ﬁrst k eigenvectors of the matrix x(cid:62)x. this matrix is often\\ncalled the data covariance matrix because [x(cid:62)x]i,j = ∑n ∑m xn,ixm,j,\\nwhich is the sample covariance between features i and j. this leads to the technique of principle components analysis,\\nor pca. for completeness, the is depicted in algorithm 15.2. the\\nimportant thing to note is that the eigenanalysis only gives you the\\nprojection directions. it does not give you the embedded data. to\\nembed a data point x you need to compute its embedding as (cid:104)x ·\\nu1, x · u2, . . . , x · uk(cid:105). if you write u for the d×k matrix of us, then this\\nis just xu. there is an alternative derivation of pca that can be informative, based on reconstruction error. consider the one-dimensional case\\nagain, where you are looking for a single projection direction u. if\\nyou were to use this direction, your projected data would be z = xu.\\neach zn gives the position of the nth datapoint along u. you can\\nproject this one-dimensional data back into the original space by\\nmultiplying it by u(cid:62). this gives you reconstructed values zu(cid:62). instead\\nof maximizing variance, you might instead want to minimize the\\nreconstruction error, deﬁned by: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x − zu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x − xuu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xuu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 − 2x(cid:62)xuu(cid:62) = x2 + deﬁnition of z (15.14) quadratic rule (15.15) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xuu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 − 2u(cid:62)x(cid:62)xu = x2 + = x2 + x2 − 2u(cid:62)x(cid:62)xu unsupervised learning 185 quadratic rule (15.16) u is a unit vector (15.17) = c − 2xu2 join constants, rewrite last term (15.18)\\nminimizing this ﬁnal term is equivalent to maximizing xu2, which\\nis exactly the form of the maximum variance derivation of pca.\\nthus, you can see that maximizing variance is identical to minimiz-\\ning reconstruction error. the same question of “what should k be” arises in dimension-\\nality reduction as in clustering. if the purpose of dimensionality\\nreduction is to visualize, then k should be 2 or 3. however, an alter-\\nnative purpose of dimensionality reduction is to avoid the curse of\\ndimensionality. for instance, even if you have labeled data, it might\\nbe worthwhile to reduce the dimensionality before applying super-\\nvised learning, essentially as a form of regularization. in this case,\\nthe question of an optimal k comes up again. in this case, the same\\ncriteria (aic and bic) that can be used for clustering can be used for\\npca. the only difference is the quality measure changes from a sum\\nof squared distances to means (for clustering) to a sum of squared\\ndistances to original data points (for pca). in particular, for bic you\\nget the reconstruction error plus k log d; for aic, you get the recon-\\nstruction error plus 2kd. ',\n",
       " ' todo 15.4 further reading todo learning objectives:\\n• explain the relationship between\\nparameters and hidden variables. • construct generative stories for clustering and dimensionality\\nreduction. • draw a graph explaining how em works by constructing convex lower\\nbounds. • implement em for clustering with mixtures of gaussians, and contrast-\\ning it with k-means. • evaluate the differences betweem em and gradient descent for hidden\\nvariable models. dependencies: a hen is only an egg’s way of making another egg. – samuel butler suppose you were building a naive bayes model for a text cate-\\ngorization problem. after you were done, your boss told you that it\\nbecame prohibitively expensive to obtain labeled data. you now have\\na probabilistic model that assumes access to labels, but you don’t\\nhave any labels! can you still do something? amazingly, you can. you can treat the labels as hidden variables,\\nand attempt to learn them at the same time as you learn the param-\\neters of your model. a very broad family of algorithms for solving\\nproblems just like this is the expectation maximization family. in this\\nchapter, you will derive expectation maximization (em) algorithms\\nfor clustering and dimensionality reduction, and then see why em\\nworks. ',\n",
       " ' alice’s machine learning professor carlos gives out an exam that\\nconsists of 50 true/false questions. alice’s class of 100 students takes\\nthe exam and carlos goes to grade their solutions. if carlos made\\nan answer key, this would be easy: he would just count the fraction\\nof correctly answered questions each student got, and that would be\\ntheir score. but, like many professors, carlos was really busy and\\ndidn’t have time to make an answer key. can he still grade the exam? there are two insights that suggest that he might be able to. sup-\\npose he know ahead of time that alice was an awesome student, and\\nis basically guaranteed to get 100% on the exam. in that case, carlos\\ncan simply use alice’s answers as the ground truth. more generally,\\nif carlos assumes that on average students are better than random\\nguessing, he can hope that the majority answer for each question is\\nlikely to be correct. combining this with the previous insight, when\\ndoing the “voting”, he might want to pay more attention to the an-\\nswers of the better students. to be a bit more pedantic, suppose there are n = 100 students and m = 50 questions. each student n has a score sn, between 0 and expectation maximization 187 1 that denotes how well they do on the exam. the score is what we\\nreally want to compute. for each question m and each student n, the\\nstudent has provided an answer an,m, which is either zero or one.\\nthere is also an unknown ground truth answer for each question m,\\nwhich we’ll call tm, which is also either zero or one. as a starting point, let’s consider a simple heuristic and then com-\\nplexify it. the heuristic is the “majority vote” heuristic and works as\\nfollows. first, we estimate tm as the most common answer for ques-\\ntion m: tm = argmaxt ∑n 1[an,m = t]. once we have a guess for each\\ntrue answer, we estimate each students’ score as how many answers\\nm ∑m 1[an,m = tm].\\nthey produced that match this guessed key: sn = 1\\nonce we have these scores, however, we might want to trust some of the students more than others. in particular, answers from stu-\\ndents with high scores are perhaps more likely to be correct, so we\\ncan recompute the ground truth, according to weighted votes. the\\nweight of the votes will be precisely the score the corresponding each\\nstudent: tm = argmax t ∑\\nn sn1[an,m = t] (16.1) you can recognize this as a chicken and egg problem. if you knew the\\nstudent’s scores, you could estimate an answer key. if you had an\\nanswer key, you could compute student scores. a very common\\nstrategy in computer science for dealing with such chicken and egg\\nproblems is to iterate. take a guess at the ﬁrst, compute the second,\\nrecompute the ﬁrst, and so on. in order to develop this idea formally, we have to case the prob-\\nlem in terms of a probabilistic model with a generative story. the\\ngenerative story we’ll use is:\\n1. for each question m, choose a true answer tm ∼ ber(0.5)\\n2. for each student n, choose a score sn ∼ uni(0, 1)\\n3. for each question m and each student n, choose an answer an,m ∼ ber(sn)tmber(1 − sn)1−tm\\nin the ﬁrst step, we generate the true answers independently by\\nﬂipping a fair coin. in the second step, each students’ overall score\\nis determined to be a uniform random number between zero and\\none. the tricky step is step three, where each students’ answer is\\ngenerated for each question. consider student n answering question\\nm, and suppose that sn = 0.9. if tm = 1, then an,m should be 1 (i.e.,\\ncorrect) 90% of the time; this can be accomplished by drawing the an-\\nswer from ber(0.9). on the other hand, if tm = 0, then an,m should 1\\n(i.e., incorrect) 10% of the time; this can be accomplished by drawing 188 a course in machine learning the answer from ber(0.1). the exponent in step 3 selects which of two\\nbernoulli distributions to draw from, and then implements this rule. this can be translated into the following likelihood: (cid:35) (cid:34) (cid:35) 0.5tm0.51−tm × ∏\\nn 1 (cid:34) = p(a, t, s)\\n∏\\nm\\n× (cid:34) (1 − sn)(1−an,m)tm (1 − sn)an,m(1−tm)(cid:105) san,mtm\\nn ∏\\n∏\\nn\\nm\\ns(1−an,m)(1−tm)\\nn\\n∏\\n= 0.5m∏\\nm\\nn san,mtm\\nn (1 − sn)(1−an,m)tms(1−an,m)(1−tm) n (16.2)\\n(1 − sn)an,m(1−tm)\\n(16.3) suppose we knew the true lables t. we can take the log of this\\nlikelihood and differentiate it with respect to the score sn of some\\nstudent (note: we can drop the 0.5m term because it is just a con-\\nstant): (cid:104) log p(a, t, s) = ∑\\nn ∑\\nm an,mtm log sn + (1 − an,m)(1 − tm) log(sn)\\n+ (1 − an,m)tm log(1 − sn) + an,m(1 − tm) log(1 − sn)\\n(16.4) (cid:104) an,mtm + (1 − an,m)(1 − tm) (cid:105) − (1 − an,m)tm + an,m(1 − tm) (cid:105) ∂ log p(a, t, s) ∂sn = ∑\\nm sn 1 − sn\\n(16.5) (16.6) (16.7) (16.8) (16.9) the derivative has the form a\\nsn\\nsolve for sn, we get an optimum of sn = a − b 1−sn . if we set this equal to zero and a+b . in this case: (cid:2)an,mtm + (1 − an,m)(1 − tm)(cid:3)\\n(cid:2)(1 − an,m)tm + an,m(1 − tm)(cid:3)\\n(cid:2)1(cid:3) = m\\n(cid:2)an,mtm + (1 − an,m)(1 − tm)(cid:3) a = ∑\\nm\\nb = ∑\\nm\\na + b = ∑\\nm sn = 1\\nm ∑\\nm putting this together, we get: in the case of known ts, this matches exactly what we had in the\\nheuristic. however, we do not know t, so instead of using the “true” val-\\nues of t, we’re going to use their expectations. in particular, we will\\ncompute sn by maximizing its likelihood under the expected values expectation maximization 189 of t, hence the name expectation maximization. if we are going\\nto compute expectations of t, we have to say: expectations accord-\\ning to which probability distribution? we will use the distribution\\np(tm a, s). let ˜tm denote etm∼p(tm a,s)[tm]. because tm is a bi-\\nnary variable, its expectation is equal to it’s probability; namely:\\n˜tm = p(tm a, s). how can we compute this? we will compute c = p(tm = 1, a, s)\\nand d = p(tm = 0, a, s) and then compute ˜tm = c/(c + d). the\\ncomputation is straightforward: c = 0.5 ∏\\nn\\nd = 0.5 ∏\\nn san,m\\nn s1−an,m n (1 − sn)1−an,m = 0.5 ∏\\nan,m=1\\n(1 − sn)an,m = 0.5 ∏\\nan,m=1 n: n: (1 − sn) (16.10) sn (16.11) sn ∏\\nn:\\nan,m=0 (1 − sn) ∏\\nan,m=0 n: if you inspect the value of c, it is basically “voting” (in a product\\nform, not a sum form) the scores of those students who agree that the\\nanswer is 1 with one-minus-the-score of those students who do not.\\nthe value of d is doing the reverse. this is a form of multiplicative\\nvoting, which has the effect that if a given student has a perfect score\\nof 1.0, their results will carry the vote completely. we now have a way to: 1. compute expected ground truth values ˜tm, given scores.\\n2. optimize scores sn given expected ground truth values. the full solution is then to alternate between these two. you can\\nstart by initializing the ground truth values at the majority vote (this\\nseems like a safe initialization). given those, compute new scores.\\ngiven those new scores, compute new ground truth values. and\\nrepeat until tired. in the next two sections, we will consider a more complex unsu-\\npervised learning model for clustering, and then a generic mathe-\\nmatical framework for expectation maximization, which will answer\\nquestions like: will this process converge, and, if so, to what? ',\n",
       " ' in chapter 9, you learned about probabilitic models for classiﬁcation\\nbased on density estimation. let’s start with a fairly simple classiﬁca-\\ntion model that assumes we have labeled data. we will shortly remove\\nthis assumption. our model will state that we have k classes, and\\ndata from class k is drawn from a gaussian with mean µk and vari-\\nance σ2\\nk . the choice of classes is parameterized by θ. the generative\\nstory for this model is: 190 a course in machine learning 1. for each example n = 1 . . . n:\\n(a) choose a label yn ∼ disc(θ)\\n(b) choose example xn ∼ nor(µyn, σ2\\nyn ) this generative story can be directly translated into a likelihood as\\nbefore: p(d) = ∏\\nn (cid:122) = ∏\\nn mult(yn θ)nor(xn µyn, σ2\\nyn )\\n(cid:34) for each example (cid:125)(cid:124) (cid:105)− d 2πσ2\\nyn 2 exp θyn(cid:124)(cid:123)(cid:122)(cid:125) choose label (cid:104)\\n(cid:124) − 1\\n2σ2\\nyn (cid:123)(cid:122) choose feature values (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xn − µyn (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2 (16.12) (16.13) (cid:123)\\n(cid:35)\\n(cid:125) if you had access to labels, this would be all well and good, and\\nyou could obtain closed form solutions for the maximum likelihood\\nestimates of all parameters by taking a log and then taking gradients\\nof the log likelihood: θk = fraction of training examples in class k = 1\\nn ∑\\nn [yn = k] µk = mean of training examples in class k = ∑n[yn = k]xn\\n∑n[yn = k] σ2\\nk = variance of training examples in class k ∑n[yn = k] xn − µk ∑n[yn = k] = (16.14) (16.15) (16.16) ? you should be able to derive the\\nmaximum likelihood solution re-\\nsults formally by now. suppose that you don’t have labels. analogously to the k-means\\nalgorithm, one potential solution is to iterate. you can start off with\\nguesses for the values of the unknown variables, and then iteratively\\nimprove them over time. in k-means, the approach was the assign\\nexamples to labels (or clusters). this time, instead of making hard\\nassignments (“example 10 belongs to cluster 4”), we’ll make soft as-\\nsignments (“example 10 belongs half to cluster 4, a quarter to cluster\\n2 and a quarter to cluster 5”). so as not to confuse ourselves too\\nmuch, we’ll introduce a new variable, zn = (cid:104)zn,1, . . . , zn,k (that sums\\nto one), to denote a fractional assignment of examples to clusters. this notion of soft-assignments is visualized in figure 16.1. here,\\nwe’ve depicted each example as a pie chart, and it’s coloring denotes\\nthe degree to which it’s been assigned to each (of three) clusters. the\\nsize of the pie pieces correspond to the zn values. figure 16.1: em:piecharts: a ﬁgure\\nshowing pie charts formally, zn,k denotes the probability that example n is assigned to cluster k: expectation maximization 191 zn,k = p(yn = k xn)\\np(yn = k, xn) = = 1\\nzn p(xn)\\nmult(k θ)nor(xn µk, σ2\\nk ) (16.17) (16.18) (16.19) here, the normalizer zn is to ensure that zn sums to one. given a set of parameters (the θs, µs and σ2s), the fractional as-\\nsignments zn,k are easy to compute. now, akin to k-means, given\\nfractional assignments, you need to recompute estimates of the\\nmodel parameters. in analogy to the maximum likelihood solution\\n(eqs (??)-(??)), you can do this by counting fractional points rather\\nthan full points. this gives the following re-estimation updates: θk = fraction of training examples in class k = 1\\nn ∑\\nn zn,k µk = mean of fractional examples in class k = ∑n zn,kxn\\n∑n zn,k σ2\\nk = variance of fractional examples in class k ∑n zn,k xn − µk = ∑n zn,k (16.20) (16.21) (16.22) all that has happened here is that the hard assignments “[yn = k]”\\nhave been replaced with soft assignments “zn,k”. as a bit of fore-\\nshadowing of what is to come, what we’ve done is essentially replace\\nknown labels with expected labels, hence the name “expectation maxi-\\nmization.” putting this together yields algorithm 16.2. this is the gmm\\n(“gaussian mixture models”) algorithm, because the probabilitic\\nmodel being learned describes a dataset as being drawn from a mix-\\nture distribution, where each component of this distribution is a\\ngaussian. just as in the k-means algorithm, this approach is succeptible to\\nlocal optima and quality of initialization. the heuristics for comput-\\ning better initializers for k-means are also useful here. ',\n",
       " ' at this point, you’ve seen a method for learning in a particular prob-\\nabilistic model with hidden variables. two questions remain: (1) can ? aside from the fact that gmms\\nuse soft assignments and k-means\\nuses hard assignments, there are\\nother differences between the two\\napproaches. what are they? // randomly initialize mean for kth cluster\\n// initialize variances\\n// each cluster equally likely a priori (cid:2)2πσ2 (cid:3)− d for k = 1 to k do zn,k ← θk\\n(unnormalized) fractional assignments 2 exp 2σ2\\nk k (cid:104)− 1 xn − µk2(cid:105) // compute 192 a course in machine learning algorithm 38 gmm(x, k) 3: 1: for k = 1 to k do\\n2: µk ← some random location\\nk ← 1\\nσ2\\nθk ← 1/k\\n4:\\n5: end for\\n6: repeat\\n7: for n = 1 to n do 8: 9: 10: 11: 12: 13: 14: 15: 16: end for\\nzn ← 1\\n∑k zn,k zn end for\\nfor k = 1 to k do\\nθk ← 1\\nn ∑n zn,k\\nµk ← ∑n zn,kxn\\n∑n zn,k\\nk ← ∑n zn,kxn−µk\\nσ2\\n∑n zn,k end for 17:\\n18: until converged\\n19: return z // normalize fractional assignments // re-estimate prior probability of cluster k\\n// re-estimate mean of cluster k\\n// re-estimate variance of cluster k // return cluster assignments you apply this idea more generally and (2) why is it even a reason-\\nable thing to do? expectation maximization is a family of algorithms\\nfor performing maximum likelihood estimation in probabilistic mod-\\nels with hidden variables.\\nthe general ﬂavor of how we will proceed is as follows. we want\\nto maximize the log likelihood l, but this will turn out to be difﬁ-\\ncult to do directly. instead, we’ll pick a surrogate function ˜l that’s a\\nlower bound on l (i.e., ˜l ≤ l everywhere) that’s (hopefully) easier\\nto maximize. we’ll construct the surrogate in such a way that increas-\\ning it will force the true likelihood to also go up. after maximizing\\n˜l, we’ll construct a new lower bound and optimize that. this process\\nis shown pictorially in figure 16.2. to proceed, consider an arbitrary probabilistic model p(x, y θ),\\nwhere x denotes the observed data, y denotes the hidden data and\\nθ denotes the parameters. in the case of gaussian mixture models,\\nx was the data points, y was the (unknown) labels and θ included\\nthe cluster prior probabilities, the cluster means and the cluster vari-\\nances. now, given access only to a number of examples x1, . . . , xn,\\nyou would like to estimate the parameters (θ) of the model. probabilistically, this means that some of the variables are un-\\nknown and therefore you need to marginalize (or sum) over their\\npossible values. now, your data consists only of x = (cid:104)x1, x2, . . . , xn(cid:105), figure 16.2: em:lowerbound: a ﬁgure\\nshowing successive lower bounds expectation maximization 193 not the (x, y) pairs in d. you can then write the likelihood as: p(x θ) = ∑\\ny1 ∑\\ny2 · · · ∑\\nyn p(x, y1, y2, . . . yn θ) = ∑\\ny1 ∑\\ny2 · · · ∑\\nyn ∏\\nn p(xn, yn θ) = ∏\\nn ∑\\nyn p(xn, yn θ) marginalization (16.23) examples are independent (16.24) algebra (16.25) at this point, the natural thing to do is to take logs and then start\\ntaking gradients. however, once you start taking logs, you run into a\\nproblem: the log cannot eat the sum!\\np(xn, yn θ) (16.26) l(x θ) = ∑\\nn log ∑\\nyn the next step is to apply the somewhat strange, but strangely namely, the log gets “stuck” outside the sum and cannot move in to\\ndecompose the rest of the likelihood term!\\nuseful, trick of multiplying by 1. in particular, let q(·) be an arbitrary\\nprobability distribution. we will multiply the p(. . . ) term above by\\nq(yn)/q(yn), a valid step so long as q is never zero. this leads to: l(x θ) = ∑\\nn log ∑\\nyn q(yn) p(xn, yn θ) q(yn) (16.27) we will now construct a lower bound using jensen’s inequality.\\nthis is a very useful (and easy to prove!) result that states that\\nf (∑i λixi) ≥ ∑i λi f (xi), so long as (a) λi ≥ 0 for all i, (b) ∑i λi = 1,\\nand (c) f is concave. if this looks familiar, that’s just because it’s a\\ndirect result of the deﬁnition of concavity. recall that f is concave if\\nf (ax + by) ≥ a f (x) + b f (x) whenever a + b = 1. you can now apply jensen’s inequality to the log likelihood by\\nidentifying the list of q(yn)s as the λs, log as f (which is, indeed,\\nconcave) and each “x” as the p/q term. this yields: ? prove jensen’s inequality using the\\ndeﬁnition of concavity and induc-\\ntion. p(xn, yn θ) q(yn) log\\nq(yn) log p(xn, yn θ) − q(yn) log q(yn) q(yn) l(x θ) ≥ ∑\\n(cid:104)\\nn\\n= ∑\\nn\\n(cid:44) ˜l(x θ) ∑\\nyn\\n∑\\nyn (cid:105) (16.28) (16.29) (16.30) note that this inequality holds for any choice of function q, so long as\\nits non-negative and sums to one. in particular, it needn’t even by the 194 a course in machine learning same function q for each n. we will need to take advantage of both of\\nthese properties.\\nwe have succeeded in our ﬁrst goal: constructing a lower bound\\non l. when you go to optimize this lower bound for θ, the only part\\nthat matters is the ﬁrst term. the second term, q log q, drops out as a\\nfunction of θ. this means that the the maximization you need to be\\nable to compute, for ﬁxed qns, is: θ(new) ← arg max θ ∑\\nn ∑\\nyn qn(yn) log p(xn, yn θ) (16.31) this is exactly the sort of maximization done for gaussian mixture\\nmodels when we recomputed new means, variances and cluster prior\\nprobabilities. the second question is: what should qn(·) actually be? any rea- sonable q will lead to a lower bound, so in order to choose one q over\\nanother, we need another criterion. recall that we are hoping to max-\\nimize l by instead maximizing a lower bound. in order to ensure\\nthat an increase in the lower bound implies an increase in l, we need\\nto ensure that l(x θ) = ˜l(x θ). in words:\\n˜l should be a lower\\nbound on l that makes contact at the current point, θ. 16.4 further reading todo further reading learning objectives:\\n• recognize when a problem should\\nbe solved using a structured predic-\\ntion technique. • implement the structured perceptron algorithm for sequence labeling. • map “argmax” problems to integer linear programs. • augment the structured perceptron with losses to derive structured\\nsvms. dependencies: it is often the case that instead of predicting a single output, you\\nneed to predict multiple, correlated outputs simultaneously. in nat-\\nural language processing, you might want to assign a syntactic label\\n(like noun, verb, adjective, etc.) to words in a sentence: there is clear\\ncorrelation among these labels. in computer vision, you might want\\nto label regions in an image with object categories; again, there is\\ncorrelation among these regions. the branch of machine learning that\\nstudies such questions is structured prediction. in this chapter, we will cover two of the most common algorithms for structured prediction: the structured perceptron and the struc-\\ntured support vector machine. we will consider two types of struc-\\nture. the ﬁrst is the “sequence labeling” problem, typiﬁed by the\\nnatural language processing example above, but also common in\\ncomputational biology (labeling amino acids in dna) and robotics\\n(labeling actions in a sequence). for this, we will develop specialized\\nprediction algorithms that take advantage of the sequential nature\\nof the task. we will also consider more general structures beyond\\nsequences, and discuss how to cast them in a generic optimization\\nframework: integer linear programming (or ilp). the general framework we will explore is that of jointly scoring input/output conﬁgurations. we will construct algorithms that learn a\\nfunction s(x, ˆy) (s for “score”), where x is an input (like an image)\\nand ˆy is some predicted output (like a segmentation of that image).\\nfor any given image, there are a lot of possible segmentations (i.e.,\\na lot of possible ˆys), and the goal of s is to rank them in order of\\n“how good” they are: how compatible they are with the input x. the\\nmost important thing is that the scoring function s ranks the true\\nsegmentation y higher than any other imposter segmentation ˆy. that\\nis, we want to ensure that s(x, y) > s(x, ˆy) for all ˆy (cid:54)= y. the main\\nchallenge we will face is how to do this efﬁciently, given that there are\\nso many imposter ˆys. 196 a course in machine learning ',\n",
       " ' in order to build up to structured problems, let’s begin with a simpli-\\nﬁed by pedagogically useful stepping stone: multiclass classiﬁcation\\nwith a perceptron. as discussed earlier, in multiclass classiﬁcation we\\nhave inputs x ∈ rd and output labels y ∈ {1, 2, . . . , k}. our goal\\nis to learn a scoring function s so that s(x, y) > s(x, ˆy) for all ˆy (cid:54)= y,\\nwhere y is the true label and ˆy is an imposter label. the general form\\nof scoring function we consider is a linear function of a joint feature\\nvector φ(x, y): s(x, y) = w · φ(x, y) (17.1) here, the features φ(x, y) should denote how “compatible” the input\\nx is with the label y. we keep track of a single weight vector w that\\nlearns how to weigh these different “compatibility” features. a natural way to represent φ, if you know nothing else about the problem, is an outer product between x and the label space. this\\nyields the following representation: (cid:68) φ(x, k) = (cid:124) (cid:123)(cid:122) 0, 0, . . . , 0\\nd(k−1) zeros (cid:125) , x(cid:124)(cid:123)(cid:122)(cid:125)∈rd (cid:124) (cid:123)(cid:122) (cid:125) , 0, 0, . . . , 0\\nd(k−k) zeros (cid:69) ∈ rdk (17.2) in this representation, w effectively encodes a separate weight for\\nevery feature/label pair. how are we going to learn w? we will start with w = 0 and then process each input one at a time. suppose we get an input x with\\ngold standard label y. we will use the current scoring function to\\npredict a label. in particular, we will predict the label ˆy that maxi-\\nmizes the score: ˆy = argmax\\nˆy∈[1,k]\\n= argmax\\nˆy∈[1,k] s(x, ˆy)\\nw · φ(x, ˆy) (17.3) (17.4) if this predicted output is correct (i.e., ˆy = y), then, per the normal\\nperceptron, we will do nothing. suppose that ˆy (cid:54)= y. this means that\\nthe score of ˆy is greater than the score of y, so we want to update w\\nso that the score of ˆy is decreased and the score of y is increased. we\\ndo this by: w ← w + φ(x, y) − φ(x, ˆy) (17.5) to make sure this is doing what we expect, let’s consider what would\\nhappen if we computed scores under the updated value of w. to make\\nthe notation clear, let’s say w(old) are the weights before update, and structured prediction 197 algorithm 39 multiclassperceptrontrain(d, maxiter)\\n1: w ← 0\\n2: for iter = 1 . . . maxiter do\\n3: for all (x,y) ∈ d do // initialize weights // compute prediction // update weights // return learned weights ˆy ← argmaxk w · φ(x, k)\\nif ˆy (cid:54)= y then w ← w + φ(x, y) − φ(x, ˆy) 4: 5: 6: 7: end if\\nend for 8:\\n9: end for\\n10: return w w(new) are the weights after update. then: (cid:16)\\nw(new) · φ(x, y)\\n=\\n(cid:123)(cid:122)\\n(cid:124)\\n(cid:125)\\n= w(old) · φ(x, y) w(old) + φ(x, y) − φ(x, ˆy)\\n(cid:123)(cid:122) (cid:17) · φ(x, y)\\n(cid:124)\\n(cid:125)\\n(cid:124)\\n(cid:125)\\n+ φ(x, y) · φ(x, y)\\n− φ(x, ˆy) · φ(x, y) (cid:123)(cid:122) =0 old prediction ≥0 (17.6)\\n(17.7)\\n(17.8) here, the ﬁrst term is the old prediction. the second term is of the\\nform a · a which is non-negative (and, unless φ(x, y) is the zero vec-\\ntor, positive). the third term is the dot product between φ for two\\ndifferent labels, which by deﬁnition of φ is zero (see eq (17.2)). this gives rise to the updated multiclass perceptron speciﬁed in\\nalgorithm 17.1. as with the normal perceptron, the generalization\\nof the multiclass perceptron increases dramatically if you do weight\\naveraging. an important note is that multiclassperceptrontrain is actually more powerful than suggested so far. for instance, suppose that you\\nhave three categories, but believe that two of them are tightly related,\\nwhile the third is very different. for instance, the categories might\\nbe {music, movies, oncology}. you can encode this relatedness by\\ndeﬁning a feature expansion φ that reﬂects this: φ(x, music) = (cid:104)x, 0, 0, x(cid:105)\\nφ(x, movies) = (cid:104)0, x, 0, x(cid:105)\\nφ(x, oncology) = (cid:104)0, 0, x, 0(cid:105) (17.9)\\n(17.10)\\n(17.11) this encoding is identical to the normal encoding in the ﬁrst three\\npositions, but includes an extra copy of the features at the end,\\nshared between music and movies. by doing so, if the perceptron\\nwants to learn something common to music and movies, it can use\\nthis ﬁnal shared position. ? verify the score of ˆy, w(new) · φ(x, ˆy),\\ndecreases after an update, as we\\nwould want. ? suppose you have a hierarchy\\nof classes arranged in a tree.\\nhow could you use that to\\nconstruct a feature representa-\\ntion. you can think of the mu-\\nsic/movies/oncology example as\\na binary tree: the left branch of the\\nroot splits into music and movies;\\nthe right branch of the root is just\\noncology. 198 a course in machine learning ',\n",
       " ' let us now consider the sequence labeling task. in sequence labeling,\\nthe outputs are themselves variable-length vectors. an input/output\\npair (which must have the same length) might look like: x = “ monsters eat tasty bunnies “\\ny = noun verb adj noun (17.12)\\n(17.13) to set terminology, we will refer to the entire sequence y as the “out-\\nput” and a single label within y as a “label”. as before, our goal is to\\nlearn a scoring function that scores the true output sequence y higher\\nthan any imposter output sequence. as before, despite the fact that y is now a vector, we can still de- ﬁne feature functions over the entire input/output pair. for instance,\\nwe might want to count the number of times “monsters” has been\\ntagged as “noun” in a given output. or the number of times “verb”\\nis followed by “noun” in an output. both of these are features that\\nare likely indicative of a correct output. we might also count the num-\\nber of times “tasty” has been tagged as a verb (probably a negative\\nfeature) and the number of times two verbs are adjacent (again, prob-\\nably a negative feature). more generally, a very standard set of features would be: • the number of times word w has been labeled with tag l, for all words w and all syntactic tags l • the number of times tag l is adjacent to tag l(cid:48) in the output, for all tags l and l(cid:48) the ﬁrst set of features are often called unary features, because they\\ntalk only about the relationship between the input (sentence) and a\\nsingle (unit) label in the output sequence. the second set of features\\nare often called markov features, because they talk about adjacent la-\\nbels in the output sequence, which is reminiscent of markov models\\nwhich only have short term memory. note that for a given input x of length l (in the example, l = 4), the number of possible outputs is kl, where k is the number of\\nsyntactic tags. this means that the number of possible outputs grows\\nexponentially in the length of the input. in general, we write y (x) to\\nmean “the set of all possible structured outputs for the input x”. we\\nhave just seen that y (x) = klen(x). despite the fact that the inputs and outputs have variable length,\\nthe size of the feature representation is constant. if there are v words\\nin your vocabulary and k labels for a given word, the the number of\\nunary features is vk and the number of markov features is k2, so structured prediction 199 algorithm 40 structuredperceptrontrain(d, maxiter)\\n1: w ← 0\\n2: for iter = 1 . . . maxiter do\\n3: for all (x,y) ∈ d do // initialize weights // compute prediction // update weights // return learned weights ˆy ← argmax ˆy∈y (x) w · φ(x, ˆy)\\nif ˆy (cid:54)= y then\\nw ← w + φ(x, y) − φ(x, ˆy) 4: 5: 6: 7: end if\\nend for 8:\\n9: end for\\n10: return w the total number of features is k(v + k). of course, more complex\\nfeature representations are possible and, in general, are a good idea.\\nfor example, it is often useful to have unary features of neighboring\\nwords like “the number of times the word immediately preceding a\\nverb was ’monsters’.” now that we have a ﬁxed size feature representation, we can de-\\nvelop a perceptron-style algorithm for sequence labeling. the core\\nidea is the same as before. we will maintain a single weight vector w.\\nwe will make predictions by choosing the (entire) output sequence\\nˆy that maximizes a score given by w · φ(x, ˆy). and if this output se-\\nquence is incorrect, we will adjust the weights word the correct output\\nsequence y and away from the incorrect output sequence ˆy. this is\\nsummarized in algorithm 17.2 you may have noticed that algorithm 17.2 for the structured per-\\nceptron is identical to algorithm 17.1, aside from the fact that in the\\nmulticlass perceptron the argmax is over the k possible classes, while\\nin the structured perceptron, the argmax is over the kl possible out-\\nput sequences! the only difﬁculty in this algorithm is in line 4:\\nˆy ← argmax\\nˆy∈y (x) w · φ(x, ˆy) (17.14) in principle, this requires you to search over kl possible output se-\\nquences ˆy to ﬁnd the one that maximizes the dot product. except for\\nvery small k or very small l, this is computationally infeasible. be-\\ncause of its difﬁculty, this is often refered to as the argmax problem\\nin structured prediction. below, we consider how to solve the argmax\\nproblem for sequences. ',\n",
       " ' we now face an algorithmic question, not a machine learning ques-\\ntion: how to compute the argmax in eq 17.14 efﬁciently. in general, 200 a course in machine learning this is not possible. however, under somewhat restrictive assump-\\ntions about the form of our features φ, we can solve this problem efﬁ-\\nciently, by casting it as the problem of computing a maximum weight\\npath through a speciﬁcally constructed lattice. this is a variant of the\\nviterbi algorithm for hidden markov models, a classic example of dy-\\nnamic programming. (later, in section 17.6, we will consider argmax\\nfor more general problems.) the key observation for sequences is that—so long as we restrict our attention to unary features and markov features—the feature\\nfunction φ decomposes over the input. this is easiest to see with\\nan example. consider the input/output sequence from before: x =\\n“monsters eat tasty bunnies” and y = [noun verb adj noun]. if we\\nwant to compute the number of times “bunnies” is tagged as “noun”\\nin this pair, we can do this by:\\n1. count the number of times “bunnies” is tagged as “noun” in the ﬁrst three words of the sentence 2. add to that the number of times “bunnies” is tagged as “noun” in the ﬁnal word we can do a similar exercise for markov features, like the number of\\ntimes “adj” is followed by “noun”. however, we don’t actually need these counts. all we need for\\ncomputing the argmax sequence is the dot product between the\\nweights w and these counts. in particular, we can compute w · φ(x, y)\\nas the dot product on all-but-the-last word plus the dot product on\\nthe last word: w · φ1:3(x, y) + w · φ4(x, y). here, φ1:3 means “fea-\\ntures for everything up to and including position 3” and φ4 means\\n“features for position 4.” more generally, we can write φ(x, y) = ∑l l=1 φl(x, y), where φl(x, y) only includes features about position l.1 in particular, we’re\\ntaking advantage of the associative law for addition: w · φ(x, y) = w · l∑ l=1 φl(x, y) decomposition of structure (17.15) = l∑ l=1 w · φl(x, y) associative law (17.16) what this means is that we can build a graph like that in figure ??,\\nwith one verticle slice per time step (l 1 . . . l).2 each edge in this\\ngraph will receive a weight, constructed in such a way that if you\\ntake a complete path through the lattice, and add up all the weights,\\nthis will correspond exactly to w · φ(x, y).\\nto complete the construction, let φl(x,· · · ◦ y ◦ y(cid:48)) denote the unary features at position l together with the markov features that end at 1 in the case of markov features, we\\nthink of them as pairs that end at\\nposition l, so “verb adj” would be the\\nactive feature for φ3. 2 a graph of this sort is called a trel-\\nlis, and sometimes a lattice in the\\nliterature. structured prediction 201 figure 17.1: a picture of a trellis se-\\nquence labeling. at each time step l\\nthe corresponding word can have any\\nof the three possible labels. any path\\nthrough this trellis corresponds to a\\nunique labeling of this sentence. the\\ngold standard path is drawn with bold\\nred arrows. the highlighted edge cor-\\nresponds to the edge between l = 2\\nand l = 3 for verb/adj as described\\nin the text. that edge has weight\\nw · φ3(x,· · · ◦ verb ◦ adj). position l. these features depend only on x, y and y(cid:48), and not any of\\nthe previous parts of the output. for example, in the running example “monsters/noun eat/verb tasty/adj bunnies/noun”, consider the edge between l = 2 and\\nl = 3 going from “verb” to “adj”. (note: this is a “correct” edge, in\\nthe sense that it belongs to the ground truth output.) the features\\nassociated with this edge will be unary features about “tasty/adj”\\nas well as markov features about “verb/adj”. the weight of this edge\\nwill be exactly the total score (according to w) of those features. formally, consider an edge in the trellis that goes from time l −\\n1 to l, and transitions from y to y(cid:48). set the weight of this edge to\\nexactly w · φl(x,· · · ◦ y ◦ y(cid:48)). by doing so, we guarantee that the\\nsum of weights along any path through this lattice is exactly equal\\nto the score of that path. once we have constructed the graph as\\nsuch, we can run any max-weight path algorithm to compute the\\nhighest scoring output. for trellises, this can be computed by the\\nviterbi algorithm, or by applying any of a number of path ﬁnding\\nalgorithms for more general graphs. a complete derivation of the\\ndynamic program in this case is given in section 17.7 for those who\\nwant to implement it directly. the main beneﬁt of this construction is that it is guaranteed to\\nexactly compute the argmax output for sequences required in the\\nstructured perceptron algorithm, efﬁciently. in particular, it’s run-\\ntime is o(lk2), which is an exponential improvement on the naive\\no(kl) runtime if one were to enumerate every possible output se-\\nquence. the algorithm can be naturally extended to handle “higher\\norder” markov assumptions, where features depend on triples or\\nquadruples of the output. the trellis becomes larger, but the algo-\\nrithm remains essentially the same. in order to handle a length m\\nmarkov features, the resulting algorithm will take o(lkm) time. in\\npractice, it’s rare that m > 3 is necessary or useful. tastynvabunniesnvaeatnvamonstersnva202 a course in machine learning ',\n",
       " ' in section 7.7 we saw the support vector machine as a very useful\\ngeneral framework for binary classiﬁcation. in this section, we will\\ndevelop a related framework for structured support vector machines.\\nthe two main advantages of structured svms over the structured\\nperceptron are (1) it is regularized (though averaging in structured\\nperceptron achieves a similar effect) and (2) we can incorporate more\\ncomplex loss functions. in particular, one suboptimal thing about the structured percep-\\ntron is that all errors are consider equally bad. for structured prob-\\nlems, we often have much more nuanced and elaborate loss functions\\nthat we want to optimize. even for sequence labeling, it is typically\\nfar worse to label every word incorrectly than to just label one word\\nincorrectly. it is very common to use hamming loss as a general loss\\nfunction for structured prediction. hamming loss simply counts:\\nof all the predictions you made, how many were incorrect? for se-\\nquence labeling, it is:\\nl∑ (cid:96)(ham)(y, ˆy) = 1[yl (cid:54)= ˆyl] (17.17) l=1 in order to build up to structured svms, recall that svms began with\\nthe following optimization problem: min\\nw,ξ w2 (cid:124) (cid:123)(cid:122) (cid:125) + c ∑\\nn 1\\n2\\nlarge margin (cid:124) (cid:123)(cid:122) (cid:125)\\nsubj. to yn (w · xn + b) ≥ 1 − ξn small slack ξn ξn ≥ 0 (17.18) (∀n)\\n(∀n) after a bit of work, we were able to reformulate this in terms of a\\nstandard loss optimization algorithm with hinge loss: min\\nw w2 (cid:124) (cid:123)(cid:122) (cid:125) 1\\n2\\nlarge margin + c ∑\\nn (cid:124) (cid:125)\\n(cid:96)(hin)(yn, w · xn + b) (cid:123)(cid:122) small slack (17.19) we can do a similar derivation in the structured case. the question\\nis: exactly what should the slack be measuring? our goal is for the\\nscore of the true output y to beat the score of any imposter output\\nˆy. to incorporate loss, we will say that we want the score of the true\\noutput to beat the score of any imposter output by at least the loss\\nthat would be suffered if we were to predict that imposter output. an\\nalternative view is the ranking view: we want the true output to be\\nranked above any imposter by an amount at least equal to the loss. structured prediction 203 to keep notation simple, we will write sw(x, y) to denote the score\\nof the pair x, y, namely w · φ(x, y). this suggests a set of constraints\\nof the form: sw(x, y) − sw(x, ˆy) ≥ (cid:96)(ham)(y, ˆy) − ξ ˆy (∀n,∀ ˆy ∈ y (x)) (17.20) the rest of the optimization problem remains the same, yielding: 1\\n2 min\\nw,ξ w2 + c ∑\\nn ∑\\nˆy∈y xn\\nsubj. to sw(x, y) − sw(x, ˆy) ξn, ˆy (17.21) ξn, ˆy ≥ 0 ≥ (cid:96)(ham)(yn, ˆy) − ξn, ˆy (∀n,∀ ˆy ∈ y (xn))\\n(∀n,∀ ˆy ∈ y (xn))\\nthis optimization problem asks for a large margin and small slack,\\nwhere there is a slack very for every training example and every\\npossible incorrect output associated with that training example. in\\ngeneral, this is way too many slack variables and way too many con-\\nstraints!\\nthere is a very useful, general trick we can apply. if you focus on\\nthe ﬁrst constraint, it roughly says (letting s() denote score): s(y) ≥ (cid:2)s( ˆy) + (cid:96)(y, ˆy)(cid:3) for all ˆy, modulo slack. we’ll refer to the thing in (cid:2)s( ˆy) + (cid:96)(y, ˆy)(cid:3), modulo slack. expanding out the deﬁnition brackets as the “loss-augmented score.” but if we want to guarantee\\nthat the score of the true y beats the loss-augmented score of all ˆy, it’s\\nenough to ensure that it beats the loss-augmented score of the most\\nconfusing imposter. namely, it is sufﬁcient to require that s(y) ≥\\nmax ˆy\\nof s() and adding slack back in, we can replace the exponentially\\nlarge number of constraints in eq (17.21) with the simpler set of\\nconstraints: (cid:105) − ξn (∀n) sw(xn, yn) ≥ max\\nˆy∈y (xn) sw(xn, ˆy) + (cid:96)(ham)(yn, ˆy) we can now apply the same trick as before to remove ξn from the\\nanalysis. in particular, because ξn is constrained to be ≥ 0 and be-\\ncause we are trying to minimize it’s sum, we can ﬁgure out that out\\n(cid:41)\\nthe optimum, it will be the case that: (cid:40) (cid:105) − sw(xn, yn) sw(xn, ˆy) + (cid:96)(ham)(yn, ˆy) (cid:104) (cid:104) ξn = max 0, max\\nˆy∈y (xn) = (cid:96)(s-h)(yn, xn, w) (17.22)\\n(17.23) this value is referred to as the structured hinge loss, which we have\\ndenoted as (cid:96)(s-h)(yn, xn, w). this is because, although it is more com-\\nplex, it bears a striking resemlance to the hinge loss from chapter 7. 204 a course in machine learning in particular, if the score of the true output beats the score of every\\nthe best imposter by at least its loss, then ξn will be zero. on the\\nother hand, if some imposter (plus its loss) beats the true output, the\\nloss scales linearly as a function of the difference. at this point, there\\nis nothing special about hamming loss, so we will replace it with\\nsome arbitrary structured loss (cid:96). plugging this back into the objective function of eq (17.21), we can\\nwrite the structured svm as an unconstrained optimization problem,\\nakin to eq (17.19), as: min\\nw 1\\n2 w2 + c ∑\\nn (cid:96)(s-h)(yn, xn, w) (17.24) this is now in a form that we can optimize using subgradient descent\\n(chapter 7) or stochastic subgradient descent (chapter 14). in order to compute subgradients of eq (17.24), we need to be able\\nto compute subgradients of the structured hinge loss. mathematically\\nthis is straightforward. if the structured hinge loss on an example\\n(x, vy) is zero, then the gradient with respect to w is also zero. if the\\nstructured hinge loss is positive, then the gradient is: if the loss is > 0 ∇w(cid:96)(s-h)(y, x, w)\\nexpand deﬁnition using arbitrary structured loss (cid:96)\\n= ∇w w · φ(xn, ˆy) + (cid:96)(yn, ˆy) (cid:105) − w · φ(xn, yn) (cid:104) (cid:41) (17.25) (17.26) max\\nˆy∈y (xn) w · φ(xn, ˆy) − w · φ(xn, yn) + (cid:96)(yn, ˆy) deﬁne ˆyn to be the output that attains the maximum above, rearrange\\n= ∇w\\ntake gradient\\n= φ(xn, ˆy) − φ(xn, yn)\\n(cid:40) putting this together, we get the full gradient as: (17.27) (17.28) ∇w(cid:96)(s-h)(yn, xn, w) = (cid:40)\\n(cid:110) (cid:111) if (cid:96)(s-h)(yn, xn, w) = 0 0\\n(cid:105)\\nφ(xn, ˆyn) − φ(xn, yn) otherwise\\nw · φ(xn, ˆyn) + (cid:96)(yn, ˆyn) (cid:104) (17.29) where ˆyn = argmax\\nˆyn∈y (xn) the form of this gradient is very simple: it is equal to the features\\nof the worst imposter minus the features of the truth, unless the\\ntruth beats all imposters, in which case it’s zero. when plugged into\\nstochastic subgradient descent, you end up with an update that looks\\nvery much like the structured perceptron: if the current prediction\\n( ˆyn) is correct, there is no gradient step. but if the current prediction\\nis incorrect, you step w toward the truth and away from the imposter. structured prediction 205 algorithm 41 stochsubgradstructsvm(d, maxiter, λ, (cid:96))\\n1: w ← 0\\n2: for iter = 1 . . . maxiter do\\n3: for all (x,y) ∈ d do // initialize weights ˆy ← argmax ˆy∈y (x) w · φ(x, ˆy) + (cid:96)(y, ˆy)\\nif ˆy (cid:54)= y then w ← w + φ(x, y) − φ(x, ˆy) // loss-augmented prediction // update weights // shrink weights due to regularizer // return learned weights 4: 5: 6: 7: 8: end if\\nw ← w − λ n w end for 9:\\n10: end for\\n11: return w we will consider how to compute the loss-augmented argmax in the next section, but before that we summarize an algorithm for opti-\\nmizing structured svms using stochastic subgradient descent: algo-\\nrithm 17.4. of course there are other possible optimization strategies;\\nwe are highlighting this one because it is nearly identical to the struc-\\ntured perceptron. the only differences are: (1) on line 4 you use loss-\\naugmented argmax instead of argmax; and (2) on line 8 the weights\\nare shrunk slightly corresponding to the (cid:96)2 regularizer on w. (note:\\nwe have used λ = 1/(2c) to make the connection to linear models\\nclearer.) ',\n",
       " ' the challenge that arises is that we now have a more complicated\\nargmax problem that before. in structured perceptron, we only\\nneeded to compute ˆyn as the output that maximized its score (see\\neq 17.14). here, we need to ﬁnd the output that maximizes it score\\nplus it’s loss (eq (17.29)). this optimization problem is refered to as\\nloss-augmented search or loss-augmented inference. before solving the loss-augmented inference problem, it’s worth\\nthinking about why it makes sense. what is ˆyn? it’s the output that\\nhas the highest score among all outputs, after adding the output’s\\ncorresponding loss to that score. in other words, every incorrect\\noutput gets an artiﬁcial boost to its score, equal to its loss. the loss is\\nserving to make imposters look even better than they really are, so if\\nthe truth is to beat an imposter, it has to beat it by a lot. in fact, this\\nloss augmentation is essentially playing the role of a margin, where\\nthe required margin scales according to the loss. the algorithmic question, then, is how to compute ˆyn. in the fully\\ngeneral case, this is at least as hard as the normal argmax problem, so\\nwe cannot expect a general solution. moreover, even in cases where\\nthe argmax problem is easy (like for sequences), the loss-augmented 206 a course in machine learning argmax problem can still be difﬁcult. in order to make it easier, we\\nneed to assume that the loss decomposes of the input in a way that’s\\nconsistent with the features. in particular, if the structured loss func-\\ntion is hamming loss, this is often straightforward. as a concrete example, let’s consider loss-augmented argmax for sequences under hamming loss. in comparison to the trellis problem\\nsolved in section 17.7, the only difference is that we want to reward\\npaths that go through incorrect nodes in the trellis! in particular, in\\nfigure 17.1, all of the edges that are not part of the gold standard\\npath—those that are thinner and grey—get a free “+1” added to their\\nweights. since hamming loss adds one to the score for any word\\nthat’s predicted incorrectly, this means that every edge in the trellis\\nthat leads to an incorrect node (i.e., one that does not match the gold\\ntruth label) gets a “+1” added to its weight.\\nagain, consider an edge in the trellis that goes from time l − 1 to\\nl, and transitions from y to y(cid:48). in the non-loss-augmented, the weight\\nof this edge was exactly w · φl(x,· · · ◦ y ◦ y(cid:48)). in the loss-augmented\\ncases, the weight of this edge becomes:\\n(cid:125)\\n(cid:124)\\n(cid:124)\\n(cid:123)(cid:122)\\n(cid:125)\\nw · φl(x,· · · ◦ y ◦ y(cid:48))\\n1[y(cid:48) (cid:54)= yl] (17.30) (cid:123)(cid:122) + edge score, as before +1 for mispredictions once this loss-augmented graph has been constructed, the same max-\\nweight path algorithm can be run to ﬁnd the loss-augmented argmax\\nsequence. ',\n",
       " ' the general argmax problem for structured perceptron is the algo-\\nrithmic question of whether the following can be efﬁciently com-\\nputed: ˆy ← argmax\\nˆy∈y (x) w · φ(x, ˆy) (17.31) we have seen that if the output space y (x) is sequences and the\\nonly types of features are unary features and markov features, then\\nthis can be computed efﬁciently. there are a small number of other\\nstructured output spaces and feature restrictions for which efﬁcient\\nproblem-speciﬁc algorithms exist: • binary trees, with context-free features: use the cky algorithm\\n• 2d image segmentation, with adjacent-pixel features: use a form of graph cuts • spanning trees, with edge-based features: use kruskal’s algorithm\\n(or for directed spanning trees, use chu-liu/edmonds algorithm) structured prediction 207 3 i like gurobi best, and it’s free for\\nacademic use. it also has a really nice\\npython interface. these special cases are often very useful, and many problems can be\\ncast in one of these frameworks. however, it is often the case that you\\nneed a more general solution. one of the most generally useful solutions is to cast the argmax\\nproblem as an integer linear program, or ilp. ilps are a speciﬁc\\ntype of mathematical program/optimization problem, in which the\\nobjective function being optimized is linear and the constraints are\\nlinear. however, unlike “normal” linear programs, in an ilp you are\\nallowed to have integer constraints and disallow fractional values.\\nthe general form of an ilp is, for a ﬁxed vector a: a · z max z subj. to linear constraints on z (17.32) the main point is that the constraints on z are allowed to include\\nconstraints like z3 ∈ {0, 1}, which is considered an integer constraint.\\nbeing able to cast your argmax problem as an ilp has the advan-\\ntage that there are very good, efﬁciently, well-engineered ilp solvers\\nout there in the world.3 ilps are not a panacea though: in the worst\\ncase, the ilp solver will be horribly inefﬁcient. but for prototyping,\\nor if there are no better options, it’s a very handy technique. figuring out how exactly to cast your argmax problem as an ilp can be a bit challenging. let’s start with an example of encoding\\nsequence labeling with markov features as an ilp. we ﬁrst need\\nto decide what the variables will be. because we need to encode\\npairwise features, we will let our variables be of the form: zl,k(cid:48),k = 1[label l is k and label l − 1 is k(cid:48)] (17.33) these zs will all be binary indicator variables.\\nour next task is to construct the linear objective function. to do\\nso, we need to assign a value to al,k(cid:48),k in such a way that a · z will be\\nexactly equal to w · φ(x, y(z)), where y(z) denotes the sequence that\\nwe can read off of the variables z. with a little thought, we arrive at: al,k(cid:48),k = w · φl(x,(cid:104). . . , k(cid:48), k(cid:105)) (17.34) finally, we need to construct constaints. there are a few things that\\nthese constraints need to enforce:\\n1. that all the zs are binary. that’s easy: just say zl,k(cid:48),k ∈ {0, 1}, for all l, k(cid:48), k. 2. that for a given position l, there is exactly one active z. we can do this with an equality constraint: ∑k ∑k(cid:48) zl,k(cid:48),k = 1 for all l. 3. that the zs are internally consistent: if the label at position 5 is\\nsupposed to be “noun” then both z5,.,. and z6,.,. need to agree on 208 a course in machine learning this. we can do this as: ∑k(cid:48) zl,k(cid:48),k = ∑k(cid:48)(cid:48) zl+1,k,k(cid:48)(cid:48) for all l, k. effec-\\ntively what this is saying is that z5,?,verb = z6,verb,? where the “?”\\nmeans “sum over all possibilities.” this fully speciﬁes an ilp that you can relatively easily implement\\n(arguably more easily than the dynamic program in algorithm 17.7)\\nand which will solve the argmax problem for you. will it be efﬁcient?\\nin this case, probably yes. will it be as efﬁcient as the dynamic pro-\\ngram? probably not. it takes a bit of effort and time to get used to casting optimization\\nproblems as ilps, and certainly not all can be, but most can and it’s a\\nvery nice alternative. in the case of loss-augmented search for structured svms (as opposed to structured perceptron), the objective function of the ilp\\nwill need to be modiﬁed to include terms corresponding to the loss. ',\n",
       " ' recall the decomposition we derived earlier: w · φ(x, y) = w · l∑ l=1 φl(x, y) decomposition of structure (17.35) = l∑ l=1 w · φl(x, y) associative law (17.36) this decomposition allows us to construct the following dynamic\\nprogram. we will compute αl,k as the score of the best possible output\\npreﬁx up to and including position l that labels the lth word with\\nlabel k. more formally: αl,k = max\\nˆy1:l−1 w · φ1:l(x, ˆy ◦ k) (17.37) here, ˆy is a sequence of length l − 1, and ˆy ◦ k denotes the sequence\\nof length l obtained by adding k onto the end. the max denotes the\\nfact that we are seeking the best possible preﬁx up to position l − 1,\\nand the forcing the label for position l to be k. before working through the details, let’s consider an example. suppose that we’ve computing the αs up to l = 2, and have: α2,noun =\\n2, α2,verb = 9, α2,adj = −1 (recall: position l = 2 is “eat”). we want\\nto extend this to position 3; for example, we want to compute α3,adj.\\nlet’s assume there’s a single unary feature here, “tasty/adj” and\\nthree possible markov features of the form “?:adj”. assume these\\nweights are as given to the right. 4 now, the question for α3,adj is:\\nwhat’s the score of the best preﬁx that labels “tasty” as “adj”? we can\\nobtain this by taking the best preﬁx up to “eat” and then appending 4 w“tasty/adj” = 1.2\\nw“noun:adj” = −5\\nw“verb:adj” = 2.5\\nw“adj:adj” = 2.2 structured prediction 209 each possible label. whichever combination is best is the winner. the\\nrelevant computation is: (cid:110) α2,noun + w“tasty/adj” + w“noun:adj”\\nα2,verb + w“tasty/adj” + w“verb:adj”\\nα2,adj + w“tasty/adj” + w“adj:adj”\\n2 + 1.2 − 5, (cid:111) 9 + 1.2 + 2.5, −1 + 1.2 + 2.2 α3,adj = max = max = max (cid:110)\\n(cid:110) − 1.8, 12.7, 2.4 = 12.7 (cid:111) (cid:111) (17.38) (17.39) (17.40) this means that (a) the score for the preﬁx ending at position 3 la-\\nbeled as adjective is 12.7, and (b) the “winning” previous label was\\n“verb”. we will need to record these winning previous labels so that\\nwe can extract the best path at the end. let’s denote by ζl,k the label\\nat position l − 1 that achieves the max. from here, we can formally compute the αs recursively. the main observation that will be necessary is that, because we have\\nlimited ourselves to markov features, φl+1(x,(cid:104)y1, y2, . . . , yl, yl+1(cid:105))\\ndepends only on the last two terms of y, and does not depend on\\ny1, y2, . . . , yl−1. the full recursion is derived as: α0,k = 0 ∀k\\nζ0,k = ∅ ∀k the score for any empty sequence is zero αl+1,k = max\\nˆy1:l w · φ1:l+1(x, ˆy ◦ k) w ·(cid:16)\\n(cid:104) separate score of preﬁx from score of position l+1 = max\\nˆy1:l φ1:l(x, ˆy) + φl+1(x, ˆy ◦ k) distributive law over dot products = max\\nˆy1:l w · φ1:l(x, ˆy) + w · φl+1(x, ˆy ◦ k)\\n(cid:104) separate out ﬁnal label from preﬁx, call it k’ (cid:17) (cid:105) (17.41)\\n(17.42) (17.43) (17.44) (17.45) (17.46) (17.47) (17.48) (cid:105) = max\\nˆy1:l−1 max\\nk(cid:48) w · φ1:l(x, ˆy ◦ k(cid:48)) + w · φl+1(x, ˆy ◦ k(cid:48) ◦ k) swap order of maxes, and last term doesn’t depend on preﬁx (cid:105)\\n+ w · φl+1(x,(cid:104). . . , k(cid:48), k(cid:105)) (cid:105) (cid:20)(cid:104) (cid:104) = max k(cid:48) max\\nˆy1:l−1 w · φ1:l(x, ˆy ◦ k(cid:48)) apply recursive deﬁnition = max k(cid:48) αl,k(cid:48) + w · φl+1(x,(cid:104). . . , k(cid:48), k(cid:105)) (cid:105) 210 a course in machine learning algorithm 42 argmaxforsequences(x, w)\\n1: l ← len(x)\\n2: αl,k ← 0,\\nζk,l ← 0,\\n3: for l = 0 . . . l-1 do\\nfor k = 1 . . . k do\\n4: αl+1,k ← maxk(cid:48)(cid:2)αl,k(cid:48) + w · φl+1(x,(cid:104). . . , k(cid:48), k(cid:105))(cid:3) ∀ k = 1 . . . k, ∀l = 0 . . . l 5: 6: // recursion:\\n// here, φl+1(. . . k(cid:48), k . . . ) is the set of features associated with\\n// output position l + 1 and two adjacent labels k(cid:48) and k at that position\\n// store backpointer ζl+1,k ← the k’ that achieves the maximum above // initialize variables end for 7:\\n8: end for\\n9: y ← (cid:104)0, 0, . . . , 0(cid:105)\\n10: yl ← argmaxk αl,k\\n11: for l = l-1 . . . 1 do\\n12:\\n13: end for\\n14: return y yl ← ζl,yl+1 (cid:104) ζl+1,k = argmax k(cid:48) // initialize predicted output to l-many zeros\\n// extract highest scoring ﬁnal label // traceback ζ based on yl+1 // return predicted output and record a backpointer to the k’ that achieves the max αl,k(cid:48) + w · φl+1(x,(cid:104). . . , k(cid:48), k(cid:105)) (17.49) (cid:105) at the end, we can take maxk αl,k as the score of the best output\\nsequence. to extract the ﬁnal sequence, we know that the best label\\nfor the last word is argmax αl,k. let’s call this ˆyl once we know that,\\nthe best previous label is ζl−1, ˆyl. we can then follow a path through ζ\\nback to the beginning. putting this all together gives algorithm 17.7.\\nthe main beneﬁt of algorithm 17.7 is that it is guaranteed to ex- actly compute the argmax output for sequences required in the struc-\\ntured perceptron algorithm, efﬁciently. in particular, it’s runtime is\\no(lk2), which is an exponential improvement on the naive o(kl)\\nruntime if one were to enumerate every possible output sequence.\\nthe algorithm can be naturally extended to handle “higher order”\\nmarkov assumptions, where features depend on triples or quadru-\\nples of the output. the memoization becomes notationally cumber-\\nsome, but the algorithm remains essentially the same. in order to\\nhandle a length m markov features, the resulting algorithm will take\\no(lkm) time. in practice, it’s rare that m > 3 is necessary or useful. in the case of loss-augmented search for structured svms (as\\nopposed to structured perceptron), we need to include the scores\\ncoming from the loss augmentation in the dynamic program. the\\nonly thing that changes between the standard argmax solution (al-\\ngorithm 17.7, and derivation in eq (17.48)) is that the any time an\\nincorrect label is used, the (loss-augmented) score increases by one.\\nrecall that in the non-loss-augmented case, we have the α recursion structured prediction 211 as: αl+1,k = max\\nˆy1:l\\n= max k(cid:48) (cid:104)\\nw · φ1:l+1(x, ˆy ◦ k)\\nαl,k(cid:48) + w · φl+1(x,(cid:104). . . , k(cid:48), k(cid:105)) (cid:105) (17.50) (17.51) if we deﬁne ˜α to be the loss-augmented score, the corresponding\\nrecursion is (differences highlighted in blue): ˜αl+1,k = max\\nˆy1:l\\n= max k(cid:48) (cid:104)\\nw · φ1:l+1(x, ˆy ◦ k)+(cid:96)(ham)\\n˜αl,k(cid:48) + w · φl+1(x,(cid:104). . . , k(cid:48), k(cid:105)) (cid:105)\\n1:l+1 (y, ˆy ◦ k) +1[k (cid:54)= yl+1] (17.52) (17.53) in other words, when computing ˜α in the loss-augmented case,\\nwhenever the output prediction is forced to pass through an incorrect\\nlabel, the score for that cell in the dynamic program gets increased\\nby one. the resulting algorithm is identical to algorithm 17.7, except\\nthat eq (17.53) is used for computing αs. 17.8 further reading todo learning objectives:\\n• be able to formulate imitation learning problems. • understand the failure cases of simple classiﬁcation approaches to\\nimitation learning. • implement solutions to those prob- lems based on either classiﬁcation or\\ndataset aggregation. • relate structured prediction and imitation learning. dependencies: programming is a skill best acquired by practice and example\\nrather than from books. – alan turing so far, we have largely considered machine learning\\nproblems in which the goal of the learning algorithm is to make\\na single prediction. in many real world problems, however, an algo-\\nrithm must make a sequence of decisions, with the world possibly\\nchanging during that sequence. such problems are often called se-\\nquential decision making problems. a straightforward example—\\nwhich will be the running example for this chapter—is that of self-\\ndriving cars. we want to train a machine learning algorithm to drive\\na car. but driving a car is not a single prediction: it’s a sequence of\\npredictions over time. and as the machine is driving the car, the\\nworld around it is changing, often based on its own behavior. this\\ncreates complicated feedback loops, and one of the major challenges\\nwe will face is how to deal with these feedback loops. to make this discussion more concrete, let’s consider the case of a\\nself-driving car. and let’s consider a very simplistic car, in which the\\nonly decision that has to be made is how to steer, and that’s between\\none of three options: {left, right, none}. in the imitation learning\\nsetting, we assume that we have access to an expert or oracle that al-\\nready knows how to drive. we want to watch the expert driving, and\\nlearn to imitate their behavior. hence: imitation learning (sometimes\\ncalled learning by demonstration or programming by example, in\\nthe sense that programs are learned, and not implemented). at each point in time t = 1 . . . t, the car recieves sensor informa-\\ntion xt (for instance, a camera photo ahead of the car, or radar read-\\nings). it then has to take an action, at; in the case of the car, this is\\none of the three available steering actions. the car then suffers some\\nloss (cid:96)t; this might be zero in the case that it’s driving well, or large in\\nthe case that it crashes. the world then changes, moves to time step\\nt + 1, sensor readings xt+1 are observed, action at+1 is taken, loss (cid:96)t+1\\nis suffered, and the process continues. the goal is to learn a function f that maps from sensor readings xt\\nto actions. because of close connections to the ﬁeld of reinforcement\\nlearning, this function is typically called a policy. the measure of imitation learning 213 1 it’s completely okay for f to look\\nat more than just xt when making\\npredictions; for instance, it might want\\nto look at xt−1, or at−1 and at−2. as\\nlong as it only references information\\nfrom the past, this is ﬁne. for notational\\nsimplicity, we will assume that all of\\nthis relevant information is summarized\\nin xt. figure 18.1: a single expert trajectory in\\na self-driving car. success of a policy is: if we were to run this policy, how much total\\nloss would be suffered. in particular, suppose that the trajectory\\n(denoted τ) of observation/action/reward triples encountered by\\nyour policy is: τ = x1 , , (cid:96)1 , x2 , , (cid:96)2 , . . . , xt , (cid:104)∑t the losses (cid:96)t depend implicitly on the state of the world and the\\nactions of the policy. the goal of f is to minimize the expected loss\\nτ∼ f\\ne\\nthe world, and the sequence of actions taken is according to f .1 , where the expectation is taken over all randomness in t=1 (cid:96)t a2(cid:124)(cid:123)(cid:122)(cid:125) = f (x2) at(cid:124)(cid:123)(cid:122)(cid:125) = f (xt ) , (cid:96)t (18.1) = f (x1) a1(cid:124)(cid:123)(cid:122)(cid:125)\\n(cid:105) ',\n",
       " ' we will begin with a straightforward, but brittle, approach to imita-\\ntion learning. we assume access to a set of training trajectories taken\\nby an expert. for example, consider a self-driving car, like that in fig-\\nure 18.1. a single trajectory τ consists of a sequence of observations\\n(what is seen from the car’s sensors) and a sequence of actions (what\\naction did the expect take at that point in time). the idea in imitation\\nlearning by classiﬁcation is to learn a classiﬁer that attempts to mimic\\nthe expert’s action based on the observations at that time. in particular, we have τ1, τ2, . . . , τn. each of the n trajectories\\ncomprises a sequence of t-many observation/action/loss triples,\\nwhere the action is the action taken by the expert. t, the length of\\nthe trajectory is typically called the time horizon (or just horizon).\\nfor instance, we may ask an expert human driver to drive n = 20\\ndifferent routes, and record the observations and actions that driver\\nsaw and took during those routes. these are our training trajectories.\\nwe assume for simplicity that each of these trajectories is of ﬁxed\\nlength t, though this is mostly for notational convenience. the most straightforward thing we can do is convert this expert data into a big multiclass classiﬁcation problem. we take our favorite\\nmulticlass classiﬁcation algorithm, and use it to learn a mapping\\nfrom x to a. the data on which it is trained is the set of all observa-\\ntion/action pairs visited during any of the n trajectories. in total,\\nthis would be nt examples. this approach is summarized in algo-\\nrithm 18.1 for training and algorithm 18.1 for prediction. how well does this approach work?\\nthe ﬁrst question is: how good is the expert? if we learn to mimic\\nan expert, but the expert is no good, we lose. in general, it also seems\\nunrealistic to believe this algorithm should be able to improve on\\nthe expert. similarly, if our multiclass classiﬁcation algorithm a\\nis crummy, we also lose. so part of the question “how well does expertexpert214 a course in machine learning 1: d ←(cid:10)(x, a) : ∀n , ∀(x, a, (cid:96)) ∈ τn\\nalgorithm 43 supervisedimitationtrain(a, τ1, τ2, . . . , τn)\\n2: return a(d) // collect all observation/action pairs\\n// train multiclass classiﬁer on d (cid:11) algorithm 44 supervisedimitationtest( f )\\n1: for t = 1 . . . t do\\n2: xt ← current observation\\nat ← f (xt)\\ntake action at\\n(cid:96)t ← observe instantaneous loss 3: 4: 5:\\n6: end for\\n7: return ∑t t=1 (cid:96)t // ask policy to choose an action // return total loss this work” is the more basic question of: what are we even trying to\\nmeasure? there is a nice theorem2 that gives an upper bound on the loss\\nsuffered by the supervisedil algorithm (algorithm 18.1) as a func-\\ntion of (a) the quality of the expert, and (b) the error rate of the\\nlearned classiﬁer. to be clear, we need to distinguish between the\\nloss of the policy when run for t steps to form a full trajectory, and\\nthe error rate of the learned classiﬁer, which is just it’s average mul-\\nticlass classiﬁcation error. the theorem states, roughly, that the loss\\nof the learned policy is at most the loss of the expert plus t2 times the\\nerror rate of the classiﬁer. 2 ross et al. 2011 theorem 18 (loss of supervisedil). suppose that one runs algo-\\nrithm 18.1 using a multiclass classiﬁer that optimizes the 0-1 loss (or an\\nupperbound thereof). let \\x01 be the error rate of the underlying classiﬁer\\n(in expectation) and assume that all instantaneous losses are in the range\\n[0, (cid:96)(max)]. let f be the learned policy; then: (cid:34)\\n(cid:123)(cid:122) ∑\\nt (cid:96)t e τ∼ f (cid:124) (cid:35)\\n(cid:125) (cid:34)\\n≤ eτ∼expert\\n(cid:124)\\n(cid:123)(cid:122) ∑\\nt (cid:96)t (cid:35)\\n(cid:125) +(cid:96)(max)t2\\x01 (18.2) loss of learned policy loss of expert intuitively, this bound on the loss is about a factor of t away from\\nwhat we might hope for. in particular, the multiclass classiﬁer makes\\nerrors on an \\x01 fraction of it’s actions, measured by zero/one loss.\\nin the worst case, this will lead to a loss of (cid:96)(max)\\x01 for a single step.\\nsumming all these errors over the entire trajectory would lead to\\na loss on the order of (cid:96)(max)t\\x01, which is a factor t better than this\\ntheorem provides. a natural question (addressed in the next section)\\nis whether this is analysis is tight. a related question (addressed in\\nthe section after that) is whether we can do better. before getting\\nthere, though, it’s worth highlighting that an extra factor of t is really imitation learning 215 bad. it can cause even very small multiclass error rates to blow up; in\\nparticular, if \\x01 ≥ 1/t, we lose, and t can be in the hundreds or more. ',\n",
       " ' the biggest single issue with the supervised learning approach to\\nimitation learning is that it cannot learn to recover from failures. that\\nis: it has only been trained based on expert trajectories. this means\\nthat the only training data it has seen is that of an expert driver. if\\nit ever veers from that state distribution, it may have no idea how\\nto recover. as a concrete example, perhaps the expert driver never\\never gets themselves into a state where they are directly facing a\\nwall. moreover, the expert driver probably tends to drive forward\\nmore than backward. if the imperfect learner manages to make a few\\nerrors and get stuck next to a wall, it’s likely to resort to the general\\n“drive forward” rule and stay there forever. this is the problem of\\ncompounding error; and yes, it does happen in practice. it turns out that it’s possible to construct an imitation learning problem on which the t2 compounding error is unavoidable. con-\\nsider the following somewhat artiﬁcial problem. at time t = 1 you’re\\nshown a picture of either a zero or a one. you have two possible ac-\\ntions: press a button marked “zero” or press a button marked “one.”\\nthe “correct” thing to do at t = 1 is to press the button that corre-\\nsponds to the image you’ve been shown. pressing the correct button\\nleads to (cid:96)1 = 0; the incorrect leads to (cid:96)1 = 1. now, at time t = 2 you\\nare shown another image, again of a zero or one. the correct thing to\\ndo in this time step is the xor of (a) the number written on the picture\\nyou see right now, and (b) the correct answer from the previous time\\nstep. this holds in general for t > 1. there are two important things about this construction. the ﬁrst is that the expert can easily get zero loss. the second is that once the\\nlearned policy makes a single mistake, this can cause it to make all\\nfuture decisions incorrectly. (at least until it “luckily” makes another\\n“mistake” to get it back on track.) based on this construction, you can show the following theorem3. 3 kääriäinen 2006 theorem 19 (lower bound for supervisedil). there exist imitation\\nlearning problems on which algorithm 18.1 is able to achieve small classiﬁ-\\ncation error \\x01 ∈ [0, 1/t] under an optimal expert, but for which the test loss\\nis lower bounded as: (cid:104) 1 − (1 − 2\\x01)t+1(cid:105) (18.3) (cid:34)\\n(cid:123)(cid:122) e τ∼ f (cid:124) (cid:35)\\n(cid:125) ∑\\nt (cid:96)t ≥ t + 1 2 − 1\\n4\\x01 loss of learned policy which is bounded by t2\\x01 and, for small \\x01, grows like t2\\x01. 216 a course in machine learning up to constants, this gives matching upper and lower bounds for\\nthe loss of a policy learned by supervised imitation learning that is\\npretty far (a factor of t) from what we might hope for. ',\n",
       " ' supervised imitation learning fails because once it gets “off the ex-\\npert path,” things can go really badly. ideally, we might want to train\\nour policy to deal with any possible situation it could encounter.\\nunfortunately, this is unrealistic: we cannot hope to be able to train\\non every possible conﬁguration of the world; and if we could, we\\nwouldn’t really need to learn anyway, we could just memorize. so\\nwe want to train f on a subset of world conﬁgurations, but using\\n“conﬁgurations visited by the expert” fails because f cannot learn to\\nrecover from its own errors. somehow what we’d like to do is train f\\nto do well on the conﬁgurations that it, itself, encounters! this is a classic chicken-and-egg problem. we want a policy f that\\ndoes well in a bunch of world conﬁgurations. what set of conﬁgura-\\ntions? the conﬁgurations that f encounters! a very classic approach\\nto solving chicken-and-egg problems is iteration. start with some\\npolicy f . run f and see what conﬁgurations is visits. train a new f\\nto do well there. repeat. this is exactly what the dataset aggregation algorithm (“dagger”) does. continuing with the self-driving car analogy, we ﬁrst let a\\nhuman expert drive a car for a while, and learn an initial policy f0 by\\nrunning standard supervised imitation learning (algorithm 18.1) on\\nthe trajectories visited by the human. we then do something unusual.\\nwe put the human expert in the car, and record their actions, but the\\ncar behaves not according to the expert’s behavior, but according to\\nf0. that is, f0 is in control of the car, and the expert is trying to steer,\\nbut the car is ignoring them4 and simply recording their actions as\\ntraining data. this is shown in figure 18.2. based on trajectories generated by f0 but actions given by the expert, we generate a new dataset that contains information about\\nhow to recover from the errors of f0. we now will train a new policy,\\nf1. because we don’t want f1 to “forget” what f0 already knows, f1\\nis trained on the union of the initial expert-only trajectories together\\nwith the new trajectories generated by f0. we repeat this process a\\nnumber of times maxiter, yielding algorithm 18.3. this algorithm returns the list of all policies generated during its\\nrun. a very practical question is: which one should you use? there\\nare essentially two choices. the ﬁrst choice is just to use the ﬁnal\\npolicy learned. the problem with this approach is that dagger can\\nbe somewhat unstable in practice, and policies do not monotonically 4 this is possibly terrifying for the\\nexpert! figure 18.2: in dagger, the trajectory\\n(red) is generated according to the\\npreviously learned policy, f0, but the\\ngold standard actions are given by the\\nexpert. expertexpertff00imitation learning 217 (cid:11) // collect all pairs (same as supervised) // train initial policy (multiclass classiﬁer) on d0 (0) (0)\\nn algorithm 45 daggertrain(a, maxiter, n, expert)\\n1: (cid:104)τ n=1 ← run the expert n many times 3:\\n4: for i = 1 . . . maxiter do\\n5: n (cid:105)n\\nf0 ← a(d0)\\n(cid:104)τ 2: d0 ←(cid:10)(x, a) : ∀n , ∀(x, a, (cid:96)) ∈ τ\\ndi ←(cid:10)(x, expert(x)) : ∀n , ∀(x, a, (cid:96)) ∈ τ\\nfi ← a(cid:16)(cid:83)i n (cid:105)n n=1 ← run policy fi−1 n-many times (cid:17) (cid:11) (i) 6: j=0 dj 7:\\n8: end for\\n9: return (cid:104) f0, f1, . . . , fmaxiter(cid:105) (i)\\nn // trajectories by fi−1\\n// collect data set\\n// observations x visited by fi−1\\n// but actions according to the expert\\n// train policy fi on union of all data so far // return collection of all learned policies improve. a safer alternative (as we’ll see by theory below) is to test\\nall of them on some held-out “development” tasks, and pick the one\\nthat does best there. this requires a bit more computation, but is a\\nmuch better approach in general. one major difference in requirements between dagger (algo- rithm 18.3) and supervisedil (algorithm 18.1) is the requirement\\nof interaction with the expert. in supervisedil, you only need access\\nto a bunch of trajectories taken by the expert, passively. in dagger,\\nyou need access to them expert themselves, so you can ask questions\\nlike “if you saw conﬁguration x, what would you do?” this puts\\nmuch more demand on the expert. another question that arises is: what should n, the number of\\ntrajectories generated in each round, be? in practice, the initial n\\nshould probably be reasonably large, so that the initial policy f0\\nis pretty good. the number of trajectories generated by iteration\\nsubsequently can be much smaller, perhaps even just one. intuitively, dagger should be less sensitive to compounding error\\nthan supervisedil, precisely because it gets trained on observations\\nthat it is likely to see at test time. this is formalized in the following\\ntheorem: theorem 20 (loss of dagger). suppose that one runs algorithm 18.3\\nusing a multiclass classiﬁer that optimizes the 0-1 loss (or an upperbound\\nthereof). let \\x01 be the error rate of the underlying classiﬁer (in expectation)\\nand assume that all instantaneous losses are in the range [0, (cid:96)(max)]. let f be\\nthe learned policy; then: (cid:18) (cid:96)(max)t log t (cid:19) maxiter (18.4) (cid:34)\\n(cid:123)(cid:122) ∑\\nt (cid:96)t e τ∼ f (cid:124) (cid:35)\\n(cid:125) (cid:34)\\n≤ eτ∼expert\\n(cid:123)(cid:122)\\n(cid:124) ∑\\nt (cid:96)t (cid:35)\\n(cid:125) loss of learned policy loss of expert +(cid:96)(max)t\\x01 + o furthermore, if the loss function is strongly convex in f , and maxiter is 218 a course in machine learning ˜o(t/\\x01), then: (cid:34)\\n(cid:123)(cid:122) ∑\\nt (cid:96)t e τ∼ f (cid:124) (cid:35)\\n(cid:125) (cid:34)\\n≤ eτ∼expert\\n(cid:123)(cid:122)\\n(cid:124) ∑\\nt (cid:96)t (cid:35)\\n(cid:125) +(cid:96)(max)t\\x01 + o(\\x01) (18.5) loss of learned policy loss of expert both of these results show that, assuming maxiter is large enough, the loss of the learned policy f (here, taken to be the best on of all\\nthe maxiter policies learned) grows like t\\x01, which is what we hope\\nfor. note that the ﬁnal term in the ﬁrst bound gets small so long as\\nmaxiter is at least t log t. ',\n",
       " ' because of the strong requirement on the expert in dagger (i.e., that\\nyou need to be able to query it many times during training), one of\\nthe most substantial use cases for dagger is to learn to (quickly) imi-\\ntate otherwise slow algorithms. here are two prototypical examples:\\n1. game playing. when a game (like chess or minecraft) can be run in simulation, you can often explicitly compute a semi-optimal\\nexpert behavior with brute-force search. but this search might be\\ntoo computationally expensive to play in real time, so you can\\nuse it during training time, learning a fast policy that attempts\\nto mimic the expensive search. this learned policy can then be\\napplied at test time. 2. discrete optimizers. many discrete optimization problems can be\\ncomputationally expensive to run in real time; for instance, even\\nshortest path search on a large graph can be too slow for real time\\nuse. we can compute shortest paths ofﬂine as “training data” and\\nthen use imitation learning to try to build shortest path optimizers\\nthat will run sufﬁciently efﬁciently in real time. consider the game playing example, and for concreteness, sup-\\npose you are trying to learn to play solitaire (this is an easier exam-\\nple because it’s a single player game). when running daggertrain\\n(algorithm 18.3 to learn a chess-playing policy, the algorithm will\\nrepeatedly ask for expert(x), where x is the current state of the game.\\nwhat should this function return? ideally, it should return the/an ac-\\ntion a such that, if a is taken, and then the rest of the game is played\\noptimally, the player wins. computing this exactly is going to be very\\ndifﬁcult for anything except the simplest games, so we need to restort\\nto an approxiamtion. imitation learning 219 algorithm 46 depthlimiteddfs(x, h, maxdepth)\\n1: if x is a terminal state or maxdepth ≤ 0 then return (⊥, h(x)) 3: else\\n4: bestaction, bestscore ← ⊥, −∞\\nfor all actions a from x do // if we cannot search deeper\\n// return “no action” (⊥) and the current heuristic score // keep track of best action & its score (_, score) ← depthlimiteddfs(x ◦ a, h, maxdepth − 1) // get score\\n// for action a, depth reduced by one by appending a to x if score > bestscore then bestaction, bestscore ← a, score // update tracked best action & score 2: 5: 6: 7: 8: 9: end if\\nend for 10:\\n11: end if\\n12: return (bestaction, bestscore) // return best found action and its score a common strategy is to run a depth-limited depth ﬁrst search, starting at state x, and terminating after at most three of four moves\\n(see figure 18.3). this will generate a search tree. unless you are\\nvery near the end of the game, none of the leaves of this tree will\\ncorrespond to the end of the game. so you’ll need some heuristic, h,\\nfor evaluating states that are non-terminals. you can propagate this\\nheuristic score up to the root, and choose the action that looks best\\nwith this depth four search. this is not necessarily going to be the\\noptimal action, and there’s a speed/accuracy trade-off for searching\\ndeeper, but this is typically effective. this approach summarized in\\nalgorithm 18.4. ',\n",
       " '1structured prediction via imitation learning a ﬁnal case where an expert can often be computed algorithmically\\narises when one solves structured prediction (see chapter 17) via\\nimitation learning. it is clearest how this can work in the case of\\nsequence labeling. recall there that predicted outputs should be\\nsequences of labels. the running example from the earlier chapter\\nwas: figure 18.3: imit:dldfs: depth limited\\ndepth-ﬁrst search x = “ monsters eat tasty bunnies “\\ny = noun verb adj noun (18.6)\\n(18.7) one can easily cast the prediction of y as a sequential decision mak-\\ning problem, by treating the production of y in a left-to-right manner.\\nin this case, we have a time horizon t = 4. we want to learn a policy\\nf that ﬁrst predicts “noun” then “verb” then “adj” then “noun” on\\nthis input. 220 a course in machine learning let’s suppose that the input to f consists of features extracted both\\nfrom the input (x) and the current predicted output preﬁx ˆy, denoted\\nφ(x, ˆy). for instance, φ(x, ˆy) might represent a similar set of features\\nto those use in chapter 17. it is perhaps easiest to think of f as just\\na classiﬁer: given some features of the input sentence x (“monsters\\neat tasty bunnies”), and some features about previous predictions in\\nthe output preﬁx (so far, produced “noun verb”), the goal of f is to\\npredict the tag for the next word (“tasty”) in this context. an important question is: what is the “expert” in this case? in-\\ntuitively, the expert should provide the correct next label, but what\\ndoes this mean? that depends on the loss function being optimized.\\nunder hamming loss (sum zero/one loss over each individual pre-\\ndiction), the expert is straightforward. when the expert is asked to\\nproduce an action for the third word, the expert’s response is always\\n“adj” (or whatever happens to be the correct label for the third word\\nin the sentence it is currently training on). more generally, the expert gets to look at x, y and a preﬁx ˆy of the\\noutput. note, importantly, that the preﬁx might be wrong! in particular,\\nafter the ﬁrst iteration of dagger, the preﬁx will be predicted by\\nthe learned policy, which may make mistakes! the expert also has\\nsome structured loss function (cid:96) that it is trying to minimize. like\\nin the previous section, the expert’s goal is to choose the action that\\nminimizes the long-term loss according to (cid:96) on this example. to be more formal, we need a bit of notation. let best((cid:96), y, ˆy) denote the loss (according to (cid:96) and the ground truth y) of the best\\nreachable output starting at ˆy. for instance, if y is “noun verb adj\\nnoun” and ˆy is “noun noun”, and the loss is hamming loss, then the\\nbest achievable output (predicting left-to-right) is “noun noun adj\\nnoun” which has a loss of 1. thus, best for this situation is 1. given that notion of best, the expert is easy to deﬁne: expert((cid:96), y, ˆy) = argmin a best((cid:96), y, ˆy ◦ a) (18.8) namely, it is the action that leads to the best possible completion\\nafter taking that action. so in the example above, the expert action\\nis “adj”. for some problems and some loss functions, computing\\nthe expert is easy. in particular, for sequence labeling under ham-\\nming loss, it’s trivial. in the case that you can compute the expert\\nexactly, it is often called an oracle.5 for some other problems, exactly\\ncomputing an oracle is computationally expensive or intractable. in\\nthose cases, one can often resort to depth limited depth-ﬁrst-search\\n(algorithm 18.4) to compute an approximate oracle as an expert. to be very concrete, a typical implementation of dagger applied\\nto sequence labeling would go as follows. each structured training\\nexample (a pair of sentence and tag-sequence) gives rise to one trajec- 5 some literature calls it a “dynamic\\noracle”, though the extra word is\\nunnecessary. imitation learning 221 tory. at training time, a predict tag seqence is generated left-to-right,\\nstarting with the empty sequence. at any given time step, you are\\nattempting to predict the label of the tth word in the input. you de-\\nﬁne a feature vector φ(x, ˆy), which will typically consist of: (a) the tth\\nword, (b) left and right neighbors of the tth word, (c) the last few pre-\\ndictions in ˆy, and (d) anything else you can think of. in particular, the\\nfeatures are not limited to markov style features, because we’re not\\nlonger trying to do dynamic programming. the expert label for the\\ntth word is just the corresponding label in the ground truth y. given\\nall this, one can run dagger (algorithm 18.4) exactly as speciﬁed.\\nmoving to structured prediction problems other than sequence labeling problems is beyond the scope of this book. the general\\nframework is to cast your structured prediction problem as a sequen-\\ntial decision making problem. once you’ve done that, you need to\\ndecide on features (this is the easy part) and an expert (this is often\\nthe harder part). however, once you’ve done so, there are generic\\nlibraries for “compiling” your speciﬁcation down to code. 1further reading todo further reading code and datasets rating +2\\n+2\\n+2\\n+2\\n+2\\n+1\\n+1\\n+1\\n0\\n0\\n0\\n0\\n-1\\n-1\\n-1\\n-1\\n-2\\n-2\\n-2\\n-2 easy? ai? sys? thy? morning? y\\ny\\nn\\nn\\nn\\ny\\ny\\nn\\nn\\ny\\nn\\ny\\ny\\nn\\nn\\ny\\nn\\nn\\ny\\ny y\\ny\\ny\\nn\\ny\\ny\\ny\\ny\\nn\\nn\\ny\\ny\\ny\\nn\\nn\\nn\\nn\\ny\\nn\\nn n\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\nn\\nn\\nn\\ny\\ny\\ny\\ny\\ny\\ny\\ny\\ny\\ny y\\ny\\nn\\ny\\nn\\nn\\ny\\ny\\nn\\ny\\ny\\ny\\nn\\ny\\nn\\nn\\ny\\nn\\nn\\nn n\\nn\\nn\\nn\\ny\\nn\\nn\\nn\\ny\\ny\\nn\\ny\\ny\\nn\\ny\\ny\\nn\\ny\\nn\\ny table 1: course rating data set bibliography shai ben-david, john blitzer, koby crammer, and fernando pereira.\\nanalysis of representations for domain adaptation. advances in\\nneural information processing systems, 19:137, 2007. steffen bickel, michael bruckner, and tobias scheffer. discriminative\\nlearning for differing training and test distributions. in proceedings\\nof the international conference on machine learning (icml), 2007. sergey brin. near neighbor search in large metric spaces. in confer-\\nence on very large databases (vldb), 1995. hal daumé iii. frustratingly easy domain adaptation. in conference\\nof the association for computational linguistics (acl), prague, czech\\nrepublic, 2007. sorelle a friedler, carlos scheidegger, and suresh venkatasub-\\nramanian. on the (im)possibility of fairness. arxiv preprint\\narxiv:1609.07236, 2016. moritz hardt, eric price, and nathan srebro. equality of oppor-\\ntunity in supervised learning. in advances in neural information\\nprocessing systems, pages 3315–3323, 2016. matti kääriäinen. lower bounds for reductions. talk at the atomic\\nlearning workshop (tti-c), march 2006.\\ntom m. mitchell. machine learning. mcgraw hill, 1997.\\nj. ross quinlan. induction of decision trees. machine learning, 1(1):\\n81–106, 1986. frank rosenblatt. the perceptron: a probabilistic model for infor-\\nmation storage and organization in the brain. psychological review,\\n65:386–408, 1958. reprinted in neurocomputing (mit press, 1998). stéphane ross, geoff j. gordon, and j. andrew bagnell. a reduction\\nof imitation learning and structured prediction to no-regret online\\nlearning. in proceedings of the workshop on artiﬁcial intelligence and\\nstatistics (aistats), 2011. 224 a course in machine learning index k-nearest neighbors, 58\\nda-distance, 113\\np-norms, 92\\n0/1 loss, 88\\n80% rule, 111 absolute loss, 14\\nactivation function, 130\\nactivations, 41\\nadaboost, 166\\nadaptation, 105\\nalgorithm, 87\\nall pairs, 80\\nall versus all, 80\\napproximation error, 71\\narchitecture selection, 139\\narea under the curve, 64, 84\\nargmax problem, 199\\nauc, 64, 83, 84\\nava, 80\\naveraged perceptron, 52 back-propagation, 134, 137\\nbag of words, 56\\nbagging, 165\\nbase learner, 164\\nbatch, 173\\nbayes error rate, 20\\nbayes optimal classiﬁer, 19\\nbayes optimal error rate, 20\\nbayes rule, 117\\nbernouilli distribution, 121\\nbias, 42\\nbias/variance trade-off, 72\\nbinary features, 30\\nbipartite ranking problems, 83\\nboosting, 155, 164\\nbootstrap resampling, 165\\nbootstrapping, 67, 69 categorical features, 30\\nchain rule, 117, 120\\nchord, 90\\ncircuit complexity, 138\\nclustering, 35, 178\\nclustering quality, 178\\ncomplexity, 34\\ncompounding error, 215\\nconcave, 90\\nconcavity, 193\\nconcept, 157\\nconﬁdence intervals, 68\\nconstrained optimization problem, 100 contour, 92\\nconvergence rate, 95\\nconvex, 87, 89\\ncovariate shift, 105\\ncross validation, 65, 68\\ncubic feature map, 144\\ncurvature, 95 data covariance matrix, 184\\ndata generating distribution, 15\\ndecision boundary, 34\\ndecision stump, 168\\ndecision tree, 8, 10\\ndecision trees, 57\\ndensity estimation, 107\\ndevelopment data, 26\\ndimensionality reduction, 178\\ndiscrepancy, 113\\ndiscrete distribution, 121\\ndisparate impact, 111\\ndistance, 31\\ndomain adaptation, 105\\ndominates, 63\\ndot product, 45\\ndual problem, 151 dual variables, 151 early stopping, 53, 132\\nembedding, 178\\nensemble, 164\\nerror driven, 43\\nerror rate, 88\\nestimation error, 71\\neuclidean distance, 31\\nevidence, 127\\nexample normalization, 59, 60\\nexamples, 9\\nexpectation maximization, 186, 189\\nexpected loss, 16\\nexpert, 212\\nexponential loss, 90, 169 feasible region, 101\\nfeature augmentation, 109\\nfeature combinations, 54\\nfeature mapping, 54\\nfeature normalization, 59\\nfeature scale, 33\\nfeature space, 31\\nfeature values, 11, 29\\nfeature vector, 29, 31\\nfeatures, 11, 29\\nforward-propagation, 137\\nfractional assignments, 191\\nfurthest-ﬁrst heuristic, 180 gaussian distribution, 121\\ngaussian kernel, 147\\ngaussian mixture models, 191\\ngeneralize, 9, 17\\ngenerative story, 123\\ngeometric view, 29\\nglobal minimum, 94\\ngmm, 191 226 a course in machine learning gradient, 93\\ngradient ascent, 93\\ngradient descent, 93 hamming loss, 202\\nhard-margin svm, 101\\nhash kernel, 177\\nheld-out data, 26\\nhidden units, 129\\nhidden variables, 186\\nhinge loss, 90, 203\\nhistogram, 12\\nhorizon, 213\\nhyperbolic tangent, 130\\nhypercube, 38\\nhyperparameter, 26, 44, 89\\nhyperplane, 41\\nhyperspheres, 38\\nhypothesis, 71, 157\\nhypothesis class, 71, 160\\nhypothesis testing, 67 i.i.d. assumption, 117\\nidentically distributed, 24\\nilp, 195, 207\\nimbalanced data, 73\\nimitation learning, 212\\nimportance sampling, 106\\nimportance weight, 74\\nindependent, 24\\nindependently, 117\\nindependently and identically dis- tributed, 117 indicator function, 88\\ninduce, 16\\ninduced distribution, 76\\ninduction, 9\\ninductive bias, 20, 31, 33, 91, 121\\ninteger linear program, 207\\ninteger linear programming, 195\\niteration, 36 jack-kniﬁng, 69\\njensen’s inequality, 193\\njoint, 124 kkt conditions, 152 label, 11\\nlagrange multipliers, 119\\nlagrange variable, 119\\nlagrangian, 119\\nlattice, 200\\nlayer-wise, 139\\nlearning by demonstration, 212\\nleave-one-out cross validation, 65\\nlevel-set, 92\\nlicense, 2\\nlikelihood, 127\\nlinear classiﬁer, 169\\nlinear classiﬁers, 169\\nlinear decision boundary, 41\\nlinear regression, 98\\nlinearly separable, 48\\nlink function, 130\\nlog likelihood, 118\\nlog posterior, 127\\nlog probability, 118\\nlog-likelihood ratio, 122\\nlogarithmic transformation, 61\\nlogistic loss, 90\\nlogistic regression, 126\\nloo cross validation, 65\\nloss function, 14\\nloss-augmented inference, 205\\nloss-augmented search, 205 margin, 49, 100\\nmargin of a data set, 49\\nmarginal likelihood, 127\\nmarginalization, 117\\nmarkov features, 198\\nmaximum a posteriori, 127\\nmaximum depth, 26\\nmaximum likelihood estimation, 118\\nmean, 59\\nmercer’s condition, 146\\nmodel, 87\\nmodeling, 25\\nmulti-layer network, 129 k-nearest neighbors, 32\\nkarush-kuhn-tucker conditions, 152\\nkernel, 141, 145\\nkernel trick, 146\\nkernels, 54 naive bayes assumption, 120\\nnearest neighbor, 29, 31\\nneural network, 169\\nneural networks, 54, 129\\nneurons, 41\\nnoise, 21 non-convex, 135\\nnon-linear, 129\\nnormal distribution, 121\\nnormalize, 46, 59\\nnull hypothesis, 67 objective function, 88\\none versus all, 78\\none versus rest, 78\\nonline, 42\\noptimization problem, 88\\noracle, 212, 220\\noracle experiment, 28\\noutput unit, 129\\nova, 78\\noverﬁtting, 23\\noversample, 76 p-value, 67\\npac, 156, 166\\npaired t-test, 67\\nparametric test, 67\\nparity, 21\\nparity function, 138\\npatch representation, 56\\npca, 184\\nperceptron, 41, 42, 58\\nperpendicular, 45\\npixel representation, 55\\npolicy, 212\\npolynomial kernels, 146\\npositive semi-deﬁnite, 146\\nposterior, 127\\nprecision, 62\\nprecision/recall curves, 63\\npredict, 9\\npreference function, 82\\nprimal variables, 151\\nprinciple components analysis, 184\\nprior, 127\\nprobabilistic modeling, 116\\nprobably approximately correct, 156\\nprogramming by example, 212\\nprojected gradient, 151\\nprojection, 46\\npsd, 146 radial basis function, 139\\nrandom forests, 169\\nrandom variable, 117\\nrbf kernel, 147 rbf network, 139\\nrecall, 62\\nreceiver operating characteristic, 64\\nreconstruction error, 184\\nreductions, 76\\nredundant features, 56\\nregularized objective, 89\\nregularizer, 88, 91\\nreinforcement learning, 212\\nrepresenter theorem, 143, 145\\nroc curve, 64 sample complexity, 157, 158, 160\\nsample mean, 59\\nsample selection bias, 105\\nsample variance, 59\\nsemi-supervised adaptation, 106\\nsensitivity, 64\\nseparating hyperplane, 87\\nsequential decision making, 212\\nsgd, 173\\nshallow decision tree, 21, 168\\nshape representation, 56\\nsigmoid, 130\\nsigmoid function, 126\\nsigmoid network, 139\\nsign, 130\\nsingle-layer network, 129\\nsingular, 98\\nslack, 148\\nslack parameters, 101\\nsmoothed analysis, 180\\nsoft assignments, 190\\nsoft-margin svm, 101 span, 143\\nsparse, 92\\nspeciﬁcity, 64\\nsquared loss, 14, 90\\nstatistical inference, 116\\nstatistically signiﬁcant, 67\\nsteepest ascent, 93\\nstochastic gradient descent, 173\\nstochastic optimization, 172\\nstrong law of large numbers, 24\\nstrong learner, 166\\nstrong learning algorithm, 166\\nstrongly convex, 95\\nstructural risk minimization, 87\\nstructured hinge loss, 203\\nstructured prediction, 195\\nsub-sampling, 75\\nsubderivative, 96\\nsubgradient, 96\\nsubgradient descent, 97\\nsum-to-one, 117\\nsupport vector machine, 100\\nsupport vectors, 153\\nsurrogate loss, 90\\nsymmetric modes, 135 t-test, 67\\ntest data, 25\\ntest error, 25\\ntest set, 9\\ntext categorization, 56\\nthe curse of dimensionality, 37\\nthreshold, 42\\ntikhonov regularization, 87 index 227 time horizon, 213\\ntotal variation distance, 113\\ntrain/test mismatch, 105\\ntraining data, 9, 16, 24\\ntraining error, 16\\ntrajectory, 213\\ntrellis, 200\\ntrucated gradients, 175\\ntwo-layer network, 129 unary features, 198\\nunbiased, 47\\nunderﬁtting, 23\\nunit hypercube, 39\\nunit vector, 46\\nunsupervised adaptation, 106\\nunsupervised learning, 35 validation data, 26\\nvapnik-chernovenkis dimension, 162\\nvariance, 59, 165\\nvariational distance, 113\\nvc dimension, 162\\nvector, 31\\nvisualize, 178\\nvote, 32\\nvoted perceptron, 52\\nvoting, 52 weak learner, 166\\nweak learning algorithm, 166\\nweights, 41 zero/one loss, 14 ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
