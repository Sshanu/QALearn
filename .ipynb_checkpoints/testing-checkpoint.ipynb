{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2id(text_file_loc):\n",
    "\n",
    "    with open(text_file_loc, 'r') as f:\n",
    "        test_str = f.read()\n",
    "\n",
    "    # retrieving indexes\n",
    "    regex = r\"((\\d{1,2})[\\d.]*) ([^}].+)\"\n",
    "    matches = re.finditer(regex, test_str)\n",
    "    contents = []\n",
    "    ids = []\n",
    "    parents = []\n",
    "    count = 0\n",
    "    last = None\n",
    "    for matchNum, match in enumerate(matches):\n",
    "        matchNum = matchNum + 1\n",
    "\n",
    "        id = match.group(1)\n",
    "        if(id in ids):\n",
    "            break\n",
    "\n",
    "        count = max(count, int(match.group(2)))\n",
    "        contents.append(match.group(3))\n",
    "        parents.append(match.group(2))\n",
    "        ids.append(id)\n",
    "        last = match.end(3) \n",
    "    regex = \"[.â€¦]{2,}\\s*\\d*\"\n",
    "    for i,c in enumerate(contents):\n",
    "        contents[i] = re.sub(regex, \"\", c)\n",
    "        \n",
    "    for i,c in enumerate(contents):\n",
    "        contents[i]  = \" \".join(c.split())\n",
    "        \n",
    "    # Graph Class\n",
    "    class Graph:\n",
    "\n",
    "        def __init__(self,v):\n",
    "            self.graph = [list() for i in range(v)]\n",
    "\n",
    "        def addEdge(self,u,v):\n",
    "            self.graph[u].append(v)\n",
    "\n",
    "        def DFSUtil(self, v, visited,res,ans):\n",
    "            visited[v]= True\n",
    "            res.append(contents[v])\n",
    "            #print(v)\n",
    "            count=0\n",
    "            for i in self.graph[v]:\n",
    "                count+=1\n",
    "                if visited[i] == False:\n",
    "                    self.DFSUtil(i, visited,res,ans)\n",
    "            if count==0:\n",
    "                final_res=[' '.join(res),res[-1],ind2id[v]]\n",
    "                #print(final_res)\n",
    "                ans.append(final_res)\n",
    "            res.pop()    \n",
    "\n",
    "        def DFS(self):\n",
    "            res=[]\n",
    "            ans=[]\n",
    "            V = len(self.graph) \n",
    "            visited =V*[False]\n",
    "            for i in range(V):\n",
    "                if visited[i] == False:\n",
    "                    self.DFSUtil(i, visited,res,ans)\n",
    "                    res=[]\n",
    "\n",
    "            return ans\n",
    "        \n",
    "    id2ind = dict([(w,i) for i,w in enumerate(ids)])\n",
    "    ind2id = dict([(i,w) for i,w in enumerate(ids)])\n",
    "    nodes=[]\n",
    "    for it in contents:\n",
    "        nodes.append(it.split())\n",
    "    parents=[int(i) for i in parents]\n",
    "    g=Graph(len(ids))\n",
    "    for i in ids:\n",
    "        if i not in [str(i) for i in set(parents)]:\n",
    "            g.addEdge(id2ind['.'.join(i.split('.')[:-1])],id2ind[i])\n",
    "\n",
    "    index_list = g.DFS()\n",
    "    \n",
    "    final_str = test_str[last:]    \n",
    "    \n",
    "    regex = \"(\" + index_list[0][2] + \")\"  + \"?\\s*\" + index_list[0][1] \n",
    "    match  = re.search(regex, final_str)\n",
    "    start = match.start()\n",
    "    sections = []\n",
    "    for i in range(len(index_list)-1):\n",
    "\n",
    "    #     regex = \"\\s*(\" + index_list[i+1][2] + \")\"  + \"\\s*\" \n",
    "    #     match  = re.search(regex, final_str)\n",
    "    #     end = match.start()\n",
    "        try:\n",
    "            regex = \"(\" + index_list[i+1][2] + \")\"  + \"\\s*\" + index_list[i+1][1] \n",
    "            match  = re.search(regex, final_str)\n",
    "            end = match.start()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                regex = \"(\" + index_list[i+1][2] + \")\"  + \"\\s*\"\n",
    "                match  = re.search(regex, final_str)\n",
    "                end = match.start()\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    regex = \"(\" + index_list[i+1][1]  + \")\"  + \"\\s*\" \n",
    "                    match  = re.search(regex, final_str)\n",
    "                    end = match.start()\n",
    "                except AttributeError:\n",
    "                    print(\"error, i\")\n",
    "\n",
    "        if(end>start):\n",
    "            sections.append(final_str[start:end])\n",
    "        else:            \n",
    "            print(\"error\", i, start, end)\n",
    "\n",
    "        start = end\n",
    "    #     print(i, start, end)\n",
    "    sections.append(final_str[start:])\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_idf(n, raw_text_file, sections, query):\n",
    "    \n",
    "    with open(raw_text_file, 'r') as f:\n",
    "        text = f.readlines()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectorizer.fit(text)\n",
    "    \n",
    "    titles = [i[0] for i in index_list]\n",
    "    \n",
    "    titles_vector = vectorizer.transform(titles)\n",
    "    titles_arr = titles_vector.toarray()\n",
    "\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    query_arr = query_vector.toarray()\n",
    "\n",
    "    sim_mat = cosine_similarity(titles_arr, query_arr)\n",
    "    top_n = [i[0] for i in sorted(enumerate(sim_mat), key=lambda x:x[1], reverse=True)][:n]\n",
    "    top_n_ids = [index_list[top_n[i]][2] for i in range(n)]    \n",
    "    top_n_titles = [index_list[top_n[i]][0] for i in range(n)]\n",
    "    top_n_sim = [sim_mat[top_n[i]][0] for i in range(n)]\n",
    "    print(top_n_sim)\n",
    "    print(top_n_ids)\n",
    "    print(top_n_titles)\n",
    "    \n",
    "    return top_n_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12703265055075788, 0.10850426730133642, 0.0]\n",
      "['6.1.4', '7.1.2', '1']\n",
      "['Registration Academic Registration Cancellation of Registration in a Course', 'Teaching and Evaluation Teaching Offering a New Course', 'Introduction']\n"
     ]
    }
   ],
   "source": [
    "top_n_ids = top_idf(3, './docs/UG-Manual.txt', index_list, 'when would i fail in a course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./docs/UG-Manual.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
